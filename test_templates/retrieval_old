from langchain.tools.retriever import create_retriever_tool
from utils.chunk_doc import get_retriever
import os
import streamlit as st
from langchain import hub
from typing import Annotated, Literal, Sequence
from typing_extensions import TypedDict
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from langgraph.prebuilt import tools_condition
from langgraph.graph.message import add_messages
from langgraph.graph import END, StateGraph, START
from langgraph.prebuilt import ToolNode
from model import llm_selected

# Initialize the retriever
print("Initializing retriever...")
retriever = get_retriever()

# Create the retriever tool
retriever_tool = create_retriever_tool(
    retriever,
    "retrieve_documents",
    "Search and return relevant documents based on user's query.",
)

# Add the retriever tool to the list of tools
tools = [retriever_tool]

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_level: str

def validate_dsa_question(state) -> Literal["proceed", "redirect"]:
    """
    Determines whether the question is DSA-related, with improved handling of
    context-dependent questions and pronoun references.
    """
    print("\n=== DEBUG: VALIDATION NODE ===")
    messages = state["messages"]
    question = messages[-1].content
    user_level = state["user_level"]
    print(f"Validating question: {question}")
    
    # Get context from previous messages if they exist
    context_messages = messages[-6:-1] if len(messages) > 6 else messages[:-1]
    conversation_context = "\n".join([
        f"{'User: ' if isinstance(m, HumanMessage) else 'Assistant: '}{m.content}"
        for m in context_messages
    ])
    
    class ValidationResult(BaseModel):
        is_dsa: str = Field(description="Binary 'yes' or 'no' indicating if question is DSA-related")
        explanation: str = Field(description="Brief explanation of why")
        redirect_message: str = Field(
            description="Polite redirect message if needed",
            default="Let's focus on data structures and algorithms. What DSA topic would you like to learn about?"
        )

    model = ChatOpenAI(
        model_name="gpt-4o-mini", 
        temperature=0, 
        streaming=True, 
        api_key=st.secrets["OpenAI_key"]
    )
    llm_with_validation = model.with_structured_output(ValidationResult)
    
    prompt = PromptTemplate(
        template="""You are validating whether questions are related to Data Structures and Algorithms (DSA).

Previous conversation:
{context}

Current question: {question}

Task: Determine if this question is DSA-related, including questions that reference previous DSA discussions.

Consider as DSA-related:
1. Direct questions about DSA topics:
   - Data structures (arrays, linked lists, trees, etc.)
   - Algorithms (sorting, searching, etc.)
   - Time/space complexity
   - Implementation details
2. Follow-up questions about previously discussed DSA topics:
   - Questions using pronouns (it, this, that) referring to DSA concepts
   - Questions asking for examples/implementations of previously mentioned algorithms
   - Questions about complexity or variations of discussed data structures

Examples of DSA-related questions:
✓ "What is quicksort?" (direct question)
✓ "What is the code for it?" (after discussing a DSA topic)
✓ "Can you show an example?" (after discussing a DSA concept)
✓ "What's its time complexity?" (referring to a previously discussed algorithm)

Examples of non-DSA questions:
✗ "How are you?"
✗ "What's the weather like?"
✗ "Can you write a poem?"

Return "yes" for DSA-related questions (including follow-ups), "no" otherwise.
If returning "no", provide a polite redirect to DSA topics.""",
        input_variables=["context", "question"]
    )
    
    # Initialize the chain
    chain = prompt | llm_with_validation
    
    try:
        result = chain.invoke({
            "context": conversation_context,
            "question": question
        })
        
        print(f"Validation result: {result.is_dsa}")
        print(f"Explanation: {result.explanation}")
        
        if result.is_dsa.lower() == "yes":
            print("---PROCEEDING TO AGENT---")
            return {
                "messages": state["messages"],
                "user_level": state["user_level"],
                "next": "proceed"
            }
        else:
            print("---REDIRECTING---")
            messages = state["messages"].copy()
            messages.append(AIMessage(content=result.redirect_message))
            return {
                "messages": messages,
                "user_level": state["user_level"],
                "next": "redirect"
            }
            
    except Exception as e:
        print(f"Error in validation: {str(e)}")
        # On error, proceed since this is a DSA chatbot
        return {
            "messages": state["messages"],
            "user_level": state["user_level"],
            "next": "proceed"
        }

def clarify_question(state):
    """
    Clarifies ambiguous questions by maintaining conversation context,
    with improved handling of pronoun references to DSA concepts.
    """
    print("\n=== DEBUG: CLARIFY NODE ===")
    messages = state["messages"]
    current_question = messages[-1].content
    print(f"Original question: {current_question}")
    
    # Get the last 3 exchanges (up to 6 messages) for relevant context
    context_messages = messages[-6:-1] if len(messages) > 6 else messages[:-1]
    conversation_context = "\n".join([
        f"{'User: ' if isinstance(m, HumanMessage) else 'Assistant: '}{m.content}"
        for m in context_messages
    ])
    
    class ClarificationResult(BaseModel):
        needs_clarification: bool = Field(
            description="Set to true if the question contains pronouns or implicit references to previously discussed DSA concepts"
        )
        clarified_question: str = Field(
            description="The clarified version of the question with pronouns replaced by their referents",
            default=""  # Provide empty string as default
        )
        referenced_concept: str = Field(
            description="The main DSA concept being referenced from previous context",
            default=""  # Provide empty string as default
        )
    
    model = ChatOpenAI(
        model_name="gpt-4o-mini", 
        temperature=0, 
        streaming=True, 
        api_key=st.secrets["OpenAI_key"]
    )
    llm_with_clarification = model.with_structured_output(ClarificationResult)
    
    prompt = PromptTemplate(
        template="""You are helping clarify user questions about Data Structures and Algorithms (DSA).

Previous conversation:
{context}

Current question: {question}

Task: Determine if the current question needs clarification, particularly for pronoun references to DSA concepts.

Rules:
1. Set needs_clarification=true ONLY if:
   - The question uses pronouns (it, this, that) referring to DSA concepts
   - The question implicitly references a previously discussed DSA concept
2. Common patterns to clarify:
   - "how to implement it" → "how to implement [algorithm/data structure]"
   - "show me the code for it" → "show me the code for [algorithm/data structure]"
   - "what's its time complexity" → "what's [algorithm/data structure]'s time complexity"
3. Do NOT clarify:
   - Questions about the conversation itself
   - Questions already containing the full concept name
   - General questions not referencing previous concepts
4. The clarified_question should maintain the exact same meaning as the original

Examples:
Previous: "What is insertion sort?"
Current: "What is the code for it?"
✓ needs_clarification=true
✓ clarified_question="What is the code for insertion sort?"
✓ referenced_concept="insertion sort"

Previous: "Can you explain binary search?"
Current: "What's its time complexity?"
✓ needs_clarification=true
✓ clarified_question="What's binary search's time complexity?"
✓ referenced_concept="binary search"

Previous: "How does quicksort work?"
Current: "What is bubble sort?"
✓ needs_clarification=false
✓ clarified_question=""
✓ referenced_concept=""
[No clarification needed as it's a new concept]

Previous: "Explain merge sort"
Current: "Can you explain that again?"
✓ needs_clarification=false
✓ clarified_question=""
✓ referenced_concept=""
[No clarification needed as it's about the conversation]

Previous: Any previous conversation
Current: "hi" or "hello"
✓ needs_clarification=false
✓ clarified_question=""
✓ referenced_concept=""
[No clarification needed for greetings]""",
        input_variables=["context", "question"]
    )
    
    chain = prompt | llm_with_clarification
    
    try:
        result = chain.invoke({
            "context": conversation_context,
            "question": current_question
        })
        
        # For non-DSA questions or greetings, use original question
        if not result.needs_clarification:
            print("No clarification needed - using original question")
            return {"messages": messages, "user_level": state["user_level"]}
            
        # For questions needing clarification, verify we have the clarified version
        if result.needs_clarification and result.clarified_question:
            print(f"Referenced concept: {result.referenced_concept}")
            print(f"Clarified to: {result.clarified_question}")
            return {"messages": [HumanMessage(content=result.clarified_question)], "user_level": state["user_level"]}
        else:
            print("No clarification needed")
            return {"messages": messages, "user_level": state["user_level"]}
            
    except Exception as e:
        print(f"Error in clarification: {str(e)}")
        # On error, proceed with original question
        return {"messages": messages, "user_level": state["user_level"]}

def agent(state):
    """Debug version of agent with explicit tool usage"""
    print("\n=== DEBUG: AGENT NODE ===")
    messages = state["messages"]
    question = messages[-1].content
    print(f"Question: {question}")
    
    try:
        # Force tool usage for DSA questions
        tool_prompt = """You must use the retrieve_documents tool for any questions about data structures or algorithms.
        Current question: {question}
        ALWAYS use the tool to search for relevant documentation before answering."""
        
        model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, api_key=st.secrets["OpenAI_key"])
        model = model.bind_tools(tools)
        
        response = model.invoke(messages)
        print(f"Agent response: {response.content}")
        return {"messages": [AIMessage(content=response.content)], "user_level": state["user_level"]}
        
    except Exception as e:
        print(f"Error in agent: {str(e)}")
        # On error, try direct retrieval
        try:
            docs = retriever.get_relevant_documents(question)
            return {"messages": [AIMessage(content=str(docs))], "user_level": state["user_level"]}
        except Exception as e2:
            print(f"Backup retrieval failed: {str(e2)}")
            return {"messages": [AIMessage(content="I encountered an error retrieving information.")], "user_level": state["user_level"]}

def rewrite(state):
    """Debug version of rewrite with validation"""
    print("\n=== DEBUG: REWRITE NODE ===")
    messages = state["messages"]
    question = messages[-1].content
    print(f"Rewriting question: {question}")

    try:
        msg = [
            HumanMessage(
                content="""Improve this question to be more specific and searchable:
                Question: {question}
                Make it clearly about DSA concepts.""".format(question=question)
            )
        ]

        model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, api_key=st.secrets["OpenAI_key"])
        response = model.invoke(msg)
        print(f"Rewritten as: {response.content}")
        return {"messages": [AIMessage(content=response.content)], "user_level": state["user_level"]}
    except Exception as e:
        print(f"Error in rewrite: {str(e)}")
        return {"messages": [HumanMessage(content=question)], "user_level": state["user_level"]}

def grade_documents(state) -> Literal["generate", "rewrite"]:
    """Debug version of grading with content validation"""
    print("\n=== DEBUG: GRADE DOCUMENTS ===")
    
    messages = state["messages"]
    last_message = messages[-1]
    question = messages[0].content
    docs = last_message.content
    
    print(f"Grading question: {question}")
    print(f"Documents content length: {len(docs) if docs else 'None'}")
    
    # Basic content validation
    if not docs or len(docs.strip()) < 10:
        print("---NO VALID DOCS TO GRADE---")
        return "rewrite"
        
    try:
        class grade(BaseModel):
            binary_score: str = Field(description="Relevance score 'yes' or 'no'")
            
        model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, api_key=st.secrets["OpenAI_key"])
        llm_with_tool = model.with_structured_output(grade)
        
        prompt = PromptTemplate(
            template="""Grade if this content is relevant to the question.
            Question: {question}
            Content: {context}
            Give 'yes' if there's ANY relevant information about the topic.""",
            input_variables=["context", "question"],
        )
        
        chain = prompt | llm_with_tool
        result = chain.invoke({"question": question, "context": docs})
        print(f"Grade result: {result.binary_score}")
        
        return "generate" if result.binary_score.lower() == "yes" else "rewrite"
        
    except Exception as e:
        print(f"Error in grading: {str(e)}")
        return "generate" if docs else "rewrite"

def generate(state):
    """Debug version of generate with content validation"""
    print("\n=== DEBUG: GENERATE NODE ===")
    messages = state["messages"]
    question = messages[-1].content
    user_level = state["user_level"]
    docs = messages[-1].content
    
    print(f"Generate received question: {question}")
    print(f"User level: {user_level}")
    print(f"Docs length: {len(docs) if docs else 'None'}")
    
    if not docs or len(docs.strip()) < 10:
        print("---NO CONTENT TO GENERATE FROM---")
        return {"messages": [AIMessage(content="I apologize, but I couldn't find relevant information to answer your question. Please try rephrasing it.")], "user_level": state["user_level"]}
        
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, api_key=st.secrets["OpenAI_key"])
        
        prompt = PromptTemplate(
            template="""Answer based on the following context. If context is insufficient, say so clearly.
            Question: {question}
            User Level: {user_level}
            Context: {context}
            
            If you don't find specific information about the topic, you can provide a general explanation based on your knowledge, but clearly state that you're doing so.""",
            input_variables=["context", "question", "user_level"],
        )
        
        chain = prompt | llm | StrOutputParser()
        response = chain.invoke({
            "context": docs,
            "question": question,
            "user_level": user_level
        })
        
        print(f"Generated response length: {len(response)}")
        return {"messages": [AIMessage(content=response)], "user_level": state["user_level"]}
        
    except Exception as e:
        print(f"Error in generate: {str(e)}")
        return {"messages": [AIMessage(content="I encountered an error generating a response. Please try asking your question again.")], "user_level": state["user_level"]}

# Graph setup
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("clarify", clarify_question)
workflow.add_node("validate_topic", validate_dsa_question)
workflow.add_node("agent", agent)
retrieve = ToolNode([retriever_tool])
workflow.add_node("retrieve", retrieve)
workflow.add_node("rewrite", rewrite)
workflow.add_node("generate", generate)

# Add edges
workflow.add_edge(START, "clarify")
workflow.add_edge("clarify", "validate_topic")

workflow.add_conditional_edges(
    "validate_topic",
    lambda x: x["next"],
    {
        "proceed": "agent",
        "redirect": END
    }
)

workflow.add_conditional_edges(
    "agent",
    tools_condition,
    {
        "tools": "retrieve",
        END: END,
    },
)

workflow.add_conditional_edges(
    "retrieve",
    grade_documents,
    {
        "generate": "generate",
        "rewrite": "rewrite"
    }
)

workflow.add_edge("rewrite", "agent")
workflow.add_edge("generate", END)

# Compile graph
from test_templates.memory import memory
graph = workflow.compile(memory)