{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing and Embedding with ChromaDB and Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['815', '831', '818', '525', '526']], 'embeddings': None, 'documents': [['##### 9.5 Insertion Sort\\n\\n_Insertion Sort is (not surprisingly) a form of insertion sorting. It starts by treating the first_\\nentry a[0] as an already sorted array, then checks the second entry a[1] and compares it with\\nthe first. If they are in the wrong order, it swaps the two. That leaves a[0],a[1] sorted.\\nThen it takes the third entry and positions it in the right place, leaving a[0],a[1],a[2]\\nsorted, and so on. More generally, at the beginning of the ith stage, Insertion Sort has the\\nentries a[0],..., a[i-1] sorted and inserts a[i], giving sorted entries a[0],...,a[i].\\nFor the example starting array 4 1 3 2, Insertion Sort starts by considering a[0]=4\\nas sorted, then picks up a[1] and ‘inserts it’ into the already sorted array, increasing the size\\nof it by 1. Since a[1]=1 is smaller than a[0]=4, it has to be inserted in the zeroth slot,\\n\\n67\\n\\n|4|1|3|2|\\n|---|---|---|---|\\n\\n\\n-----', '**Bubble Sort** This is stable because no item is swapped past another unless they are\\nin the wrong order. So items with identical keys will have their original\\norder preserved.\\n\\n**Insertion Sort** This is stable because no item is swapped past another unless it has a\\nsmaller key. So items with identical keys will have their original order\\npreserved.\\n\\n**Selection Sort** This is not stable, because there is nothing to stop an item being swapped\\npast another item that has an identical key. For example, the array\\n\\n[21, 22, 13] would be sorted to [13, 22, 21] which has items 22 and 21 in the\\nwrong order.\\n\\nThe issue of sorting stability needs to be considered when developing more complex sorting\\nalgorithms. Often there are stable and non-stable versions of the algorithms, and one has to\\nconsider whether the extra cost of maintaining stability is worth the effort.\\n\\n##### 9.9 Treesort', '```\\nThe outer loop goes over the n `1 items to be inserted, and the inner loop takes each next`\\n_−_\\nitem and swaps it back through the currently sorted portion till it reaches its correct position.\\nHowever, this typically involves swapping each next item many times to get it into its right\\nposition, so it is more efficient to store each next item in a temporary variable t and only\\ninsert it into its correct position when that has been found and its content moved:\\n```\\n for ( i = 1 ; i < n ; i++ ) {\\n   j = i\\n   t = a[j]\\n   while ( j > 0 && t < a[j-1] ) {\\n    a[j] = a[j-1]\\n    j-   }\\n   a[j] = t\\n }', '4 75 74\\n\\n4 75 74 2 54 4 75 74 2 54 4 75 74 2 54\\n\\n2 54\\n\\n4 74 75 2 54 2 4 74 75 54 2 4 54 74 75\\n\\nFigure 8.4: Insertion Sort Iterations\\n\\n1) algorithm Insertionsort(list)\\n2) **Pre:** _list_ =\\n_̸_ _∅_\\n3) **Post: list has been sorted into values of ascending order**\\n4) _unsorted_ 1\\n_←_\\n5) **while unsorted < list.Count**\\n6) _hold_ _list[unsorted]_\\n_←_\\n7) _i_ _unsorted_ 1\\n_←_ _−_\\n8) **while i** 0 and hold < list[i]\\n_≥_\\n9) _list[i + 1]_ _list[i]_\\n_←_\\n10) _i_ _i_ 1\\n_←_ _−_\\n11) **end while**\\n12) _list[i + 1]_ _hold_\\n_←_\\n13) _unsorted_ _unsorted + 1_\\n_←_\\n14) **end while**\\n15) **return list**\\n16) end Insertionsort\\n\\n|4|Col2|\\n|---|---|\\n|||\\n\\n|75|Col2|\\n|---|---|\\n|||\\n\\n|4|75|74|2|54|\\n|---|---|---|---|---|\\n\\n|4|75|74|2|54|\\n|---|---|---|---|---|\\n\\n|4|75|74|2|54|\\n|---|---|---|---|---|\\n||||||\\n\\n|2|4|54|74|75|\\n|---|---|---|---|---|\\n\\n|4|74|75|2|54|\\n|---|---|---|---|---|\\n||||||\\n\\n|2|4|74|75|54|\\n|---|---|---|---|---|\\n||||||\\n\\n\\n-----\\n\\n_CHAPTER 8. SORTING_ 68\\n\\n###### 8.5 Shell Sort', '|2|4|54|74|75|\\n|---|---|---|---|---|\\n\\n|4|74|75|2|54|\\n|---|---|---|---|---|\\n||||||\\n\\n|2|4|74|75|54|\\n|---|---|---|---|---|\\n||||||\\n\\n\\n-----\\n\\n_CHAPTER 8. SORTING_ 68\\n\\n###### 8.5 Shell Sort\\n\\nPut simply shell sort can be thought of as a more efficient variation of insertion\\nsort as described in 8.4, it achieves this mainly by comparing items of varying\\n_§_\\ndistances apart resulting in a run time complexity of O(n log[2] _n)._\\n\\nShell sort is fairly straight forward but may seem somewhat confusing at\\nfirst as it differs from other sorting algorithms in the way it selects items to\\ncompare. Figure 8.5 shows shell sort being ran on an array of integers, the red\\ncoloured square is the current value we are holding.']], 'uris': None, 'data': None, 'metadatas': [[{'source': 'data\\\\md\\\\dsa1.md'}, {'source': 'data\\\\md\\\\dsa1.md'}, {'source': 'data\\\\md\\\\dsa1.md'}, {'source': 'data\\\\md\\\\dsa.md'}, {'source': 'data\\\\md\\\\dsa.md'}]], 'distances': [[0.46953117847442627, 0.8047666549682617, 0.8465173244476318, 0.8499680757522583, 0.8767349720001221]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"../db/pdfs\")\n",
    "md_collection = client.get_or_create_collection(\"markdown_chunks_collection\")\n",
    "\n",
    "def retrieve_relevant_chunks(query, collection, model, top_k=5):\n",
    "    # Encode the query using the same SentenceTransformer model\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    # Query the ChromaDB collection for the top_k similar chunks\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k,\n",
    "    )\n",
    "    \n",
    "    # Extract the documents from the query results\n",
    "    # relevant_chunks = results[\"documents\"][0]  # Since there's only one query\n",
    "    # return relevant_chunks\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Create system-level prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a question-answering chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Function to generate a response using retrieval\n",
    "def get_response(query, collection, model, question_answer_chain):\n",
    "    # Retrieve relevant chunks from ChromaDB\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, collection, model)\n",
    "    \n",
    "    # Combine the retrieved chunks into a single context string\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    \n",
    "    # Invoke the QA chain with the provided context\n",
    "    response = question_answer_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"input\": query\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run a query\n",
    "query = \"What is Insertion Sort?\"\n",
    "\n",
    "#Load the model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "results = retrieve_relevant_chunks(query, md_collection, model)\n",
    "print(results)\n",
    "# for i, document in enumerate(results[\"documents\"][0]):\n",
    "#     print(f\"Result {i+1}: {document}\")\n",
    "#     print(f\"ID: {results['ids'][0][i]}\")\n",
    "    # print(f\"Metadata: {results['metadatas'][0][i]}\")\n",
    "    # print(f\"Similarity score: {results['distances'][0][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "        def __init__(self):\n",
    "            self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return [self.model.encode(t).tolist() for t in texts]\n",
    "        \n",
    "        def embed_query(self, query: str) -> List[float]:\n",
    "            return self.model.encode(query).tolist()\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    # model=\"gemma2:2b\",\n",
    "    model = \"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\"  # Adjust this URL if needed\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "#     # api_key=st.secrets[\"OpenAI_key\"]\n",
    "# )\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a prompt template for Ollama to generate keywords\n",
    "# Gemma2:2b Template\n",
    "# keyword_prompt_template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are an assistant that generates keywords for a chunk of text. The keywords must be single words or two-word phrases. Format the output as: ['keyword1', 'keyword2']\"),\n",
    "#         (\"human\", \"Extract relevant keywords for the following chunk:\\n\\n{chunk_text}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Llama3.2:1b Template\n",
    "keyword_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an assistant that generates keywords for a chunk of text. \"\n",
    "                   \"Your response should only be a Python list of keywords with no introductory text, in the format: ['keyword1', 'keyword2']\"),\n",
    "        (\"human\", \"Extract relevant keywords for the following chunk:\\n\\n{chunk_text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = keyword_prompt_template | llm\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "vector_store = Chroma(\n",
    "    # client = client,\n",
    "    collection_name=\"markdown_chunks_collection\",\n",
    "    embedding_function=embedding_func,\n",
    "    persist_directory = \"../db/test_pdfs\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "def split_chunks():\n",
    "    try:\n",
    "        from pathlib import Path\n",
    "        from langchain_core.documents import Document\n",
    "        from langchain_text_splitters import RecursiveCharacterTextSplitter as Rec\n",
    "        \n",
    "        # Path to markdown directory\n",
    "        md_dir = Path(\"../data/md/\")\n",
    "        chunk_id_counter = 1  # Initialize a counter for unique chunk IDs\n",
    "        ids = []\n",
    "        documents = []\n",
    "\n",
    "        # Loop through all markdown files in the md directory\n",
    "        for md_file in md_dir.glob(\"*.md\"):\n",
    "            with open(md_file, \"r\") as f:\n",
    "                md_content = f.read()\n",
    "\n",
    "            # Chunk the markdown content\n",
    "            text_splitter = Rec(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            chunks = text_splitter.split_text(md_content)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                # Generate keywords for the chunk using Ollama\n",
    "                response = chain.invoke({\"chunk_text\": chunk})\n",
    "                # keywords = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                keywords = response.strip(\"[]\").replace(\"'\", \"\").split(\", \")\n",
    "                print(keywords)\n",
    "                \n",
    "                # Create a Document object with metadata for the chunk, including keywords\n",
    "                document_to_add = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": str(md_file), \"keywords\": str(keywords)}\n",
    "                )\n",
    "                \n",
    "                documents.append(document_to_add)\n",
    "                ids.append(str(chunk_id_counter))  # Add document ID to the list\n",
    "                chunk_id_counter += 1  # Increment the ID counter\n",
    "        \n",
    "        # Assuming vector_store is defined and initialized elsewhere\n",
    "        vector_store.add_documents(documents=documents, ids=ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    split_chunks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elroy\\anaconda3\\envs\\chatbot_20241028\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\elroy\\AppData\\Local\\Temp\\ipykernel_15096\\1701805498.py:58: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Insertion Sort is a sorting algorithm that builds a sorted array (or list) one element at a time. It is similar to the way one might sort a hand of playing cards. The algorithm works by taking one element from the unsorted portion of the array and inserting it into the correct position in the sorted portion, ensuring that the sorted portion remains in order at each step.\\n\\nThe process can be summarized as follows:\\n1. Start with an initially empty sorted list.\\n2. Take each item from the unsorted list and find the appropriate position in the sorted list.\\n3. Shift elements in the sorted list as necessary to make room for the new item.\\n4. Repeat this process until all items have been sorted.\\n\\nInsertion Sort has a time complexity of O(n²) in the average and worst cases, making it less efficient for large datasets compared to more advanced algorithms. However, it is stable (it preserves the relative order of items with equal keys) and can be efficient for small datasets or partially sorted lists.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "        def __init__(self):\n",
    "            self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return [self.model.encode(t).tolist() for t in texts]\n",
    "        \n",
    "        def embed_query(self, query: str) -> List[float]:\n",
    "            return self.model.encode(query).tolist()\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "gpt4 = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    # api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# llm = OllamaLLM(model=\"gemma2:2b\", base_url=\"http://localhost:11434\")\n",
    "llm = OllamaLLM(model=\"gemma2:2b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | gpt4\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "vector_store = Chroma(\n",
    "    # client = client,\n",
    "    collection_name=\"markdown_chunks_collection\",\n",
    "    embedding_function=embedding_func,\n",
    "    persist_directory = \"../db/pdfs\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is Insertion Sort?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "# print(docs)\n",
    "len(docs)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | gpt4\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Insertion Sort is a sorting algorithm that builds a sorted array (or list) one element at a time. It starts by treating the first entry as an already sorted array and then compares the next entry with the sorted portion. If the next entry is smaller than the sorted entries, it is inserted into the correct position by shifting the larger entries up to make space. This process continues for each subsequent entry until the entire array is sorted.\\n\\nThe general algorithm for Insertion Sort can be described as follows:\\n\\n1. Start with the first element as sorted.\\n2. For each subsequent element, compare it with the elements in the sorted portion.\\n3. Shift the larger elements up to make space for the new element.\\n4. Insert the new element into its correct position.\\n5. Repeat until all elements are sorted.\\n\\nInsertion Sort is considered stable, meaning that it preserves the relative order of items with equal keys. Its average and worst-case time complexity is O(n²), making it less efficient on large lists compared to more advanced algorithms like Quicksort or Mergesort. However, it can be efficient for small datasets or partially sorted arrays.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "        def __init__(self):\n",
    "            self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return [self.model.encode(t).tolist() for t in texts]\n",
    "        \n",
    "        def embed_query(self, query: str) -> List[float]:\n",
    "            return self.model.encode(query).tolist()\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    # api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "vector_store = Chroma(\n",
    "    # client = client,\n",
    "    collection_name=\"markdown_chunks_collection\",\n",
    "    embedding_function=embedding_func,\n",
    "    persist_directory = \"../db/pdfs\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is Insertion Sort?\"\n",
    "retrieval_chain_rag_fusion  = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "# print(docs)\n",
    "len(docs)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD BM25\n",
      "14.584834019480919   377 \n",
      "\n",
      "12.523017371922746   378 \n",
      "\n",
      "12.050891594269249   375 \n",
      "\n",
      "11.880403985989389   866 \n",
      "\n",
      "11.260478820127473   535 \n",
      "\n",
      "11.123203021744485   815 \n",
      "\n",
      "11.035082004738484   814 \n",
      "\n",
      "11.002614755875888   406 \n",
      "\n",
      "10.965935890038589   260 \n",
      "\n",
      "10.352130470797842   805 \n",
      "\n",
      "NEW BM25\n",
      "Chunk ID: 815, BM25 Score: 11.123203021744485, Embedding Score: 0.4674035310745239, Source: data\\md\\dsa.md\n",
      "Most libraries provide implementations of unordered sets and so DSA does\n",
      "not; we simply mention it here to disambiguate between an unordered set and\n",
      "ordered set.\n",
      "\n",
      "We will only look at insertion for an unordered set and cover briefly why a\n",
      "hash table is an efficient data structure to use for its implementation.\n",
      "\n",
      "###### 5.1.1 Insertion\n",
      "\n",
      "An unordered set can be efficiently implemented using a hash table as its backing\n",
      "data structure. As mentioned previously we only add an item to a set if that\n",
      "item is not already in the set, so the backing data structure we use must have\n",
      "a quick look up and insertion run time complexity.\n",
      "\n",
      "A hash map generally provides the following:\n",
      "\n",
      "1. O(1) for insertion\n",
      "\n",
      "2. approaching O(1) for look up\n",
      "Chunk ID: 814, BM25 Score: 11.035082004738484, Embedding Score: 0.43229666352272034, Source: data\\md\\dsa.md\n",
      "The run time of our Intersection algorithm is O(n) where n is the number\n",
      "of items in the smaller of the two sets. Just like our Union algorithm a linear\n",
      "runtime can only be attained when operating on a set with O(1) insertion.\n",
      "\n",
      "###### 5.1 Unordered\n",
      "\n",
      "Sets in the general sense do not enforce the explicit ordering of their members. For example the members of B = 6, 2, 9 conform to no ordering scheme\n",
      "_{_ _}_\n",
      "because it is not required.\n",
      "\n",
      "Most libraries provide implementations of unordered sets and so DSA does\n",
      "not; we simply mention it here to disambiguate between an unordered set and\n",
      "ordered set.\n",
      "\n",
      "We will only look at insertion for an unordered set and cover briefly why a\n",
      "hash table is an efficient data structure to use for its implementation.\n",
      "\n",
      "###### 5.1.1 Insertion\n",
      "Chunk ID: 866, BM25 Score: 11.880403985989389, Embedding Score: 0.4283413887023926, Source: data\\md\\dsa.md\n",
      "-----\n",
      "\n",
      "_CHAPTER 8. SORTING_ 69\n",
      "\n",
      "Figure 8.5: Shell sort\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_CHAPTER 8. SORTING_ 70\n",
      "\n",
      "1. Ones\n",
      "\n",
      "2. Tens\n",
      "\n",
      "3. Hundreds\n",
      "\n",
      "For further clarification what if we wanted to determine how many thousands\n",
      "the number 102 has? Clearly there are none, but often looking at a number as\n",
      "final like we often do it is not so obvious so when asked the question how many\n",
      "thousands does 102 have you should simply pad the number with a zero in that\n",
      "location, e.g. 0102 here it is more obvious that the key value at the thousands\n",
      "location is zero.\n",
      "Chunk ID: 375, BM25 Score: 12.050891594269249, Embedding Score: 0.42387768626213074, Source: data\\md\\cp1.md\n",
      "Exercise: This implementation can be improved by about O(log V ) factor by using other data\n",
      "\n",
      "structure other than set<int> in_stack. How?\n",
      "\n",
      "**Topological Sort (on a Directed Acyclic Graph)**\n",
      "\n",
      "Topological sort or topological ordering of a Directed Acyclic Graph (DAG) is a linear ordering of\n",
      "\n",
      "the vertices in the DAG so that vertex u comes before vertex v if edge (u _v) exists in the DAG._\n",
      "_→_\n",
      "\n",
      "Every DAG has one or more topological sorts. There are several ways to implement a Topological\n",
      "\n",
      "Sort algorithm. The simplest is to slightly modify the simplest DFS implementation in this section.\n",
      "\n",
      "void topoVisit(int u) {\n",
      "dfs_num[u] = DFS_BLACK;\n",
      "TRvii (AdjList[u], v)\n",
      "if (dfs_num[v->first] == DFS_WHITE)\n",
      "topoVisit(v->first);\n",
      "topologicalSort.push_back(u); // this is the only change\n",
      "}\n",
      "Chunk ID: 377, BM25 Score: 14.584834019480919, Embedding Score: 0.41046738624572754, Source: data\\md\\cp1.md\n",
      "// inside int main()\n",
      "topologicalSort.clear(); // this global vector stores topological sort in reverse order\n",
      "memset(dfs_num, DFS_WHITE, sizeof dfs_num);\n",
      "REP (i, 0, V - 1)\n",
      "if (dfs_num[i] == DFS_WHITE)\n",
      "topoVisit(i);\n",
      "reverse(topologicalSort.begin(), topologicalSort.end());\n",
      "REP (i, 0, topologicalSort.size() - 1)\n",
      "printf(\"%d\\n\", topologicalSort[i]);\n",
      "\n",
      "In topoVisit(u), we append u to the list vertices explored only after visiting all subtrees below u.\n",
      "\n",
      "As vector only support efficient insertion from back, we work around this issue by simply reversing\n",
      "\n",
      "the print order in the output phase. This simple algorithm for finding (a valid) topological sort is\n",
      "\n",
      "again due to Tarjan. It again runs in O(V + E) as with DFS.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Exercise: Do you understand why appending topologicalSort.push_back(u) in the standard\n",
      "\n",
      "DFS code is enough to help us find topological sort of a DAG? Explain in your own words!\n",
      "\n",
      "Programming Exercises for Depth First Search (DFS):\n",
      "\n",
      "Finding Connected Components / Flood Fill\n",
      "\n",
      "_•_\n",
      "Chunk ID: 805, BM25 Score: 10.352130470797842, Embedding Score: 0.4102642238140106, Source: data\\md\\dsa.md\n",
      "When you come across a heap and you are not told what strategy it enforces\n",
      "you should assume that it uses the min-heap strategy. If the heap can be\n",
      "configured otherwise, e.g. to use max-heap then this will often require you to\n",
      "state this explicitly. The heap abides progressively to a strategy during the\n",
      "invocation of the insertion, and deletion algorithms. The cost of such a policy is\n",
      "that upon each insertion and deletion we invoke algorithms that have logarithmic\n",
      "run time complexities. While the cost of maintaining the strategy might not\n",
      "seem overly expensive it does still come at a price. We will also have to factor\n",
      "in the cost of dynamic array expansion at some stage. This will occur if the\n",
      "number of items within the heap outgrows the space allocated in the heap’s\n",
      "backing array. It may be in your best interest to research a good initial starting\n",
      "size for your heap array. This will assist in minimising the impact of dynamic\n",
      "array resizing.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "#### Chapter 5\n",
      "\n",
      "## Sets\n",
      "Chunk ID: 378, BM25 Score: 12.523017371922746, Embedding Score: 0.39959654211997986, Source: data\\md\\cp1.md\n",
      "the print order in the output phase. This simple algorithm for finding (a valid) topological sort is\n",
      "\n",
      "again due to Tarjan. It again runs in O(V + E) as with DFS.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Exercise: Do you understand why appending topologicalSort.push_back(u) in the standard\n",
      "\n",
      "DFS code is enough to help us find topological sort of a DAG? Explain in your own words!\n",
      "\n",
      "Programming Exercises for Depth First Search (DFS):\n",
      "\n",
      "Finding Connected Components / Flood Fill\n",
      "\n",
      "_•_\n",
      "\n",
      "1. UVa 260 - Il Gioco dell’X\n",
      "\n",
      "2. UVa 352 - Seasonal War (Flood Fill)\n",
      "\n",
      "3. UVa 459 - Graph Connectivity (also solvable with Union-Find Disjoint Sets)\n",
      "\n",
      "4. UVa 469 - Wetlands of Florida (Flood Fill)\n",
      "\n",
      "5. UVa 572 - Oil Deposits (Flood Fill)\n",
      "\n",
      "6. UVa 657 - The Die is Cast (Flood Fill)\n",
      "\n",
      "7. UVa 782 - Countour Painting (Flood Fill)\n",
      "\n",
      "8. UVa 784 - Maze Exploration (Flood Fill)\n",
      "\n",
      "9. UVa 785 - Grid Colouring (Flood Fill)\n",
      "\n",
      "10. UVa 852 - Deciding victory in Go (Flood Fill)\n",
      "\n",
      "11. UVa 10336 - Rank the Languages (Flood Fill)\n",
      "\n",
      "12. UVa 10926 - How Many Dependencies?\n",
      "Chunk ID: 406, BM25 Score: 11.002614755875888, Embedding Score: 0.14705872535705566, Source: data\\md\\cp1.md\n",
      "### 4.5 Dijkstra’s\n",
      "\n",
      "Motivating problem: Given a weighted graph G and a starting source vertex s, what are the shortest\n",
      "\n",
      "_paths from s to the other vertices of G?_\n",
      "\n",
      "This problem is called the Single-Source[3] _Shortest Paths (SSSP) problem on a weighted graph._\n",
      "\n",
      "It is a classical problem in graph theory and efficient algorithms exist. If the graph is unweighted,\n",
      "\n",
      "we can use the BFS algorithm as shown earlier in Section 4.3. For a general weighted graph, BFS\n",
      "\n",
      "does not work correctly and we should use algorithms like the O((E +V ) log V ) Dijkstra’s algorithm\n",
      "\n",
      "(discussed here) or the O(V E) Bellman Ford’s algorithm (discussed in Section 4.6).\n",
      "\n",
      "Edsger Wybe Dijkstra’s algorithm is a greedy algorithm: Initially, set the distance to all vertices\n",
      "\n",
      "to be INF (a large number) but set the dist[source] = 0 (base case). Then, repeat the following\n",
      "process from the source vertex: From the current vertex u with the smallest dist[u], ‘relax’ all\n",
      "Chunk ID: 260, BM25 Score: 10.965935890038589, Embedding Score: 0.08466075360774994, Source: data\\md\\cp1.md\n",
      "we only know Complete Search, there is no way we can solve this problem.\n",
      "\n",
      "**Approach 2: Greedy (Wrong Answer)**\n",
      "\n",
      "Since we want to maximize the budget spent, why don’t we take the most expensive model in each\n",
      "\n",
      "garment_id which still fits our budget? For example in test case A above, we choose the most\n",
      "\n",
      "expensive model 3 of garment_id = 0 with cost 8 (money_left = 20-8 = 12), then choose the\n",
      "\n",
      "most expensive model 2 of garment_id = 1 with cost 10 (money_left = 12-10 = 2), and then for\n",
      "\n",
      "garment_id = 2, we can only choose model 1 with cost 1 as money_left does not allow us to buy\n",
      "\n",
      "other models with cost 3 or 5. This greedy strategy ‘works’ for test cases A+B above and produce\n",
      "\n",
      "the same optimal solution (8+10+1) = 19 and “no solution”, respectively. It also runs very fast,\n",
      "\n",
      "which is 20 + 20 + ... + 20 of total 20 times = 400 operations in the worst case.\n",
      "\n",
      "But greedy does not work for many other cases. This test case below is a counter-example:\n",
      "\n",
      "M = 12, C = 3\n",
      "Chunk ID: 535, BM25 Score: 11.260478820127473, Embedding Score: 0.06979474425315857, Source: data\\md\\cp1.md\n",
      "exercises below (e.g. UVa 900, etc). Note that Fibonacci sequence grows very fast and sometime\n",
      "\n",
      "problems involving Fibonacci have to be solved using Java BigInteger library (see Section 5.4 for a\n",
      "\n",
      "quick solution involving large integers).\n",
      "\n",
      "Fibonacci numbers have many interesting properties. One of them is the Zeckendorf’s the\n",
      "**orem: every positive integer can be written in a unique way as a sum of one or more distinct**\n",
      "\n",
      "Fibonacci numbers such that the sum does not include any two consecutive Fibonacci numbers.\n",
      "\n",
      "Programming Exercises related to Fibonacci:\n",
      "\n",
      "1. UVa 495 - Fibonacci Freeze (use Java BigInteger class)\n",
      "\n",
      "2. UVa 763 - Fibinary Numbers (Zeckendorf representation)\n",
      "\n",
      "3. UVa 900 - Brick Wall Patterns (Combinatorics, the pattern is similar to Fibonacci)\n",
      "\n",
      "4. UVa 948 - Fibonaccimal Base (Zeckendorf representation)\n",
      "\n",
      "5. UVa 10183 - How many Fibs?\n",
      "\n",
      "6. UVa 10229 - Modular Fibonacci\n",
      "\n",
      "7. UVa 10334 - Ray Through Glasses (use Java BigInteger class)\n"
     ]
    }
   ],
   "source": [
    "from utils.bm25_ranking import find_closest_chunks_bm25, new_bm25, re_rank_chunks_with_embeddings\n",
    "print(\"OLD BM25\")\n",
    "results_top_n = find_closest_chunks_bm25(query, results2, top_n=10)\n",
    "for res in results_top_n:\n",
    "    print(res['score'],\" \",res['id'],\"\\n\")\n",
    "    # print(res['document'],\"\\n\")\n",
    "\n",
    "\n",
    "print(\"NEW BM25\")\n",
    "bm25_results = new_bm25(query, results2, top_n=10)\n",
    "sorted_results = re_rank_chunks_with_embeddings(query, bm25_results)\n",
    "\n",
    "# print(results_top_n)\n",
    "# for res in results_top_n:\n",
    "#     print(res['score'],\" \",res['id'],\"\\n\")\n",
    "#     print(res['document'],\"\\n\")\n",
    "\n",
    "# organised_list = {}\n",
    "# for item in results2:\n",
    "#     print(item)\n",
    "#     # organised_list[results2[item]] = ## id and document\n",
    "# print(organised_list)\n",
    "# print(results_top_n)\n",
    "# print(md_collection.get(ids=\"297\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['220'], 'embeddings': None, 'metadatas': [{'source': 'data\\\\md\\\\cp1.md'}], 'documents': ['best possible acorns collected when Jayjay is at this height. The bottom-up DP code that requires\\n\\nonly 2000 = 2K states and time complexity of 2000 2000 = 4M is as follow:\\n_×_\\n\\nfor (int tree = 0; tree < t; tree++) // initialization\\ndp[h] = max(dp[h], acorn[tree][h]);\\nfor (int height = h - 1; height >= 0; height--)\\nfor (int tree = 0; tree < t; tree++) {\\nacorn[tree][height] +=\\nmax(acorn[tree][height + 1], // from this tree, +1 above\\n((height + f <= h) ? dp[height + f] : 0)); // best from tree at height + f\\ndp[height] = max(dp[height], acorn[tree][height]); // update this too\\n}\\nprintf(\"%d\\\\n\", dp[0]); // solution will be here\\n\\nLesson: When na¨ıve DP states are too large causing the overall DP time complexity not-doable,\\n\\nthink of different ways other than the obvious to represent the possible states. Remember that no\\n\\nprogramming contest problem is unsolvable, the problem setter must have known a trick.\\n\\n**5. Free Parentheses (ACM ICPC Jakarta 2008 LA 4143)**'], 'uris': None, 'data': None}\n"
     ]
    }
   ],
   "source": [
    "print(md_collection.get(ids=['220']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_20241028",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
