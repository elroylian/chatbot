{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "\n",
    "# Load .env from the project root\n",
    "env_path = Path('..') / '.env'  # Go one directory up to locate .env\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY']= st.secrets[\"EXTRA_Langchain_key\"]\n",
    "# os.environ['LANGCHAIN_PROJECT']=\"pr-advanced-theism-85\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Retrieval using Metadata filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from utils.chunk_doc import get_retriever, get_vector_store\n",
    "\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     api_key=st.secrets[\"OpenAI_key\"]\n",
    "# )\n",
    "\n",
    "vector_store = get_vector_store()\n",
    "\n",
    "query = \"What is Insertion Sort?\"\n",
    "metadata_filter = {\"keywords\": \"Insertion\"}\n",
    "# response = vector_store.search(query,search_type=\"mmr\", k=5, fetch_k=10)\n",
    "# print(response)\n",
    "found_docs = vector_store.max_marginal_relevance_search(query, filter=metadata_filter)\n",
    "print(found_docs)\n",
    "for i, doc in enumerate(found_docs):\n",
    "    print(f\"{i + 1}.\", doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Generation using LLAMA 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.custom_embeddings import MyEmbeddings\n",
    "from utils.chunk_doc import get_vector_store\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    # model=\"gemma2:2b\",\n",
    "    model = \"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\"  # Adjust this URL if needed\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a prompt template for Ollama to generate keywords\n",
    "# Gemma2:2b Template\n",
    "# keyword_prompt_template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are an assistant that generates keywords for a chunk of text. The keywords must be single words or two-word phrases. Format the output as: ['keyword1', 'keyword2']\"),\n",
    "#         (\"human\", \"Extract relevant keywords for the following chunk:\\n\\n{chunk_text}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Llama3.2:1b Template\n",
    "keyword_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an assistant that generates keywords for a chunk of text. \"\n",
    "                   \"Your response should only contain keywords in json format.\"),\n",
    "        (\"human\", \"Extract relevant keywords from the following chunk:\\n\\n{chunk_text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = keyword_prompt_template | llm\n",
    "\n",
    "# Initialize Vector Store\n",
    "vector_store = get_vector_store()\n",
    "\n",
    "def split_chunks():\n",
    "    try:\n",
    "        from pathlib import Path\n",
    "        from langchain_core.documents import Document\n",
    "        from langchain_text_splitters import RecursiveCharacterTextSplitter as Rec\n",
    "        \n",
    "        # Path to markdown directory\n",
    "        md_dir = Path(\"../data/md/\")\n",
    "        chunk_id_counter = 1  # Initialize a counter for unique chunk IDs\n",
    "        ids = []\n",
    "        documents = []\n",
    "\n",
    "        # Loop through all markdown files in the md directory\n",
    "        for md_file in md_dir.glob(\"*.md\"):\n",
    "            with open(md_file, \"r\") as f:\n",
    "                md_content = f.read()\n",
    "\n",
    "            # Chunk the markdown content\n",
    "            text_splitter = Rec(\n",
    "                chunk_size=2000,\n",
    "                chunk_overlap=500,\n",
    "                length_function=len,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            chunks = text_splitter.split_text(md_content)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                # Generate keywords for the chunk using Ollama\n",
    "                response = chain.invoke({\"chunk_text\": chunk})\n",
    "                \n",
    "                import json\n",
    "                # Convert the JSON string to a Python dictionary\n",
    "                dictionary_output = json.loads(response)\n",
    "\n",
    "                # Access the \"keywords\" list\n",
    "                keywords_list = dictionary_output[\"keywords\"]\n",
    "                \n",
    "                # Create a Document object with metadata for the chunk, including keywords\n",
    "                document_to_add = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": str(md_file), \"keywords\": str(keywords_list)}\n",
    "                )\n",
    "                \n",
    "                documents.append(document_to_add)\n",
    "                ids.append(str(chunk_id_counter))  # Add document ID to the list\n",
    "                chunk_id_counter += 1  # Increment the ID counter\n",
    "        \n",
    "        # Assuming vector_store is defined and initialized elsewhere\n",
    "        vector_store.add_documents(documents=documents, ids=ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # split_chunks()\n",
    "    \n",
    "    # print(response)\n",
    "\n",
    "    response = chain.invoke({\"chunk_text\": \"\"\" ###### 2.1.1 Insertion\n",
    "\n",
    "In general when people talk about insertion with respect to linked lists of any\n",
    "form they implicitly refer to the adding of a node to the tail of the list. When\n",
    "you use an API like that of DSA and you see a general purpose method that\n",
    "adds a node to the list, you can assume that you are adding the node to the tail\n",
    "of the list not the head.\n",
    "\n",
    "Adding a node to a singly linked list has only two cases:\n",
    "\n",
    "1. head = in which case the node we are adding is now both the head and\n",
    "_∅_\n",
    "_tail of the list; or_\n",
    "\n",
    "2. we simply need to append our node onto the end of the list updating the\n",
    "_tail reference appropriately._\n",
    "\n",
    "1) algorithm Add(value)\n",
    "2) **Pre: value is the value to add to the list**\n",
    "3) **Post: value has been placed at the tail of the list**\n",
    "4) _n_ node(value)\n",
    "_←_\n",
    "5) **if head =**\n",
    "_∅_\n",
    "6) _head_ _n_\n",
    "_←_\n",
    "7) _tail_ _n_\n",
    "_←_\n",
    "8) **else**\n",
    "9) _tail.Next_ _n_\n",
    "_←_\n",
    "10) _tail_ _n_\n",
    "_←_\n",
    "11) **end if**\n",
    "12) end Add\n",
    "\n",
    "As an example of the previous algorithm consider adding the following sequence of integers to the list: 1, 45, 60, and 12, the resulting list is that of\n",
    "Figure 2.2.\n",
    "\n",
    "###### 2.1.2 Searching\n",
    "\n",
    "Searching a linked list is straightforward: we simply traverse the list checking\n",
    "the value we are looking for with the value of each node in the linked list. The\n",
    "algorithm listed in this section is very similar to that used for traversal in 2.1.4.\n",
    "_§_\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "_CHAPTER 2. LINKED LISTS_ 11\n",
    "\n",
    "1) algorithm Contains(head, value)\n",
    "2) **Pre: head is the head node in the list**\n",
    "3) _value is the value to search for_\n",
    "4) **Post: the item is either in the linked list, true; otherwise false**\n",
    "5) _n_ _head_\n",
    "_←_\n",
    "6) **while n** = **and n.Value** = value\n",
    "_̸_ _∅_ _̸_\n",
    "7) _n_ _n.Next_\n",
    "_←_\n",
    "8) **end while**\n",
    "9) **if n =**\n",
    "_∅_\n",
    "10) **return false**\n",
    "11) **end if**\n",
    "12) **return true**\n",
    "13) end Contains\n",
    "\n",
    "###### 2.1.3 Deletion\n",
    "\n",
    "Deleting a node from a linked list is straightforward but there are a few cases\n",
    "we need to account for:\n",
    "\n",
    "1. the list is empty; or\n",
    "\n",
    "2. the node to remove is the only node in the linked list; or\n",
    "\n",
    "3. we are removing the head node; or\n",
    "\n",
    "4. we are removing the tail node; or\n",
    "\n",
    "5. the node to remove is somewhere in between the head and tail; or\n",
    "\n",
    "6. the item to remove doesn’t exist in the linked list\n",
    "\n",
    "The algorithm whose cases we have described will remove a node from anywhere within a list irrespective of whether the node is the head etc. If you know\n",
    "that items will only ever be removed from the head or tail of the list then you\n",
    "can create much more concise algorithms. In the case of always removing from\n",
    "the front of the linked list deletion becomes an O(1) operation.\"\"\"})\n",
    "\n",
    "    # print(dictionary_output)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from utils.custom_embeddings import MyEmbeddings\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from utils.chunk_doc import get_vector_store\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY']= st.secrets[\"EXTRA_Langchain_key\"]\n",
    "os.environ['LANGCHAIN_PROJECT']=\"chatbot-test\"\n",
    "\n",
    "\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by a single newline. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "gpt4 = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# llm = OllamaLLM(model=\"gemma2:2b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | gpt4\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: [line for line in x.split(\"\\n\") if line.strip() != \"\"])  # Ensure empty strings are removed\n",
    ")\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "vector_store = get_vector_store()\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is Insertion Sort?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "# print(docs)\n",
    "len(docs)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | gpt4\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from utils.custom_embeddings import MyEmbeddings\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.chunk_doc import get_retriever, get_vector_store\n",
    "\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    # api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "# Initialize Vector Store\n",
    "# vector_store = get_vector_store()\n",
    "\n",
    "retriever = get_retriever()\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is Insertion Sort?\"\n",
    "retrieval_chain_rag_fusion  = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "# print(docs)\n",
    "len(docs)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from utils.custom_embeddings import MyEmbeddings\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.chunk_doc import get_retriever, get_vector_store\n",
    "from prompt_templates.retrieval_check import get_rc_chain\n",
    "import streamlit as st\n",
    "from langchain_core.messages import HumanMessage\n",
    "from utils.image_processing import process_image, encode_image\n",
    "from prompt_templates.image_template import get_image_chain\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "def read_image_bytes(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        return image_file.read()\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")\n",
    "import base64\n",
    "\n",
    "\n",
    "img = read_image_bytes('46bfac9.png')\n",
    "\n",
    "\n",
    "image_data = base64.b64encode(img).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "image_chain = get_image_chain(llm)\n",
    "\n",
    "message_content = [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe the image provided\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"image_data\": image_data})\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "question = \"Okay thank you so much thats all?\"\n",
    "retrieval_chain  = get_rc_chain(llm)\n",
    "response = retrieval_chain.invoke({\"input\":question})\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.db_connection import ChatDatabase\n",
    "\n",
    "db = ChatDatabase('chat.db')\n",
    "\n",
    "print(db.load_chat_history(chat_id=\"6fcf537a-8e1e-496b-be68-84841722fa57_1\",user_id=\"6fcf537a-8e1e-496b-be68-84841722fa57\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit User Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_user_data(\"6fcf537a-8e1e-496b-be68-84841722fa57\",\"\",\"elroy7602@gmail.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "users = db.get_all_users()\n",
    "\n",
    "for user in users:\n",
    "    print(user['user_id'],\" \",db.get_user_level(user['user_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Specific User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.delete_user(\"5b2429f2-7dde-4469-8b88-75910f2e5a5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level = \"intermediate\"\n",
    "levels = (\"Beginner\", \"Intermediate\", \"Advanced\")\n",
    "print(levels.index(user_level.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_level_tool(user_id: str) -> str:\n",
    "    \"\"\"Get user level for given user_id\"\"\"\n",
    "    try:\n",
    "        user_level = db.get_user_level(user_id)\n",
    "        return f\"Current user level is {user_level}.\" if user_level else \"User not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving user level: {str(e)}\"\n",
    "    \n",
    "\n",
    "hello = get_user_level_tool(\"6fcf537a-8e1e-496b-be68-84841722fa57\")\n",
    "\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tools with Langchain LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_StJMon95i5N4wk1gyNP4oyOr', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_l9XGcLbfxcdPv4w09zg1hzNR', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from utils.custom_embeddings import MyEmbeddings\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.chunk_doc import get_retriever, get_vector_store\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\", \n",
    "        temperature=0, \n",
    "        streaming=True, \n",
    "        api_key=st.secrets[\"OpenAI_key\"]\n",
    "    )\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(ai_msg.tool_calls)\n",
    "\n",
    "messages.append(ai_msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_StJMon95i5N4wk1gyNP4oyOr', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}, {'index': 1, 'id': 'call_l9XGcLbfxcdPv4w09zg1hzNR', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-6f538260-5986-4429-af39-b01af767677a-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_StJMon95i5N4wk1gyNP4oyOr', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_l9XGcLbfxcdPv4w09zg1hzNR', 'type': 'tool_call'}]),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='call_StJMon95i5N4wk1gyNP4oyOr'),\n",
       " ToolMessage(content='60', name='add', tool_call_id='call_l9XGcLbfxcdPv4w09zg1hzNR')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of \\\\(3 \\\\times 12\\\\) is 36, and the result of \\\\(11 + 49\\\\) is 60.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-9c6964b2-9be9-4755-bc21-d71b95e0d175-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Retrieval Tool for Langgraph [Important]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAKmCAIAAABL5ysIAAAAAXNSR0IArs4c6QAAIABJREFUeJzs3WdAFNfaB/CzBVjYXTrSi4r0qmLFEmtEsGBiCxqTcNXEEluMsSSWKzHReNFYI5YIGlsUxKAgdlQUEAyo2EBEeodd2L7vh/ElRJciW87s+Pw+yczuzLPIf8+cKefQ5HI5AgBQFB13AQAANYKEA0BlkHAAqAwSDgCVQcIBoDJIOABUxsRdAHjvNNRI6irF/DoJv14iFctlMtwFdQBDh8Zk0gwMGWxDpqmVnj5Ha5pGGlwPB5pRXSJ+9ndDfg5fT5+BkJxtyDQwZLAMGDKZFvwF6ujSeXUSfr2ksU4qaJLq6NK7ebN7+HO5JmRvIyHhQO14tZJb8VUIISNznW7ebAs7PdwVKaskX5Cfw68uFXFNmANCzHT0yNukQ8KBeqUl1Ty4XTcg2MylFxd3LaqXnVJ361xl/7HmPoOMcNeiGCQcqFHsriLXXobufSmY7ZYyLtVUl4hGhlniLkQB8h5dAG23f01+7xGmlI83QqjXcBMHN4O43UW4C1EA2nCgFvvX5E+cb2tqqYu7EM15fp+fnlw9Zak97kL+BRIOVC92V1HvEaZ2Lvq4C9G0R3fqi/MEw6d1wV3IPyDhQMXSEms4xgz3voa4C8EjPblGn83w7E+Wjw/9cKBKDdWSh3fq3tt4I4R6jzC5cqIcdxX/gIQDVboVXzkgxBx3FZj1DzYjrv+TASQcqExlsYjGoPXw52hmdzk5OUKhENfb29BruElViVDQSIrbcSHhQGWe3+cZW+hoZl/x8fGzZs1qamrC8vZ2GXCZ+dk8NW38nUDCgcrk5/C6ebE1s69ON7/EqWU1td7Nunqx83P4at1FB0HCgWo01EhYbIa5rervOS8oKJg7d25gYGBQUFBERIRMJouPj9+0aRNCaMSIEb17946Pj0cIZWVlzZ8/PzAwMDAwcM6cOY8ePSLeXltb27t37+jo6NWrVwcGBv7nP/9R+HbV6urFbqiVkOE6FdmfjAHaoq5SrKYtb9iw4cWLF0uXLuXz+enp6XQ6feDAgWFhYTExMZGRkRwOx8HBASFUXFwsFArDw8PpdPrJkycXLlwYHx/PYrGIjezfv//jjz/es2cPg8GwtLR8++2qRaMhYZO0oVpsaKahbktrIOFANfh1ErahWv6ciouL3dzcJk6ciBAKCwtDCJmamtrZ2SGEvLy8jI2NiZeNGTMmKCiI+LeHh8fcuXOzsrL69etHLPH29p43b17zNt9+u8qxDZn8OikkHFAEv15iYMhQx5aDgoIOHTr0888/h4eHm5qatvYyGo125cqVmJiY/Px8AwMDhFBV1T+XrPr06aOO2trANmTy6yUa3unboB8OVESOdPTUkvB58+YtWbIkKSlp3LhxJ06caO1lUVFR33zzjYeHx9atWxctWoQQkrUYPkZfX9O30Oro0eUkuF4GCQeqoc9l1FeJ1LFlGo02ffr0uLi4IUOG/Pzzz1lZWc2rmu+5FgqFBw8enDBhwtKlS/38/Ly9vTuyZbXesl1fJVbTQc07gYQD1WAbMhvrperYMnFli81mz507FyGUm5vb3CZXVFQQr2lqahIKhe7u7sSPtbW1b7Thb3jj7erAr5cYqOfExDvBXwGgBq6Jjq6+WhqMb7/9lsPh9OvXLyUlBSFExNjX15fBYGzZsmXcuHFCoXDSpEnOzs7Hjh0zMzPj8Xi//fYbnU5/9uxZa9t8++0qL5tjosM1xnyaDdpwoDImljrlhUJ1XDPz8vLKycmJiIjIzc1dtWqVr68vcTJ81apVBQUFW7ZsuXjxIkIoIiJCX1//u+++i46OXrx48RdffBEfHy8WK67n7ber1qsnTUguZ5Lg6Xh4ehSozI0zlVxTpt8QdV1/0iI3zlQamjJ9SfCrgKN0oDLdfdi56Q1tvKCurm78+PEKV9nZ2b169ert5UOGDFm3bp3qalQsPDxc4SG9u7t7871xLXl5ee3YsaONDdZXi30Hk2JsRmjDgSqd3lHU90NTW2fFl6ZkMllpaanCVTSa4j9FfX19ExMTVZf5poqKCoXH861Vpaura27e6kOyuXcbXj1rHDGdFAMzQsKBKpUVCK6frvh4MbnGKtOw/Wvypy13MODiv1QGZ9qAilk6siwd9V8+asRdCDYPU+t9BhmTJN6QcKB6g0PNL58ob6jBf8Om5hXnCR6l1QeMUnu3ouMg4UD1PlnhcPSnAtxVaJqoUXZuX/GkBXa4C/kX6IcDtZCI5Qd/ePHJd2TpjqpbZZHw7G/Fs77vSifZx4WEA3Vp4kmP/vRyzCxrm+4s3LWo1/P7/LSkqqnfqP45c+VBwoF6XT5ezq+T9A82N7chwR1eqlb8vOnWuSorR1bgBJKOMAsJB2pX8KjxVnylozu7i71eN282nUHDXZGyRAJZXg6/9IWgukQ4IMTcyom8BymQcKAheX/zn9xryMvhufY21NGlsQ2ZBlyGngFDJtOCv0Amk85vkDTWSxobpPw6aeGTxm7ebNdeXAc3A9yltQMSDjTtZW5jTbmosUHaWC+VSZFEospxEiQSSVZWVu/evVW4TYQQS58uR8iAy2AbMk2t9Wy158wCJBxQCo/HCw4Ovnr1Ku5CyAKuhwNAZZBwAKgMEg6oxs3NDXcJJAIJB1RDDOQGCJBwQDXqm+RAG0HCAdUQA60CAiQcUI2NjQ3uEkgEEg6opri4GHcJJAIJB1TTwQlP3hOQcEA12dnZuEsgEUg4AFQGCQdU08YMxO8hSDigmurqatwlkAgkHFCNhYUF7hJIBBIOqEatcwZrHUg4AFQGCQdU4+zsjLsEEoGEA6pROIvoewsSDgCVQcIB1Xh4eOAugUQg4YBqHj58iLsEEoGEA0BlkHBANV5eXrhLIBFIOKCanJwc3CWQCCQcACqDhAOqgdGUW4KEA6qB0ZRbgoQDQGWQcEA1MF56S5BwQDUwXnpLkHBANS4uLrhLIBFIOKCaJ0+e4C6BRCDhAFAZJBxQjZWVFe4SSAQSDqimtLQUdwkkAgkHVOPp6Ym7BBKBhAOqefDgAe4SSAQSDqgGnh5tCRIOqAaeHm0JEg6oxt7eHncJJEKTy+W4awBAWeHh4aWlpQwGQyaTVVZWmpub0+l0sVickJCAuzTMoA0HVDB16tS6urqioqKSkhKxWFxSUlJUVMRgMHDXhR8kHFDBiBEjevTo0XKJXC739vbGVxFZQMIBRcyYMcPAwKD5R2tr62nTpmGtiBQg4YAiPvjgg65duzafV/L19YU2HBIOKOWzzz5js9kIIUtLy6lTp+IuhxQg4YA6hg4d6uzsLJfLoQFvxsRdAHiP8OsklUUisVimvl2MHzFH1nBm5IAZz+7z1LcXPT26uZ2ePkcLztXD9XCgCfVVkutnKsoLBU4eHH6DFHc5ytJj0V/m8m2764/4xFJHl4a7nLZAwoHa8WolsbuKP5hiY2hOqWPG8peCOwkVoQtsWQbk7e2StzJAGYfWvRg/z4Fi8UYIdXFgfTDV+tjml7gLaQu04UC9UhOqdA10e/hzcReiLtk3arjGdO9AI9yFKAZtOFCv4jwB14RqrXdLBobM0hcC3FW0ChIO1EsmQ1wTHdxVqJGhmY5QoMarA0qChAP14teJZZTuCMplSMgn79UBSDgAVAYJB4DKIOEAUBkkHAAqg4QDQGWQcACoDBIOAJVBwgGgMkg4AFQGCQeAyiDhAFAZJBxoh6vXkj8Y3vvlyxedeG9e3rNx4z9IuXmV+FEmk+0/sOujyR+OmzAsNTVF1ZWSC5Uf6wOAwGQyORwuk/H6r/3cX2f+OPb7nNkL7e0cvbz8cFenXpBwQGVyuZxGozk4OB09crZ54d20Wz39Az7+6BOspWkIJByQjkAgiI6JunIlqaKy3NLSetTIsZ9M/+yN12RnZ0XHRGXnZCGE3Fw9585d5OrijhCqq6udEDpi7pyvnz57fPPm1R493ILGjP/p53UIoc0/7+zdq+/wkX1kMhlC6IPhvRfM/0YgaDr0+96TJy4YGb4epGXjj2sePvj7SEwcjo+uetAPB+QilUpXrlp04mTMoEHDli/7fsjg4YWvCt6eY7C0tFgoEs4IC/905uzS0uIV3y0UCP4ZaCUmZr+VpfUvW/bM+2qpv1/A7P8saF61fu1mBwenHs6uG9Zv6dcvcPSoYKlUeuVKErFWLBanpt4YNmy0pj6u2kEbDsjl2vVLmVnp3yxbEzRmfBsvGzFizMiRQcS/XV09liydm52TFdC7H7HEw8M7/It5zS/29enZ/O+BA4ccO3FYn6UfOHAosSQgoH9i0rkJ4z9GCKWnp/J4vOHDPlTPh8MAEg7I5W7aLT09vdGjgtt+GY1Gu5Fy5cTJmIKCfGJCwprqqua1PXv26fgePxwdsm79ipcvXzg4OF29nty9ew8np25KfAJygaN0QC411VXmZhbtTv19ODrq+x++cXXx2Lhh69w5ixBCMvk/g6WxWPod3+PAAUMMDY0Sk86JxeJbN69RqQGHNhyQDofDra6pavs1QqHw6B8HxwZNmD9vKUKovLxMmT3q6OiMGDEm6eJfHu7ePD5v2AfU6YRDGw5Ix98/oKmp6dLlxOYlEokEIaSro4sQqq+vQwgJBE1CodDFxZ14QV19LXEfS6d3+uHokMrKil17/uft7WdpaaWKz0EW0IYDchk5Iig27sSmn37IzX3g3N0lL/9Zxr07v+050rWbM51O/9+2H+fPW+bv17tbN+fTZ46ZmprxebzfD/9Gp9Pz8p51eqc9nF0dHJxevnwx+eMwlX4a/KANB+Sip6f3y5Y9o0cFX0xOiNy+6W7arcGDhkskEmsrm2+/+UEoFBL3ma5ZFaHP0l+/4bvjJ6O//HLxjLAvEhPjxWJxp/fr4e7NZDKHDhmh0k+DH8xqBNTr9w0vRs604xqT/WhxzffLJFLJjxsj3/WN5S8FWZcrJ31tp566lEX23zsA6nYx+XzypfNpabd/2bIbdy2qBwkH77vz5+PEEvFPm3719+uNuxbVg4SD993WX/bgLkGN4EwbAFQGCQeAyiDhAFAZJBwAKoOEA0BlkHAAqAwSDgCVQcIBoDJIOABUBgkHgMog4UC9zKx0Kf/4opGFLu4SWgUJB+rF0KFXFws68EJtVVEo0OeQN0fkrQxQQzcvTnWpEHcValRbKXL04OCuolWQcKBerr05wkbJ3zdqcBeiFncSKozMGHbOLNyFtArGeAGakBRdpmfANLHUNbNl0XAXozypVF5ZJCx90WhupRsw2gR3OW2BhAMNeZLRkP+AL5XIK4tEatyNXM7j8zgcrhp3gZCptS7LgN7Dn+vobqDWHSkPEg4ohcfjBQcHX716FXchZAH9cACoDBIOAJVBwgHVeHt74y6BRCDhgGqys7Nxl0AikHBANc7OzrhLIBFIOKCaZ886P4EZ9UDCAdV4eHjgLoFEIOGAah4+fIi7BBKBhAOqgX54S5BwQDXQD28JEg4AlUHCAdW4uLjgLoFEIOGAap48eYK7BBKBhANAZZBwQDUsFnlHXNE8SDigGoGAygM/vitIOKAaExNSD6ukYZBwQDU1NdQc9bFzIOEAUBkkHFCNnZ0d7hJIBBIOqObVq1e4SyARSDgAVAYJB1QDd622BAkHVAN3rbYECQeAyiDhgGpgNOWWIOGAamA05ZYg4QBQGSQcUA2M09YSJBxQDYzT1hIkHFANPFvWEiQcUA08W9YSJBwAKoOEA6pxc3PDXQKJQMIB1eTm5uIugUQg4YBqPD09cZdAIpBwQDUPHjzAXQKJQMIB1bi7u+MugUQg4YBqHj16hLsEEoGEA6qBfnhLNLlcjrsGAJQ1e/bsFy9eMBgMmUxWXV1tYmLCYDCkUmlSUhLu0jCDNhxQwdixY0UiUUVFRVVVlVwur66urqioqK6uxl0XfpBwQAXjx4+3sbFpuUQmkwUEBOCriCwg4YAipk6dymazm380NjaeNm0a1opIARIOKGLcuHG2trbNP/bo0WPw4MFYKyIFSDigjqlTp+rp6SGEjIyMwsLCcJdDCpBwQB1EMy6Xy52dnQcNGoS7HFJg4i4AkJRMhhqqJQhp2cXUSeNmREdHTwn9rK5SjLuWdyRHRhY6Kt8qXA8Hb3rxsDHram3R80YLW1YTT4q7nPeFoZlO0bPGrt6cgBEmXRz0VLVZSDj4l8fpvAep9f2CLLhmqm9PQLtqK0Qpp8sGhVrYObNUskFIOPjHozv1T7P4H0y1xl3I+y4hqjBwvLmts77ym4IzbeA1qVj+KL0B4k0Gw6bbZlxSzWhzkHDwWmWJSCSQ4a4CIIQQy4BeUSTk16vgJAgkHLxWVymydjLAXQV4zd6VXVsmUn47kHDwmlQihzPn5MGrkchkKjhHBgkHgMog4QBQGSQcACqDhANAZZBwAKgMEg4AlUHCAaAySDgAVAYJB4DKIOEAUBkkHAAqg4QDzdm2/afQj0Y1/5iX92zc+A9Sbl5V+OKPp4zZ+r+Idrf58FGOUChUsjAej/fkqbKzjkskkrCZE3fviVRyO6oFCQfYMJlMDofLZHR+sMALifHz5s8SCJqUrCR89tTz5+OU3AiNRuNyDVks1YzNoiowEiPAxsHB6eiRs8psQfnWmyASqeA5TQaDsXvn76ooR5Ug4aCT4s+d3vq/iMOH/rS3dySWLF4yp6mpcc/u6PMXzsbGnsjLf6avb9AnoP/8ecuMjU3eePuFxPiffl6HENr8887evfoihKRS6eHofef+OiMQNPn59RYKBMQry8vL9h/cdefOTT6fZ2/vOH3aZyOGf0hsIXLbJoTQhNARCKFvl//w4egQhFBmVvq+qB3Pnz8xMTH19wsI/2KemZl5Gx9k6vTgmprq2LiTsXEnLS2tjh09hxCqqqrcved/d+7elEgk3l5+c+cs6tbNGSG0+vulL/Kf9+jhlp6RSqPR+/Yd+NXcxSYmpiWlxdM/GYcQCvvk8y8+/wohJBAIomOirlxJqqgst7S0HjVy7CfTP2MwGGr7D1EMjtJBJw0ePJzJZCZfOk/8WFZWmnU/IyRkEkLo4cNsBwenObMXhgSH3rx17afN695+u79fwOz/LGi5ZNv2nw5HR/XtM3Dh/OUsPVYDr4FYLpFKcnMfjB/30ZdzFhkaGm2MWP0o9wFCqG+fgZM/DkMI/bgxcntkVN8+AxFCGffuLv92vpNjt2VL10z+KOzvv+8tWTZX8P9fFgqt/eFnLtdwUOAH2yOj1v7wMxHOJcvmZty7O/s/C5csWllZVbFk2dzmeioqy93dvX7+aecXn391587N5d/Ol0gkJsamG9ZvYTJfN5lSqXTlqkUnTsYMGjRs+bLvhwweXviqQPPxhjYcdJ6RoVHgwKHJyec/mzUXIZR86TyHwxk+7EOE0JLFK2k0GvEyJpMZc+SAUCgkZiNpZmlp5evTs/nHJ09z48+dbm4AR48OzrqfQayysbY9dOAkscExY8ZPnDTi5s2r7m6eJiamNjZ2CCF3dy8jI2Pixb/u2BwSHLpwwXLix969+3362Udp6bcHBX7Q2gdxc/VgMplmZube3n7EkovJCS9fvvhly+6e/gEIIW9v/+lh406fPvbpzP8ghJwcuxHfLO5unmw2Z2PE6rt3bw0YMDhw4NDmT33t+qXMrPRvlq0JGjNepb/1dwYJB50XHBy67JuvcnLue3n5Jl38a+TIscR5JrFYfPrMsYvJCeXlpXp6LJlMVltbY2lp1cambty4jBD66KNPmpfQ6f8cYD57/uTQ73sfP35INI/V1VUKN1JaWlJQkF9UVHjurzMtl5eXl73T57p/P4PD5hDxRghZWVk7ODg9fvLw7Vf26TMAIfQoN2fAgH/NkXY37Zaent7oUcHvtF91gISDzuvpH2Bra5986TxTR+flyxfrfvgZISSXy1euWvT4ycNPZ8728PC5cePyseOHZfJ2xngsKy/lcDhGhkZvr7qXmfbtigX+fr2Xf/MD24D9/dpvWttaTU0VQujTmbMHDxrWcrmpaVv98Lfx+Dyjf584MDQ0qqqsePuVHDaHRqM1NjW+WUl1lbmZBZbD8jdAwkHn0Wi0sUETjh0/LJfLfXz8nZy6IYTu37+Xce/uqpX/Jc6HFb162ZFNGRuZ8Hg8kUikq6v7xqro6CgbG7uIjZFEL1ef9eYo4s1j/nM4XISQUChwcHB618/ScuIAC/MuDx9mt1xbXV1l2UXBMUhlZYVcLu9iYfnGcg6HW12j+EBDw+BMG1DKmA/HNTby48+dHhfyEbGkrr4WIeTSw63ljzKZDCGko6Pb1NQokUje3o6LiztC6NLlC2+vqquvde7uQsRbJBI1NjUSW2tOe+X/t652dg6WllbnL5xtanp9hVwikYjF7U9gps/Sr6qqbP7R09OnoaH+0aMc4sfnz58WFRU299JbSjgfhxDy9PB5Y7m/f0BTU9Oly4nNSxR+ag1grF27FsuOAdlUFgnrKiUObux3eheLpZ+f/6ympvqbpWuIg1K2ASfu7MmyshIDA/b1G5ejY6LEYrG/X28HB6fa2porVy/m5T91dfU05BpWVJQnnI8bNXKsjY2do2PXq9eSky7+xeM11NbWxJ/7MzMz3dXFvX//QQUvX1y7lmxiYlpWVhq5fVNRUSENoeDgUBqNxtI3iDt78kVBHg3RHj7KdnP1sLS0TkiIu3X7ulyOHj7M3v7rz2KJ2MPDu+1P8fTp4xspl5lM5ouCPB2mjr9/wJWrSZcuX9DXN3j2/Elk5I9MHZ1vv/lBX1//8pWkBw/+FggE5eWlsbEnTv15tG/fgdOnzSK2Ex2z38vTt6d/gKNjt9upN/7660xDQ31NddXF5IR9Ub8Gjw1tPhXXrry/G2y7s4zMlZ1bChIOXutcwhFCXK4hh83pE9Cf+JHNZjs5dbuQGH8hMV4ikaxa+d/KyvKcnKzRo4O7du0uEDSlpd12d/V0cHBqmXA6nd6/36DCVwXXriX/nZ3Z1al7SUmRo2PX/v0HeXr4FhTknT5zLOt++tAhI0MnTLl8JbFHDzdra1tDrqGFheXVqxdv377R0FA/enSwo0NXN1ePv//OTLr416PcnO7deowcObbt6+FEo/3s2eOLyQlPn+a6uXl2deo+oP/g/PxnZ+NP3blz08XF/fs1P1pZWSOELl9JamzkC4XChPOxJSVFo0aOXfz1d809i+aEM5nMIUNG1tXVXr128eatq3X1tUOHjPTw8O54z1xVCYd5y8Brj+7WFzwSDJzQBXchpLb6+6UV5WV798Soe0cXo4sDRhrbuyo7RwWcaQPUx+Pxpn2i+MLVnNlfB4+dqPGKNAcSDqjPwMDgt71HFa4y5Cq4PkclkHBAfXQ63drKRiWb+u/6X1SyHY2Bq2UAUBkkHAAqg4QDQGWQcACoDBIOAJVBwgGgMkg4AFQGCQeAyiDhAFAZJBwAKoOEg9cYOnR9Lvw9kAXXhElndPRh8jbA/yh4zcRCp/iZspOHAFV58ZBnZv3mgFadAAkHr1nY6enqw98DKfDrJDZd9VlsFQzkCP+j4B/+Q40TDxXhrgKgizFFfceYqmRTMMYL+JdXT5tuxFb2C+piaKYDTbqGNfGkDdXi63+WTphra2Kl7PhNBEg4eFP5S2HG5ZrCJ436BszGhvYHKlUHuVwul8tbToqgQjKZjEajd3hMRA0xttTj1YidPNh9RptyTVU2cAMkHLRKLJAjHDEQCAQhISEXL15U0/YTEhLu37//3XffqWn7nSOXI12W6n/dMMYLaJWOGv7gOqK2npd8+QKTqa69j584lmXARHSJjo5qjoTJDNpwQC5Pnz7lcrlWVm1NcgY6Dk6lABKJjY09duyYZuI9ffp0DewFO0g4IIvGxkYnJ6c1a9ZoZnd9+/Y9fPiwZvaFERylA7LIz893dHRU0/nz9xb8NgEpfPXVV+Xl5RqOd05OTklJiSb3qHnQhgP8ysrK6HS6hYWFhvf77Nmz1atXHzt2TMP71SRIOMCMx+NVVlY6Ob3zjN8qkZqa2r17d81/uWgMJBzgxOfzx4wZc/36ddyFUBb0wwFOiYmJcXFxeGvYsmVLZWUl3hrUBxIOcAoNDTUxMcFbg729/YEDB/DWoD6QcIDH3bt3f/zxR9xVIITQlClTRo0ahbsKdYGEAwyEQmFsbCx5nv3w9PSUSCS4q1ALONMGAJUvm0EbDjTtwoULFy5cwF3Fvzg7Ozs7Oz9//hx3IaoHbTjQqIcPH27dujUqKgp3Ie8LSDgAr92/f9/X1xd3FSoGR+lAc54+fVpaWoq7ilYdP348MTERdxUqBgkHGnL79u1t27aReWiHadOmPX36FHcVKgZH6UBDLl26NGTIECYTBg7TKEg4AP9ITU01NjZ2c3PDXYjKwFE6ULvMzMzw8HDcVXQIg8GIjIzEXYUqwSETULtz5879/PPPuKvokICAgOrqarFYTJlhWOEoncqkUillbsbU0dGBAZ46AdpwKhOJRA0NDRgLkMvlTU1NBgYGym/KyMhIT09PFUW1Izs7++7du1988YUG9qUB8KUI1KihoUHrTp57eHjs3bsXdxUqA0fpVNbU1ISxDVft3GMaa8MRQo8ePbK2tjY2NtbM7tRKy75fgRah0Wg0sk3/1zHu7u64S1AZOEoHaiEUCvGeAlBGaWnpli1bcFehGpBwoBYikYjD4XTijRcuXAgKCqqurlZDUR1lZWWVlJRUVVWFsQZVgaN0oBZcLhd3CUo5evSoxrr9agUJB4rJ5fJO96IFAoGenp6WdsIJ5ubmuEtQDUj4+2X9+vUFBQXOzs737t2j0+m9e/cODw8nRjvdtWtXSkrKwoULo6KiiouLIyIi/Pz8cnNz9+/f//TpUxaL1bdv3/Dw8ObGOTEx8ezZs69evWKz2X379p05cyaxnbi4uNjY2JqaGktLy6FDh4aGhhKNoUAg+P33369evSoSiezs7EJDQ4cMGUJs6vnz53v27Hn69KmJiYmdnR3W39BrBQUFq1atiokeaSMnAAAgAElEQVSJwV2IsiDh752qqqqxY8eGhoY+e/bs8OHDBQUFkZGRxFXrxsbGw4cPz5s3TyAQ+Pr6FhQUrFy50tHRcdGiRXV1dTExMeXl5cQAqTExMUePHh00aNDEiRNra2szMjKI2zyPHDly+vTpcePGOTg4vHr16tSpU0VFRcuWLZPJZOvWrSsrK5syZYqxsfH9+/d/+ukngUAwevTowsLCb7/91tDQcNasWQwG4+jRo7h/Qwgh5OjoWFZWVl9fb2hoiLsWpUDC3zsODg6hoaEIIVdXVwMDg82bN6enp/fr1484PbZw4cLmJ6uOHTtGo9E2bNhAnDPjcrlbtmzJzs62trY+fvz4sGHDli1bRrzyo48+Ir47jh8/vnz58sDAQGK5mZnZjh075syZk5WV9eDBg4MHD5qZmSGEhg4dKhAI4uLiRo8evX//fjqdvnXrVuL6M51O37lzJ75fzz8uXryIuwQVgIS/13r37o0Qevz4MZFwPT29lg9OZmdn+/r6Np8S79mzZ/M4LVKpdOzYsW9sLTMzUyKRbN68efPmzcQS4n6qqqqqtLQ0iUTy+eefN79YKpWy2WyBQHDv3r2xY8c2317CYDDU/7k7RCqVyuVyrbsn7w3aXT1QEpvNptFoAoGA+FFfX7/l2sbGRiMjo+YfiR54VVUV8TTL2+eiiEtca9eufWOVtbV1TU2NqanpG1MgMJnMmpoaiURiaWmp+s+mtNTU1OPHj2/fvh13IUqBhL/Xqqqq5HJ5a+eNzczMWt61UltbixDicDhEq15TU/PGlJ3NJ+Hs7e3f2BSHw6mrq+vSpcsbl6AaGxubt0w2rq6ujx8/xl2FsuCOl/daUlJSGzdpuru7Z2dnN7fwKSkpxIMZPj4+xLn05lcSrbqvry+NRjt79mzz8qamJuIffn5+Uqk0ISHhjVUGBgY2NjY3btwQi8Vq+5SdZG5uToGBGRlr167FXQNQF4lEIhKJWi65du3ao0ePhEJheXn5uXPnzpw5ExAQMHnyZIRQWlray5cvJ02a1PxiBweHuLi47OxsJpOZlpYWHR3t5eU1ffp0IyOjqqqqCxcuFBQU8Pn8e/fubd26tW/fviwWSyqVXrp06enTp0KhMD09fcuWLb6+vqampo6OjpmZmcnJyfX19TU1NcnJyXv27Pnwww+ZTCaXy01MTExPT5dIJM+ePYuNja2vrw8NDX2jy8BisbS9S4wFJJzKFCa8qalJJBIlJiaWlpYOHz58/vz5urq6ChNuaGjo6emZkZFx/vz5Z8+eDRo0aNGiRcSLAwICdHR07t69e/369aKiop49e7q7u7PZ7H79+hkYGNy9e/fatWtFRUX9+vXr27evvr4+g8EYNGgQj8e7cePGzZs3+Xz+qFGjPD096XR6165dDQ0N79+/f/v27crKSmdn57y8PJIkPDIysrCw0NPTU8P7VSF4epTK3n56dP369ZWVldp49kiTT482i4uLy8/PX7RokYb3q0Jw2ANUQyKRUO8oevz48bhLUBacaQMqIJFIeDwe7ipUTyqVFhcX465CKXCUTmUaG+NFKBTK5XIWi6W+XWA5SkcIDRgw4MqVK9r7nBm04UAF9PT01BpvjPr161dZWYm7is6DNpzKNNaGa6ATjqsN13aQcCqTSqUauJNEKBRu3Lhx/fr1at2Lrq4ulvHSKyoq9PT0tPcJM6qd/AQtMRgMDTzIUVJSYm1tTdWj9Li4OIlEMnfuXNyFdBL0w4GyunbtunjxYtxVqEvXrl21et4YOEoHynr06JGOjo6zszPuQoAC0IYDZUVHRz9//hx3FeoilUq1d1hoSDhQAVtbWypNuP2G2tralvfqax040waUNW/ePNwlqJGZmZlWz3kK/XCgrPT09F69emn12MkUpsVfToAMhELhwoULqR3v0tJS7T2dDgkHShGJREFBQbirUK8VK1bk5ubirqKTIOFAKVwud/Xq1birUC83NzcSDjLVQdAPB0ppaGi4f/9+8wDpgGygDQdKKSws3Lt3L+4q1Ku8vJyco8F2BCQcKEVXV5fCF8MJJ06cOHPmDO4qOgmO0gFoR2xsbFNT07Rp03AX0hmQcKCUmpqa3Nzc/v374y4EKAZH6UApL1++PHz4MO4q1IvP5xMTNmkjSDhQioWFxQcffIC7CvW6ffv2zz//jLuKToL70kFnrFq16vz58zQajbib7aeffkIIWVpatpy3iDIMDQ2J2dG1ESQcdEZYWFhWVlZZWRnxI41Gk8lkfn5+uOtSiz59+vTp0wd3FZ0ER+mgM9zd3f39/VueprW1tZ0+fTrWotRFLBbX1NTgrqKTIOGgk2bMmGFlZUX8Wy6Xe3p6enl54S5KLfLz87/66ivcVXQSJBx0kqurq6+vL9GM29jYaOnl4o7gcDitTbFOfpBw0HkzZsywtrYmGnBfX1/c5aiLjY3Nr7/+iruKToKEg85zd3f38fGxsLCgag+cIJFInj59iruKToJ72iiOXy+9e6G6+HmTXC7n16l+GAO5XC6VyphMtYzKbunIkslQd2+O7xAjdWy/g4ih2i5duoSxhk6Dq2VUVl0mPrPjVf9gS+eeRlxjHa37MpfJUXWxoKpYGLenePxcG1xlsFgsd3d3XHtXErThlFXyQnDleHnIXAfchajAk4z6gocNofNtcReifaAfTll3Eqo/nGWHuwrVcOllaOVk8OB2Pa4CMjIycO1aSZBwaqopEzXUinVY1Pn/NbLQffGQj2vvX375pVQqxbV3ZVDnLwC0VF0mtndh465ClcxsWDIZtr3369dPS/uzcKaNmiQiWVODVrY5rZNXFglx7Xv79u24dq0kaMMBaF96eroM4yGEEiDhALRvyZIlTU1NuKvoDEg4AO0LCAjQ0tnLoB8OQPt++eUX3CV0klZ+LQGgYVlZWVo6dRkkHID2LVu2rKGhAXcVnQEJB6B9Pj4+DIZanq5RN+iHA9C+rVu34i6hk6ANB6B9ubm5Wjr9KCQcgPYtWbJESydFgIQD0L7u3btr6ZDp0A8HoH0wThsAHfXwUY5QiO0Zks559eoVXA8HoH0XEuPnzZ8lEGjZPd6zZ8+uqqrCXUVnQMLBu6mrq61v6PxYK1rXehMsLS3hejjQbtnZWdExUdk5WQghN1fPuXMXubq8Hn4wMfHckT8OlpeXdnXqTqPTrSytv1/zI0KopLR4166tGffu6OrqufRw+/zzr9xcPRBCq79fam/nyGQyz/11RiIW9+sX+PXCFRwO50JifOS2TQihCaEjEELfLv/hw9EhuD93hxw8eBB3CZ0EbTh4rbS0WCgSzggL/3Tm7NLS4hXfLRQIBAihlJtXN/281ten5+qVG3V0dR89yvlo0nSEUFVV5YKFn9c31M2ft2zO7IVisfjrReH5+c+JrZ04GVNaWhyxMXL+vGVXryXHHNmPEOrbZ+Dkj8MQQj9ujNweGdW3z0DcH7qjampqtPT5cGjDwWsjRowZOTKI+Lerq8eSpXOzc7ICeveLizvp5NRt6ZJVCCE3N8+Pp4xJvZPi4eEdHRNlYmz6y+bdTCYTITRyRFDYzAnnEs4smLcMIWRn57Dyuw00Gs3dzfN6yuW09Ntz53xtYmJqY2OHEHJ39zIyMsb9id/BJ598cvDgQUtLS9yFvDNIOHiNRqPdSLly4mRMQUG+gYEBQqimugohVF5RZmf3ekhmc3MLFovV0FCPELpz52Z5RVlQ8KDmLYjF4ory1/MNs/RYxNTiCCFLS+ucnPs4PpPKODk5QT8caLfD0VEHD+2ZFDptdviCqurKdetXyOQyhJCNjd3jxw9FIpGurm5e3jOBQODs7IoQqq6p6t9/0OzwBS03wmZz3t6yDlNHJtPuQeN27dqFu4ROgoQDhBASiURH/zg4NmjC/HlLEULl/98UI4SmTfl0ybK5S5bN7dWzz8WLCW6uHqNHBSOEuFzDurpaBwenTuxO68Ytra+v53A42jjMi/ZVDNRBKBIKhUKX/z95XldfixAizi15eflOCp0mk8mKi19NmTIz8n/7iI53z559cnLuP37yqHkjHRnJTJ+ljxCqrKxQ56dRvalTp1ZUaFnNBGjDAUIIcTncbt2cT585Zmpqxufxfj/8G51Oz8t7hhA6eepIZmba5MkzaDQak8l89epl9+49EEKfzpydmpryzfJ5kz8OMzExvXv3llQm/e/6dkY78vTyZTAYO3ZtGTN6nFAkHBcySVMfUSlGRkZa2g+HNhy8tmZVhD5Lf/2G746fjP7yy8Uzwr5ITIwXi8WuLh7VNVUbI1b/d+Oqteu+DZ89bev/IhBCtjZ2O7Yf8PT0OXL0wM5dv9TW1YwYPqbdvdja2C1dsqqwsGDHzi1Xr17UyCdTgT/++MPc3Bx3FZ0BMxNS0+P0hrzsxsBQ1VzdkUqlRAsmEon27tseG3si8fwt4lhdY/j1kvP7X322tjPdfuVpbz8cjtJBO5KS/oo6sPODoaOsrW1raqpu3Ljs5NRNw/HGburUqXA9HFCTo1M3by+/5Evn6+vrzMzMBw4YEvbJF7iL0jTt7YdDwkE7XF3c16yOwF0FZn/88QfuEjpJ+/oVAGhefX29lt6XDgkHoH3aez0cEg5A+6AfDgCVQT8cACqDfjgAVAb9cEAuxcXF1LtbUSQS5+bmYtm19vbDIeEUdOjQodjY2OYBGCiDwWBs2LDh5cuXmt+19t6XDgmnjtTU1NOnTyOEBgwY8NVXX+EuR/UYDPqRI0dMTU0RQuHh4VlZWRrbNfTDAWYPHjyIjo7u27cvQsjFxYXBpLHYWnlU2RoGnW5krosQ4nA4CKH58+cnJSURcxVoYO/QDwd4XL9+PTw8nBhIbOfOnba2tsRyrgmzvFDLZh1oW02FkEb758yCn5/f8uXLEUIVFRXBwcHPnz9X696hHw40raioCCF09+7d1atXI4TYbHbLtSZWekwdSv3n8mrEts76by/39/fft29fTU0NQigxMVFNe4d+ONCcBw8ejBw5sqGhASG0bNkyJycFj0zr6tF6+HNSzpQp2oD2EQtlaYmVfUabKlxrbW3du3dvhFBdXV2/fv3UMdE39MOBJly8eBEh1NDQcPz4cTc3t7Zf7DPIyLa7/vVTZWKhVv5pNqssEsbuKPh0TftjP0yePDklJYVGo7148SIyMpLP56uqBu3th8Ndq1pAJpPRaLTZs2ePGzcOIdSvX78OvtFnkKGOLko+Usyvk5hZswSNWjZ7JtdUJz+7wdmPO2OVk45ehy7+EUNTODk5mZub//HHH+Hh4dXV1cTpd2Vobz8cRnEiNYFAsHv37kGDBvXs2ZNGo3XuErdchvj1koZqiRyp9//6xo0bf/755+zZsz08PFSyQYYO3cJGl85Q6sJ+REQEn89ft27d+zYuDQESTlI8Ho/D4ezZs4fD4YSFheEup0OWLl169erVrl27HjlyRE9PD3c5/7hw4UJAQICZmVlOTo6Xl1cntqC947RBwklHJBJt2rTJxsaGuAymLaqrqz/77DPiDP+IESM2bdqEuyIFZs2a1aNHj1WrVr3rG4OCgrR0nDbt+06isIKCAolEkp+f7+vrq13xRghlZGQQl6wQQrdu3Tp27BjuihQ4dOjQhAkTEEJJSUmpqakdf6P29sMh4WSxa9euxYsX0+l0V1fX8ePH4y7nnaWkpDQ2NhL/bmxs/P333/Pz83EXpYCnpydCyNfXNzo6+s6dOx18l/ZeD4ejdMyePn3a0NDQs2fPtLS0gIAA3OV0klAonDJlSssbSOVyuYeHR3R0NNa62lFXV2dkZLR69eoPP/wwMDCwjVdqbz9c+yqmktu3b69Zs8bGxgYhpL3xRgilp6fzeLzmH4lm4+HDh1iLap+RkRFC6PPPPz958qRUKm3uZbxNe6+HQ8IxePHixcaNGxFCjo6Ox44ds7Kywl2Rsq5evVpbW8tgMMzMzORy+Z07dzIyMjIyMnDX1SHdunXbtm0bg8F4+fJleHh4ZWXl26/R3n44HKVrVENDA5fLXbRo0dSpUzt+44p2mTp16oYNG3r06IG7kM7IzMysr68fMmTI8+fPu3fvjrscFYA2XEN4PN7KlSvv3r2LEIqMjKRqvImnvrAM0qAS/v7+Q4YMQQjt3r2bOM4iwH3poFXEUV9qauqQIUOGDx+Ouxy1Mzc3f/r0Ke4qlLVly5ahQ4cSz/nU1tZqbz/8fbyPT5M2bdpUXFy8ffv2ESNG4K5FQ1xdXdPS0nBXoQIDBw5ECBkbG0+aNInD4WhpPxzacLUQi8XEV76rq+v27dtxl6NR1tbWRGeEGmxtbS9duhQREWFubn716lXc5bwzSLjqXblyZdCgQcRzDhMnTsRdjqY5OTm9ePECdxUqZm9vL5PJSkpKJkyYIJVKcZfzDiDhqpSdnU1cDU5NTTUxMcFdDh5MJnPAgAElJSW4C1Eloh8+bdq0X3/9VSKRVFRUFBYW4i6qQyDhqlFTUxMcHFxdXY0QGjZsGO5yMBMKhQUFBbirUKXm6+H29vZ6enpcLnfBggXqGzRKhSDhyrp//z6R8H379hEXWoC9vb1mhkDVmDfuS2exWLGxscQtcffu3cNaWjsg4UrZvn37zp07ifuirK2tcZdDFi4uLnV1dbirUCWF18OJmxry8vLCw8NJe7UcEt5JmZmZxNwDv/32G+5aSMfU1PTRo0e4q1ClNq6Hf/TRR/Pnz29qaiotLdV4Xe2DhL+z4uLifv366evrI4SIIT7BG2xtbSl2lN72fel+fn5sNpvJZI4YMYJsHxzuS38HWVlZfn5+T5486dat2/s56FcH8fn8MWPGXL9+HXchmlZbW3vnzp3Ro0fjLuQf0IZ31N69e/fv3090MiHebWOz2f7+/lTqinfwvnRjY2Mi3lOmTElJSdFIae2AhLePeAqyZ8+ev/76K+5atEZVVRWVLom/633px48f7/gAMmoFCW+LSCQKDQ0lBifS6hEaNM/S0pKcZ546pxPPhy9duhQhtH///hs3bqitrvZh7ofL5XIynAhQODpPcXGxrq4un893dHTEUZR227x5s729/dSpU3EXgt+iRYtWr16Na5g3zP1JuVyucEgNTdLV1TU2Nm655MmTJ2FhYUlJScbGxlo6/h52jo6OtbW1uKtQGWXGaYuMjOTxeA8ePGCxWJofVQKO0v9FIBAghPLz82/fvv1G7ME7MTQ0pNKNq0o+H87hcFxcXL777ru8vDyV1tU+SPg/kpOTly1bhhAaPXq0lj4MTB4WFhZaOmSCQsqP06ajo3PixImmJk1P6g4JR8TUfwihnJycHTt24K6FIiiWcFWNl06M1j569Oj6+npV1NU+SDgqKyvbt28fcUYEdy3UQbGEq3actiNHjhw9elRVW2sbGRPO5/OfPXum5Ea+/PLLjkydJZPJioqK5syZo+TuwBv09fX9/PxUOIM3Xqodp83c3Hzu3LnEoYGqttkaMiZ83rx5SUlJ6t6LTCaTSqU0Gq1nz57q3tf76dWrV8QD8xSgpvHSGQyGuid4I2PCRSKRunchlUqJEfw7NyM36AgTE5M2ZhHRLmqat2zy5MlEz1x9SHd/9axZs2pra8+dO3fu3LkuXbocOnQIISSRSGJiYpKTk+vr6+3t7cPCwvr370+8Pjc3d//+/U+fPmWxWH379g0PD+dyuW9sUyAQ7Nq1i7iL0NPTc/bs2WZmZqampjg+33uESglX37xl3t7exJ/9gQMH1LF90rXhK1eu5HK5AwYM2Lx588qVK4mF27dv//PPPz/88MNvvvnG0tJyw4YNOTk5xHS8K1eulEgkixYtmjZt2q1btyIiIt7e5okTJ5KTkydMmPDZZ5/V1tbq6+vr6Oho/JO9d0xNTSmTcHWPl75t27bmv3bVIl0b7uLiwmAwTE1Nm49eCgsLk5OTp02bFhYWhhAKDAwMDw8/cuTIjz/+eOzYMRqNtmHDBg6HgxDicrlbtmzJzs4mvheblZWVsVisjz/+WCKRDB8+HOKtGTY2Ns3zDWs7dc9bZmRk1JETw51Aujb8bURzPWDAAOJH4tzYkydPiLFNfX19iXgTj38R8/W+sYUPPvhAKBSuWbOmpKQE4q0xOjo65eXluKtQDY3NHx4SEqLau321IOHEFZeW95ByudympqbGxsbGxkZiNLzm5cRzi29soVevXkuXLq2trZ03b962bdskEokGy39/cbnchoYG3FWoBo/H08wjUvHx8RcvXiTunlYJkia85W/TzMyMmLWzeUlNTQ2TydTT0zMzM2u5nPjya27Sm4lEomHDhu3cufM///lPYmLiqVOnNPIh3neGhoYau3NL3SZPnqyx45GPP/64trZWVV8oZEw4i8VqeR3Vzc2NRqM1T5QjEonS0tLc3d0ZDIa7u3t2dnbzFx4xqoaHhwdxiEiEXyQSEZfE6HT6xIkTzczMlL+dBnSEoaEhZdpwfX19dZzobg2Hw1m4cKFKNsVYu3atSjbUOXK5/O2TMc+fP7916xYxY7uOjo69vX15eXl8fDyNRqusrIyKiiooKPj666+trKwcHBzi4uKys7OZTGZaWlp0dLSXl9f06dNpNNqTJ09SUlIqKyv79Olz9uzZAwcOSCSS1NTUu3fvDhs2zMvLq3l3DAaDxWJp/KNTX0NDQ1JS0qRJk3AXogJTpkxhs9ka252urm5gYGBqaqqTk5OSmyJjwt3c3PLy8q5cufL8+XMXFxd7e/uePXvy+fykpKRr164ZGBgsXLiwV69eRCvh6emZkZFx/vz5Z8+eDRo0aNGiRbq6usSUgCUlJWlpaSEhIXw+Pzs7++rVqy9fvhw5cmRYWFjL72NIuJpIJJLs7OxRo0bhLkQFRCKRhh831NPTs7e3l0gkSu4X8xgvMplMTSNAEL+ajtyy9vYIEEAlamtrJ02adOnSJdyFqEBQUNDBgwctLS01vN9ffvnF2tp6+vTpnd4CGfvhyuPz+WKxGO5IxYvNZlPmyRNcNzgvXbqUyWSWlZV1egsUbMPlcrlMJuv4sQ204erTr1+/lJQUGHwaIwq24TQaDUZoIQl9fX1q3NaGd87wS5cudXpsEqolvKGhQSgU4q4CvMZms6mR8JCQEGUOlZU0fPhwuVxOzHL7rih1+CSVSok7YXAXAl7z8vJS4e1Z77MFCxZ07o34E05M8YcRHNKrz6tXrzQ/9qA6JCQk4C4B/f33342NjcSUxh2HOeF0Ov3tx7k7JycnJy8vb9y4cSrZGlAJFosFnSZV8fHxCQ4OjoqKsrKy6vi7qNMP/+9//0vcrwrIQ1dXlxoJDwkJIcNzcjExMTwe753egv8oXSV4PN6mTZuUv8UPqJaenh41Ei6VSskw/ZaxsfG7XtmlSBvO4XAg3iREmYTHx8dr/oY2hVJSUr7++uuOv54KCZdIJBMnTsRdBVCAMgknz+nYwMBAExOTjs8YRYWEX7t2rUePHrirAAoYGhqqcCIBjEjSDyesXbu24/PhUqEf7u3t3adPH9xVAAWkUik1rpaRpB/eLDk5efDgwcRjlG2jQhvepUsXVV1yA6qlo6MjFotxV6EC5OmHEx4/fhwTE9ORV2p9wgUCwYQJE3BXARTT0dGhxqh45OmHE2bOnNnBW8W0PuGFhYVwFp20mEwmNRJOqn44McrltGnTOvJKrU94jx49IiMjcVcBFGMymdQ4SidbPxwhlJaWFh8f3+7LtD7hgMwMDAyoMQ5HQkICqfrhCKHu3btv37693ZdpfcKjo6MPHDiAuwqgmMJx+LQR3ufDFTI1Nd22bVu7N7FqfcLr6uqo0UpQEoPBIGE2OgHv8+Gt8fDweHt2gDdofcLnzJkzY8YM3FUAxZhMJjUSrqurS8KGJD8/f/Xq1W2/RuvveOHz+QwGA66HkxODwaDGufTY2FjcJSjQtWvXGzdu8Hi8NlpyrW/Df//99zNnzuCuAihGmaN0jc1b9q5iY2PbHuhSW9vwoKCg8vJyuVxOHDtt375dLpfb2dnFxcXhLg38gzIJnzx5Mpbx0ttlYmLS9gu0tQ0fNmyYTCZr2TViMpnwhBnZ6OnpUWPYPA6Ho8l5yzouKytr8eLFbbyAjEV3xPTp0+3s7FoucXR0DA0NxVcRUEAul1NjcsITJ05YWFjgrkIBZ2fnzMzMNl6grQm3sbEZPHhwc9eIwWCMGTPG0NAQd13gX2g0zFNuUB6Hw7lw4UIbv2RtTThC6JNPPrGxsSH+bW9vP3nyZNwVgTfR6XRqPB8eFBREwuvhBBaL1caVPC1OuLW19eDBg4keeEhIiCYnfwUdRJmEk9nOnTuPHDnS2lotTjjRG7e1tbWzs6PGJNXUQ5mEnzhxokuXLrirUMzW1jYvL6+1tSrrJuXcqi99IZBI5LwajT5LVFZWpsNkmpqZaXKnRmY6DB2adVd99z5wp01bUlJSTp06BQ//qZtUKm3tCXYVXA+XiOUnthY6eXLNbFjGXfSkUg1/Z2s02wQGnV5dJqwuk/y5/VXofDuadh8JqRE5rzB1wsyZM7du3Wpubo67EMXaGKBCBQk/sbVwwDhLMxsqXPbsOHM7PYSQsYXumZ1FoQtscZdDUjQajRp3rVZWVpL21h2ZTNa3b9+0tDSFa5X9ir18osJvqNn7Fu9mDu5sJ0/u7b+qcRdCUiR8WqNz9u/fT9oGnE6nm5iYVFVVKV6r5NYfpdbZubzXJ7HtXNiP7tThroK8qHE93NrammxDtbWUlJRk1sqpKKUSXlUicnBjv+e9UH0uw8hCl19H0kM4vChzx8tXX33VWiNJBjKZrLXfs1LpFItkjTz4y0b8OolETIVrQipHmatlL168IPMJhdWrVyclJSlc9X63v0DNKNMP37FjR2uHwWRgZmbW2swT2vr0KNAW1DhK79atG+4S2rJ06dLWVkEbDtSIwWBQ43GgRYsWVVeT94qJRCJpbdRqSDhQI5lMVldHhQsNT548IfPA76dOndq2bZvCVZBwoEaUOZe+bTsz6AcAACAASURBVNs2MvfD9fX1RSKRwlXQDwegfSSfvnr8+PHjx49XuAoSDtRIM214u7MCKO/y5csDBgxgsVjq24VcLlfHkMGQcKD1NDCtiouLi0QiUeuOaDRapxN+7dq1uLi4rVu3vr0K+uFAjShzPdzY2JjMz8m1MaYttOFAjRgMhpWVFe4qVIDM8UYIDRw4cMCAAQpXkbpuoO2kUmlJSQnuKlSgrq6OzLff0mi01r6DIOEA/MuMGTN+/fVX4t+JiYnTpk0rLy9X4cPhubm5QqFQVVsjpKenz58/X+Eqsie8tLSkpLS47dcknI+bEDqirKxUU0WB94Wurq6BgQHxALZKDtQvXry4ZMkSgUCgiur+pbUHY0id8KLiV9PDxj1+/LDtl+nq6rHZJJ2SAmCnzOW6Dz74gBj74e1Thp3bbGv3pSipd+/ee/bsUbgK85m25onHFJJKJG3/Hom3jxj+4YjhH6qnQKB96urqpk2b9sUXXzx//jw1NbV79+6bN29GCP3111+nT5+uqqqytLQcOnRoaGgoMeOSVCo9evTohQsXBAKBj49P8yH01q1bk5OTEUJnz55taGg4evTozZs3Fy5cGBUVVVxcHBER4efnV1paum/fvszMTD09ve7du8+cOdPFxYV4+4MHD44cOZKbm4sQ8vHxCQsLy8/P37lzJ0Jo2rRpCKHFixePHDlSJR9ZLpdLJBIdHZ23V2k64du2/3Tt+qVlS1bv2vO/oqLCLZt39erZp6S0eNeurRn37ujq6rn0cPv886/cXD1KSos//ewjhNC69SvWITR6dPCK5WuvXktet37FhnVbjp+Mzs19MG3qp+UVZYmJ5xBCFxNTiUkYM7PS90XteP78iYmJqb9fQPgX88zMzFes/Dov7+mxo+eIpr6pqWnSx6NCgid9OXcRQiju7KkTJ2MqK8utrGyGD/twyuQZ1Jht63127NixsWPHRkREEGOzHDly5PTp0+PGjXNwcHj16tWpU6eKioqWLVuGENq1a9f58+dHjRrl5eWVkZHRfP/MuHHjZDLZ5cuXm7fZ2Nh4+PDhefPmCQQCX1/f6urqZcuW2djYzJkzh0ajXb58efny5ZGRkU5OTvfu3fvhhx+6du0aHh4uk8nu3LkjkUh69+4dGhp6+vTptWvXstns5vk8lHfv3r29e/f+9ttvb6/C0Ibz+bz9B3ct+nqFQNDU0z+gqqpywcLPbW3t589bRqPRkpL++npR+J5d0ba29qtW/ndjxOrPZs319+ttYmLavIVtv/4U/vm8zz/70s7Woaa2WiaTXbyYQKzKuHd3xXcLR44ImjhhSkN93Z+n/1iybO7e3THBQRPX/LAs635GT/8AhFBKypWmpqaQkEkIoUO//3byVEzoxKmOjt0KC18cP3H4VdHLlSvWa/43Qz10Ol2Ff8fvxM3NbdasWcS/q6qqjh8/vnz58sDAQGKJmZnZjh075syZU1ZWdv78+SlTpnz66acIoREjRvz999/Ea5ydnR0cHIh/m5iY0Gg0kUi0cOFCNzc3YuEff/xhbGwcERFBNC3Dhg0LDw9PTEycM2fO3r17LS0tt2zZoqurixAKDg4m3mJtbY0QcnV1NTIyUuGH1dHReWMav2YYEi4SiZYtWe3u7kX8GB0TZWJs+svm3cSvaeSIoLCZE84lnFkwb5lLDzeEkIODk7e3X8stTJwwZfTo178yC4suTo7/PLv7647NIcGhCxcsJ37s3bvfp599lJZ+e0D/wWZm5hcvJhAJv5ic0LtXXztb+8rKiiNHD6xetXHI4OHEW8zMLP4X+ePyZd+3PS0z6AiZTFZc3M6JUjXx8/vnbyYzM1MikWzevJk4XG/uRVdVVd28eRMh1HLW2jZO6Ojp6TXHmziDXVFR0XI2DrFYXFFRUVpaWlhY+OmnnxLx1gAfHx8fHx+FqzD8EbNYrOZ4I4Tu3LlZXlEWFDyoeYlYLK4ob2uOqJ49+yhcXlpaUlCQX1RUeO6vMy2Xl5eXMRiMoDHjT585tujrFTxeQ8a9uz98vwkhlJFxRyKRbIxYvTFiNfFi4v9eKBRCwrVay3vIiUe7165d+8Z4qdbW1hUVFWw2u92H2Kurq+Vyub6+fsuFNTU1ffr0+eyzz1ouZLPZ5eXlCCGSzFWK4Y9YX9+g5Y/VNVX9+w+aHb6g5UI2m9PGFgz+vYVmNTVVCKFPZ84ePGhYy+WmpuYIoaAxE2KOHLh1+3p5eamJiemA/oMRQlXVlQihiI2RXSz+Nfm7gYHiXQBt1Hy/t729/RurjIyM+Hy+SCTqRHvL4XDq6+vf3iafzyfy39obVf40TmZm5oEDB5ov47eEv5nicg3r6modHJyU3xSHw0UICYUChVuzsrIOCOh/MTmhrKxkbNAEoonmcl9/eaukAEBOvr6+NBrt7Nmz/v7+xJKmpiaiQSYeC7169eqoUaPa2ALRD39joZ+f3+XLl58+fdr8bCmxWTs7O3Nz8+Tk5PHjxxN/ZnK5XC6X0+l04siiurra2NhYhR9QJpO1dhcN/mvIPXv2ycm5//jJo+YlzWPK6emxEEJVlRUd3JSdnYOlpdX5C2ebt/DG6DYhwaGpqSkvXuSNDXrd7/L3D6DRaGdij7+9d0AZNjY248aNu3Pnztq1axMTE48dOxYeHv7s2TOE0KBBg+zt7Xfs2LFv377Lly/v3LlT4ajJCq/pfvLJJ1wud/Xq1ceOHbtw4cLGjRuJfj6NRvv8888LCgqWLFly9uzZc+fOLVmy5OrVqwghDw8PBoOxd+/e5OTkhIQEVX1AHx+fn376SeEq/G34pzNnp6amfLN83uSPw0xMTO/evSWVSf+7/heEUJculjbWtidOxbD09evr60InTm17UzQabd5XS7//4Zt5C2aNC/lIJpUmJp0bOTLoo0nTiRf06xtoamrm5ubZpcvrY3I7W/vQiVP/PP3HytWLAwcOraqqjI078WPENuIkH6CM2bNnW1hYxMfH37t3z9TUdMCAAcSYLQwGY/369bt3705ISDAwMAgMDFR4lruuru7tQ2tra+stW7bs37//xIkTxLn3kJAQYtXQoUP19PSOHj0aFRVlaGjo7OxMXFOwtrZesGDB77//vnfv3u7duwcFBank0+no6JiYmChcpdQD+qUFgmt/VgZ9ofg0vULE9fDTp/41tvPLly92743Mykqn0Wg9erhNnDBl6JARxKpHuQ9+3ryutLS4Sxern378Nffxg3XrV/x+8FTLg+qjfxzaF7Wj+Xp4amrKwUN78vKfsdkcH2//sLAvWsb1wMHdnp6+ffv88yCOXC4/eerImTPHK6sqzMzM+/UN/HTm7JYX59p15teC8XNtjMwV3G/wnrt//350dPSWLVvUuhfizJZaEcfVar1vkkajdfrk3MOHD0+ePPnDDz8o2KyGE05JkPDWZGVl7dixIyoqSq170UDCZTKZum+LVibhGRkZrd3xgr8fDiiMGsMwkv/5cHd395UrVypcReq6gbZr+7kDLVJfX0/mbysDAwMnJ8UXgyDhALRP0t5DUHjl5uZu2rRJ4SpIOFAjyrThRkZGZD5Q5/P5eXl5CleRt2hAAQwGo7WrONqFzJOHE4/ZrFixQuEq/NfDAYVJJJLa2lp178XU9B0ubXbODz/8sHjxYtXeiKZCbDa7tbkTIeFAjTRzlK6BZ4QyMzOlUilpH0bKzc2NjY1V2IzDUTpQI8r0w0k+f3gb/XCSficBaqBMwkk+fzhcDwd4UCbhc+bMUfhECknA9XCAB2USXlhY2NpwxWTw8OHDdevWKVwFCQdqRKfTydx97biDBw++MT4MqTQ1NRUVFSlcpVQ/nIZoOrrwHYH09BiIvPc74SQWi+vq6nBXoQKWlpYdeBU23t7eERERClcplU+2EaOuQi0jvGuXqjIhxwTOWSogk8mocZQ+bdq0yspK3FW0SldXt7VDDOUSbshksekiAXlnbNOAhmqxlROLwaTC37HKyWQykt8N1kF1dXUqnLpM5e7fv79kyRKFq5RKOI2OPAcYpSeR97tNA9ISK30Hk/RWJ+ykUimZb+fuuPj4eDIfqEskkuaJHN6g7G/fJ9DI1Eo39VxHh1KjmGsnS3v4crr7sHEXQlLE8IO4q1ABsVhM5mfLevXqtXv3boWrVNB7DBhpnHGp5tLRYqlUbmmvL2ik/kE7i00vzW9i6tKcfdke/bm4yyEvBoNB5qav40JDQw8ePEjazyKVSoVCocIhwFVzfqjXcBP3PkZVJcL6KrFUot6Enz9/3s7OztvbW617eVtaWlp1dfXo0aMRQkwdelcPU3NbPT19KjRQ6tPU1ETMRqDtOBxST26blZWl9nnLDLh0A64+QvodeK1Sfjt+c1rwcgcHTXd9fQaNTE1N9elnXFJSQsw+BdollUqpcaaNGE2VtGg0Wmu/Z6VGYnw/xcfHFxQUzJ8/H3chWiA2NjYnJ2f16tW4C1FW5yZFIQPyHngoJBAISktL8dYQEhLC4XAqKytbm2UCNJNIJNRowydMmFBW1tZceqSlZQmPj48/duwY7irQrFmzjIyMbt68efbsWdy1kJpEIiHtM9VUkpGRMXv2bIWrtCzhDAZj6NChuKtAxCwTw4YNy8zMJPMjR9gxGAzVTpSNS0JCAmlPpLcN+uHKamxsLCsr4/F4mj+9T34HDx7k8/lwzkIDWjupqU1tuEwmS09Px13FmwwMDBwdHX/55ZcbN27groV0tPcE1RuCgoJI3g9v7XyHNiU8Jydn586duKtQgE6nHzp0iHjEgpg7GhDEYjE1Ek5ymZmZ8+bNU7hKmxIuEok++ugj3FW0KjAwECH0xRdfZGRk4K6FLEQikY4OFaZzI/l96TKZrOUs2i1BP1z1Dh06NGvWLNxVkMLOnTvt7e3HjRuHuxCKk8vlEolE4ZepNrXhKSkpNTU1uKtoHxHvyMjIwsJC3LVgRuZnqt9JSEiIBmY47TQajdbasZI2JXzFihUsFgt3FR01a9asBQsWvOd3xQgEAi36L2uDVCol89EuFfrhNTU1H3/8sb6+2u97VxVjY+PY2FiZTPY+d8sFAoEW/Ze1QXv74VqTcBMTk6+//hp3Fe9MX1/f3t4+MDCwqakJdy0YUKYNJ/m9tz179mztMpPWJDwnJycrKwt3FZ3RpUuX5OTksrIyynRKO04ikVAj4SS/Hk6FfviJEydaGy+W/FgslpOTk1wunzNnDu5aNKq+vp4aR+kMBoPMQ0reu3fvyy+/VLhKaxLu4+PTp08f3FUoxcLC4j//+c/p06dxF6I5PB6Pw+HgrkIF4uPju3TpgruKVsnl8tYGioTr4XhERUWFh4fjrkLthgwZ8tdff1Eg5Np7+612tOFNTU1Hjx7FXYUq0en0ffv24a5C7fh8PgXirRXPh7fWhmtHwh8/fnzp0iXcVajS559/PnLkSITQ06f/1959xzV1tQ8APyGDxCQkQFgBQUBlyhCwgIK1olbUSt2rbqsVV62r1Z/1fd9qrVrLa62rzoqjWqUuXLVSxQEtMmUjiAwhOwTIzu+PW3mphiEkufeG8/2jH5Ob3Ps05Mk5zz33nlOCdizGIpPJvLy80I7CMGg0GpbnacvIyMB3HU6lUqdPn452FAaGLBZ548aNxMREtGMxColE0tDQgHYUhnHhwgU7Ozu0o2iThYUFvs+le3t7x8TEoB2FUSxfvtxcT4WIxWI220zWipDJZFj+MwUHB+N7PPzRo0dlZWVoR2EsH330ETJZwtOnT9GOxZAkEol5TPACAJgyZQqWr0vXarVyuVzvJnxk+I0bN3Bxz0l3zJ0798SJE22tTYNH5tSGs1gsLF/WlpmZuW7dOr2b8JHhdnZ2rq6uaEdhXAQCYceOHVqt9tmzZ2jHYhhyudxs/mpnzpzB8vrhRCKxrROBcDwcc0Qi0cyZMy9evIj36z0TEhJsbW2RGgTvpFIpxpc9aQs+Iu5Rt2dZW1sfO3YsPT1dqcT32uw8Hg/L55/fyrRp03g87C6/qdFompqa9G7CQYYLhcINGzagHYVJOTg4REdHKxSKvXv3oh1L15lThmO8Ds/Kylq1apXeTTjI8Obm5pCQELSjQAGTyWQwGHfv3kU7kC6ysrLC8j3VbwX7dbjehUdhHY4DL1++dHR0fPHiRe/evdGO5e2Eh4enpqaax5onsA43IolEUl5ejnYUqHF0dAQAfPrpp7m5uWjH8hb4fD6LxTKP9IZ1uHGlpaXpXRi5R/nll18KCwvRjuItIF0PtKMwGFiHG5GNjU1oaCjaUaBv8uTJAIBvvvkG7UA65eXLl+7u7mhHYTCwDodMJC8v7+zZs1999RXagXTgyJEjCoVi6dKlaAdiGLAON6KioiKcztBmDP7+/sjYYV5eHtqxtAePpwbbAetwI0pNTX348CHaUWAIMqfCb7/9duHCBbRjaZNOpzOnXjqsw42oT58+vr6+aEeBOatWrcLy3df37983pzYc1uEQajC4TJpQKJw6dert27fRDsRgYB1uRLm5ufgaKDKx2NjYuLg4tKP4h+fPn0dGRqIdhSHBOtyI7t69m5aWhnYU2GVvb48U5BUVFWjH8reioiImk4l2FIYE63Aj6t+/v6enJ9pRYBry5SsqKjpy5AjasQAAQHV1tY+PD9pRGBKsww0vJibmzXldOBzOzZs3UYoIB/bt2zd37tzWf+zRo0dfv37dxGF89NFHn3/+uTmdH4V1uOFFRETodDpCKwAAZAZiqC1Lly4lk8knTpxAHkZGRtbX1+/cudPEYRQXF/fv39/EBzUqWIcb3syZM52cnFo/4+zsPGPGDPQiwgcymTxmzJihQ4eOHDlSqVQSCITU1FSpVGqyAEpKStzd3c3mnhMErMMNz9vbOzg4uKWI0Ol0Q4YM4XK5aMeFAxwOh81mC4VC5GFdXV1ycrLJjl5aWhoREWGyw5kGfutw7GY4Us61NOOwAe+8SZMmtV6nVaVSXbp0yWRHz87OdnZ2NtnhTEMqlWq1WrSjaFNQUFBCQoLeTZjOcC8vr4CAAKQZj4qKcnFxQTsifKioqGj9dSQQCHw+32RzxeTn55vZiXRYhxvR7NmzbW1tnZ2dZ86ciXYsuDFjxgx/f38XFxc6nY78PgqFwqSkJNMcPT8/35zOoiPwW4d3fDqkPK+JXyNvlqHVReEM8f6YRqOVPqaUAr7pD29hAWgMIsfZ0s1Hf52DNc9yG6MHzAvto2lqahKLxTwej8fjNTc3a8SaexeN/gGKxeJxEevvJwmMtH9qLwsak2jvQnVwszTSIfQ6c+aMKQ/3tro4Ht4s01zcW822ozBtyDQGdn/AjMrCwqJBrGqWqRvFqrhPnEkUAtoRtUkqUF/cW2XjaGnLpVKo2I2zO8gUIq9artXoaHRC9ATTTeSK3/HwNjNc3qi5duTloNF2bHtcLoxucPWV8szf+ROWuVhg8rdOIlD/dqpucJwDnWVWw1RteXJHQCSCIeNtTXO42NjYY8eOYXbqWI1Go1Ao9Dbjbf4m/bq/JmQkB6Z3C3tXqt9gm6uHa9AORL+kvVU9J70BAAOH2zbLtLmpEtMcDr91uP4Mry2XWxAJtk4mLXWwz6VfL3G9SsJXoR3I657lNto4Wvac9Eb4hLOz74lNcyxzGw8X1CrtnPG9aJaR2LtSeS8UaEfxOkGt0pbb4/5eLA5Z0azVqE1xY4W5jYc3SdUkCv5OKpgA2ZLYJFOjHcXrmmUasqV5nlprn4UFoVmmMcGB4Hg4BJkz/NbhPatyg6Cuwe94OGzDIahj5laHQxDUGqzDIcicwTocgswZrMMhyJzBOhyCzBmswyHInME6HILMGazDIcicwTocgswZrMOxSKPR5OZmoR1Fz/XsWekH44elPkhBHspksuISvK4wid863JwzfOe3/9mdsA3tKHouEonEYDBJxL/P9Sz8eNr166ab1Nmw8Ht/uLHOtFVVVbq4uBpp5y2QZY/a2qpUYO5Gbmxq/2Ps8g5dXfucPnW55UmlUmnAQ5gYxudpa6cON1iGCwT87/fuzMhII5HJISHv3Lt35+D+RHd3TwDApcu/nDufyOfXOzpyh7/3/tQpH1laWpaUFi1fMX/7tj2HDn9fVlbs4OC0eNGKwYOHInurfVmzb9/ujCdpFIpl/37e8+cv9fbyBQD8d883f9y7s2b1pn0HvquufrFr577eLm5Hju1LS3vQ2Cjr3dttxvR5McPfBwBs37HlbsptAMCw4aEAgNOnLjs5cgEAmVl//Xh4b1lZsbW1TXBQ2MIF8ba22P1tNp55C6a49/Hs08fzYtJZhUJ+/ucbDAbjzQ/Hyoo1YeKIoUNj1ny2CXnj5xtXbVi3hcViI3/0yVNHr1u7OSI8Km5CzJLFK0tKix48SOnXzzt29PhvdvwLALBzxw+hIe9MmzFWJBL+eun8r5fOOzg4nj19Fdmb3u8Gqh+MftOmTcPpPG2GyXCNRvPFxlVCkWDlyg1CIf/Hw3uDg0KR9D5+4tD5XxInfDjNzc3jxYuKn8/9VFVd+cWGfwMAFArFv/6zYfmytU6O3GPHD3y1bePZ01dZLLZAwF++Yr6zc+9l8WsIBMKtW9dWrlp4YN9JZIeNjbIjx/atWrlBLm8eGBxW+7KmsPDp+A8msazY91J/37ptk7Nzbx9vv1kz5vPq62prqz/f8G8AgK0NBwCQ8SR9w+crRsTEfhg3tUEquXDxzOo1Sw7uT6RSe9wEKQCAP/98JFfIt331XVNzE4PBaOvDiRw89OGje1qt1sLCoq7uZVragxs3r0yd8hEA4I97d4hEYmTkUJ1WCwBITDwyfvzkb3cdIBKJbJb1x4uWH/rxe+RYW77csW79sqDAkMmTZpIpf0/+1853A2uwX4cfPHjw0KFDb24yTIYXFOQVlxR+uXn7u0NjAACVlRXXb1xWKpVSqeTU6aObNm4dGj0ceaWtrd13CV8vi1+DPFy+bO17w0YCABYuXLZ4yazsnCfRUe+dTDxszbb5dud+ZHW7ETGxs2bHXU1OWh6/BunsrVm9ycfHH9kD18n5+NHzSCdz9OjxH06MefAgxcfbz8XFlcViC0WCAQOCWuL8fu/OcWMnrFi+DnkYGho+Z96kP/96FDVkmEE+B3whkkj/t3EbjUZDHrb14bwbHXPr1rX8/Fx//8AbN6/odLqr15JeZfhvAwcOsmJaSSRiAICv74CFC+Jb9h8YMLDl395eviQSydaW0/Ln4PN5bX03rJhWJvwYOgW/4+GGyfB6Xh0AgMv9e9UhFxdXrVbb3NyUkZGmVqu3btu0ddvffTxk8mY+rx55SKP+/fVycHBC/uoAgLS0B/W8utixUS37V6lUvPo65N9UKrUlvRGlZcXHTxwsKspHehNCof7p+F++rH3+vLy6+sXVa/9Y/aP+1Z57Gh8f/5b0bufD+WDcRAaDkfogxc8v4ObNK2Ni467fuJyVldG7t1tubta6tZtbXjxw4KDOH72d7wYGM7yn1+HOzr0BALm5Wf37eSNNOodjx2KxBUI+AGDb1gR7u38UMFyuS3lFWetnyCQyAECr1QAAhCJBRETUxwuXt34Bnc5A/kGj/eO36knmn+s3LA8OCl239kt6L/rmLWu1Ov1XJohEAgDAnNkfR0e91/p5G5ueWIe3/nlt/8Mhk8kREdEPHv4xaFBkPa9uzuyPJRLxteQkX98ApIve8mJqqx12qJ3vRvf+t4wC43W4TqdTq9VkMvnNTYbJcK/+PmGh4Yd+3FNXVyuWiB48/GPTxq0AAOarH2NX1z6d3xuTaSWRiDv5lpMnD3O5Ltu2JiBdeto/v2St13tgMJgAAIVC/lbB9BDtfzjvRsfcvp384+G9kRHRdnb248ZN3PR/q58/L0e66J0/Sus/R9e+G2ghEomGHW4wrCdPnrRVhxus17F82VoXF9cXVc/ZLOu93x9DCvLg4DACgZD0688tL2tubu5wVwMHDsrLyy4qLujMuyRScV/P/kh6K5XKpuamlqsLqVSaUChoeeji4urg4Hj9xuWWvanVapUKc5Ofo6L9Dyc0NJxOpxcWPh03biIAICw03N7OoaS0aNi7Izp/CBqVJhD8b+G0rn030HLlyhV7e3u0o+gKw2S4Wq1eumzO0OiYmOGjvb39GhqkMpkMAODi3HvCh9MePrz3xaZPk69fOpl4ZNbsuA4vbJoz+2Mm02rtuvjEU0evJf/65ZZ1W7/e1NaLg4JCH6elJl+/lJqasnZ9fEODtKK8DGkrAgMGNjRId3+37ebNqw8f3iMQCPFLPxMI+PHL5/566fzFi2fjl829dPm8QT4BvGv/w6FQKBER0VyuS2jIO8iLx46dQCKRWnfROzRgQPDjtNTTZ45fuXrx2bPSrn03IL1CQkL279+vd5NheukkEik0JPxk4mG1+u+5xJkM5p7/HunTxyN+6Wp7e4ekpJ///PORrS0nasgwO04Hv4XOXJe9e47uP5hw6vRRAoHQr5/3h3FT23rx/LmfCAX87/fuZDKtxo6ZMGXSrN0J2zKz/hoYHDZiRGxRcf6t29cePb7//qhxkZHRUUOGfb014djxAz/s+5ZOZwQMCA5odb63h2v/w3k3OqavZ/+Wnuro9z94+jTnrbroiz9eIRTyTyYeZrOsly5d7eHRtwvfDbTExcUdOnQIy814W4N5+lcmTL8hVMhB0DCbzh9Ao9Egx9DpdDW11QsXTZsyeda8uUu6ETMWpV/n2zmTAqLYaAfyD/cu8qkMks872IrKBH7ZXTH5UxcG2+g3QWN8ZcKsrKzjx4/rPZ1umI9GoVAsXTbH3t4xMGAgmUzJzc2Uy+Wenv0NsnMIQt25c+fodDraUbRJpVLJ5XK9mwyT4QQCYeSIMb//fvPY8QMUCsXdve+Xm7e/Nu4CQfjFYDDQDqE9QUFBO3bs0LvJMBlOoVCmTvkIuc4JgszPrFmzEhISMHt7GZlM1jsYbuZ3j0KQoQiFQo3GFEsgdk1GRsaWLVv0JDZx4AAAEjBJREFUboIZDkEdO3bsGGYbcABAY2OjRCLRuwnOxAhBHcPsWXREWFiYr6+v3k2wDYegji1cuJDP53fiheig0WhtdTFghkNQx2pqarBch6empsK5ViGo644cOYLlOlwgEEilUr2bYB0OQR1zcnJCO4T2vPfee0OGDNG7CbbhENQxjNfhTCbT1tZW7yaY4RDUMYzX4efOnUtMTNS7CWY4BHXsp59+wnIdzuPx2pqsWn8dTmNYNMmw+4uFIrVKS2Ni7uQFjUFUqfTcI2j2CERApZliClQspzcAYO7cuW3dPaq/Dedwqbwq7E64gaK65812zpib0NvWicKv1n9rkRmT8FVEEoFkaYrJlaZMmYLldcvodHpbM4Lrz3AnD6pGrRPV4XiRCmOoKWtic8hsO/2X+KPIYwBdWKtoauhZ3a7idEmgqW7Ul8lkWF57dOPGjffv39e7qc06/IPF3PTrPAkfTmP2N16VPOeecMxCjI6afBjvnJr0suckedZdIcmSEBDFMs3hkpOTsXzhqlgsbuv+Vv1zvCCapJoL31dxuFQrDpnGwFzxaRoWFoQGkaqpQS3hKT6MdyGbpE/YNRK+6sKeKntXGseZSqGa5zlUEoXAr1KoVVqKJeHdyXYmOy7G50tvR3sZjniW01hfJUexcSgqKqJQKO7u7qgcnUgk0JhEexdqHz/9a0pgTWm2TFCjbJSq0Q7EKGh0Yi8m0d6V6uRu0oWoMD6Lk1KppLxaK+o1HbfMHgF0jwA056/J/e8ZurX1e1PfQTEGHOkbyOgbiHYQkGkNHjw4LS1Nbxejh/a9IeitJCcnox1Cm4RCIZvNbquCwF9dAUFQazY2Njdu3GhrKw4ynMFgtLWuIgSZxsSJE7E8Ht7Oysc4yHBkNBLtEKAerbm5GbPj4efPn9+1a1dbW3FQhzOZzMrKSrSjgHo0LM+X/uLFi3ZubsVBhnt6eqakpKAdBdSjYXm+9NWrV7ezFQe99JCQkKqqqpYV0SDI9OLi4urr69GOQr/GxsZ2KggcZDgAYNSoUTdv3kQ7Cqjnwuz64QqFYsSIEe1cbIePDJ82bdrt27fRjgLquS5cuGBnZ7qLZDuvqqoqKiqqnRd0fNUqRvz88898Pj8+Ph7tQCAIT/DRhgMApk6dWl5efuXKFbQDgXqi2NjYuro6tKPQQywWNzU1tfMC3GQ4AGDXrl35+flZWVloBwJBWDF//vz2L8XBU4YDANavX79nzx44eAaZ2KlTpzBYh0skEisrKzc3t3Zeg5s6vLXPPvssODh41qxZaAcCQViHszYc8e2336pUqkWLFsGrWSHTmD17NgbnS6+pqRGLxe2/BpcZDgCYN2/eJ598Mnbs2EuXLqEdC2T++Hw+BudLnzFjBonUwWWpeM1wAMDAgQNTUlLKysqmT5/+9OlTtMOBzNm5c+fs7e3RjuIfSktLZ86c2eHltLisw19TXFy8f/9+CoUSHx/v6uqKdjiQGZLL5ZaWlti8rK19OG7DW/Tv3/+7774bMWLEypUrt2zZUltbi3ZEkLlZv3491q5LP3bsWGdeZg4ZjoiJiUlKSgoJCVm5cuW6devy8vLQjggyHyUlJWiH8A/nzp3r5C+OOfTS33Tnzp2TJ0/a29uPHDkyJiYG7XAg3JPJZHQ6HTu99Lt374aFhXXmnlbzzHBEdnb2mTNn0tPTJ02aNHHiRMxOhQtBxmM+vfQ3BQYGbt++PSkpydLSct68eRs3brx+/TraQUG4NG/ePOyMh2/YsKG6urqTLzbnDEewWKwFCxYkJyePHz/+wYMHoaGhmzdvfvz4MdpxQXhSV1eHkfHw5ORkBoPh7Ozcydebcy+9LdeuXUtOTubxeMHBwcOHDx80aBDaEUFYJxKJWCyWea5qZK4kEsnt27fv3LlTUFAQExMzatSosLAwtIOCMAoj4+GFhYWWlpZvtcJXz83wFg0NDb/99ltubu7169ejo6OjoqKioqJYLBMtagnhAhbWLSsoKNi6dWtiYuJbvQtm+P8olcp79+6lpqbeu3fPzc1t+PDhYWFhXl5eaMcFoQ8LGf7XX3/5+fnRaLS3ehfMcP1ycnIeP36ckpLC4/EiIyMjIiLCw8PZbBOtRw9hDeqrC2u1WgKB0IUyAWZ4B4RC4cOHDx8/fvzo0SMulzty5EgfH5/Q0FC044J6kFu3bt29e/frr7/uwnthhr+F/Pz83Nzc33//PTMzc9CgQe+8805YWJi3tzfacUFGN3v27N27d3M4HNMfWiKRnD17dvHixV17O8zwrtBoNOnp6WlpaX/++Wdtbe2YMWPc3NxCQ0P79OmDdmiQUYwbN+7IkSNYu4G0M2CGd5dEIsnKynr48GFGRoZUKg0JCQkJCYHZDhnErFmztm/f7uLi0uU9wAw3JIFAkPGKVCoNDw/38/MLDg7u378/2qFB+HP58uVBgwY5Ojp2Zycww41FIBBkZWVlZGRkZmbW1NQEBwcHBQUFBwcHBgaiHRr01qZMmfLDDz+YcrrV4uJigzQMMMNNQSaTZWVlZWZmZmZmqtVqS0vLwMDAoKCgoKAgLC9qCbUw8Xj4J5988vXXXxtkdBZmuKnpdLrMzMzs7Ozs7OysrCx7e/vAwMDAwMCBAwdyuVy0o4P0M+V4eEVFRX19vaFul4AZjrKysjIk25uamrKysgIDAwNeweN9DlB3qNXqlJSUwYMHv+2Fa+2AGY4hQqEwJycnOzs7JycnJyfHz88vICAgJCTEy8urm6dboG4ywWiZSqWKiopKSUmhUqkG3C3McOzKy8vLzs6uq6u7c+eORqPx9/cfMGAA8l8KhYJ2dD2LsevwhoYGkUhkjJmCYYbjA5/Pz83Nzc3NzcvLy83NdXd3j4qK4nK5fn5+ffv2RTs6sxUSEqLT6VquBkeSxcvL68yZM93Z7fvvv3/jxo2WhwkJCZMmTerOoHc7OlgwAcIIDoczbNiwYcOGIQ+LiopKS0szMjJOnz5dXV3t1wrszxuQh4dHeXl5y0MCgcBmsxctWtSdfcbFxbVeLTQ/P5/D4RgpvWEbbg7kcnleXt7TV+zt7dlsNpLtvr6+8Eb37khISDh16lRLjuh0usDAwKNHj3Z5h5cvX96+fbtSqbSwsDh9+nRDQ0O/fv2MOmIKM9zcCASClmzPz8+3srLy9fUNCgrq16+fj4+PYc/imL3KysrVq1dXVFQgD1ks1ueff96d+bmnT5/eeur19PR0Y4+YwF66ubG1tY2Ojo6OjkYeVlVVPX369NmzZ7du3SooKHB2dvb19fXx8fHz8/Px8SESiWjHi2murq6DBw8uLy9HSnFPT8/upPetW7eqqqpaPxMbG9u6IDcG2Ib3LGVlZQUFBfmvREVFWVtb+/j4IGmPdnRYVFlZuWLFiqqqKjqdvnnz5uHDh3d5V3PmzMnLy3ttFgdHR8erV68aIlL9YBves3h6enp6eo4dOxZ5WFxcjKR6UlJSYWEhkurBwcHu7u5w+iqEq6trRETE+fPnPTw8upPe9+/fr6ysJBAIOp1Op9NRKBQOh0On08lkskHjfR1sw6H/QbL95cuXDx8+LCkp8XnF29sbRxNdqJW6BpGqUapplmlUCm33dygSiU6cODFq1KjudHOOHj0qkUioVKoFUevAtXV247i6O3r59jH2pcowwyH9tFptQUFBQUFBYWFhQUFBUVGRt7c3ku1I2qM+tfBrGkTq8rzGoicyhVwnb9SQKESyJQlgK0YAALAgEZWNSrVSQ6WTdBpN30C6ZwDDzsVYlzDBDIc6RafTIamO/LegoCAiIsLOzs77FRQvs5M3ae8n8UU8jZZAZNj2Ytga7KJuY2uWKmW8RrVCSaGA6A9t7VwsDX4ImOFQF5WUlOTn5xe+EhoaymazWxKeTqebJoz0m6LMuyKHfjZsLtM0RzQGmaCZXy7s3Y82fJqBb0GHGQ4ZRkVFReuEt7GxQVId6dgb6cKbC3uqiQw62wnHud1aA6+5tpA3a4NrLyuDjWLCDIeM4sWLF0iqIx37Xr16ebfS4aSlw4YNGzNmzJo1a9p6gU4HDm8qd/Kxw1GfvDPUSs2zx9XT17oybQyT5DDDIVOora0tbIVAILRO+NeupZ8wYUJlZSWJRPL399+9e7eVldWbOzz65XOXQEcKzTyHe8vTq8cvcbJxMMBAGsxwCAU8Hq91wisUitYJv2DBAoFAgLySy+V+8cUX4eHhrd9+ZlcVi2vdy9qcr8DNu1W+7DsD3DUIMxxCn1gsRjrziOrq6tZbORzO5MmTFyxYgDz846JAJCKxuWY+v52iUdX4UjRhmVM39wMzHMKcNxeNsrS09Pf3P3jwoKBW+euBWs9wY91riSm1BXzfUGpgdLdOUsKZwCDMaX23JpFIdHBw8PLyQqYW/uMC397TFu0ATcShn82jq/xu7sQ8T1RA+DVu3DidTsdkMlkslq+v75AhQwIDA5EJEmqfyTU6oo2dWZ08b4cFycK+r01WiiTo3a4347CXDmHOgQMHIiMjAwICXnv+7jmeSEyy6a3n1DrqTp3fXFVTuH7lOcPuVt6g5JfxZn3e9fnbYBsOYc6SJUv0Pl+WI+sT6mzycNBEZVLkjdoGkZpp3cVUhXU4hA/1VYpeLArJssdNWcFyYjwvaOry22EbDuGDoEZBMNqMNEJRzeXrCcVl6WSSpTPXa3TMkt7OvgCAY6fW2nHciERS2l+/qjUqn/6DJ4xbR6P+PVCXlXv71t3DInGtg52HTmeA21T1IpJJdZVy/8gu1iawDYfwoUmqsSAbpUGSSvl7f1zU1CQdH7t6zKhlGo3qh8OLa+vKkK1/PDglFNXMn/VtXOzqnLw7d1KOIc8/yb6ZeG6TFcM2LvYzr37hNS9L2j1I15EsiTKxputvN2gwEGQsDWI1iWKUr+vtP44y6DaL5+0lEkkAgJDA0dsTJqb9dSluzGoAgJ2t64xJ/yIQCK4ufjn5d4tKH48Fy1UqxaXk3R5uwYvmfI/MdccXvDBSkpMtiaI6dZffDjMcwgcdIBDJRumlFxY/FEvqvvjPuy3PaDQqsbQO+TeZTG2Z68KG7VRRmQMAKH+e3dgkjoqc1jKVpYWFsSoICzKR3I2zDzDDIXyg9SLw61TG2HODTODrNWTMyPjWT1It9VwVSySStVoNAEAkeYkkvDHieY2qWUUgdH1IG2Y4hA90K5JGpTDGnnvRrBqbJPZ2fTr/FgbdGgAgaxIbI57XqBUaOqvreQrPtEH4YGVLJpKMMutaP4+wisrsF9UFLc8olM3tv4Xr2I9AsHiSbdypzhFajc7OueuzO8E2HMIHV69eVw7V2HnaGHzPI4YtLCh+8OOJFdGDZzDpNoUlj7RazbyZO9t5izXbcdDAcWkZl9RqhVe/CGkDv6D4AZNhlAvmpXUNzjFdn9oJZjiEDwQLwPXs1cBrZhr6unSOrcuyRT9eubnn9z+OAwLBxcl7cPjkDt8VN+YzEomSmXOzqDTN3TWQ69i/QSYwbGAAAI1KK5epnDy6fic8vC4dwo2CtIanfyk4HtZoB2I6krpGK7py2OQOJr1qB2zDIdzweYd5/1eetQuLSNF//kgsqd+1d/qbz+t0OgB0BIKed40dtTw8NM5QERYUPTj1y2a9mzg2Lnxh1ZvPx45YGjloYls7FJSLouO7tWQCbMMhPHn6WJr7qNnRW3+bptGoJdL6N5/XarXIreZvbupFY1GpBpv4WamUyxqFbWwkAKAn12g0q5bLYF8jqm6gU5UjZ9l3JySY4RDOXPi+xsrZlmymczC2Vpv3cvwSx17Mbl1LA0fLIJyJnefwLL26Ey/Et+q8urAR7G6mN8xwCH9oDOLoeU6VT2rRDsSI6ksEHr7UvkEGKB9gLx3CpbpK5Y2f6t1CTHHdqInVFQu8gqhBQw0zlQ1swyFccnClRH9oU/rghUZlVk1UdW5dHy+SodIbtuEQvjWI1LdO1uuIZI6H4a91MzFxtbRRIIscY+Pub8hFHWGGQ7iX8Zvo0TWBs68tjUWlMlFb5LhrVHKNTNDMrxD2DWBEjrO1pBm4Ww0zHDITT34XP30slTdprZ2ZgEAgW5LIlkQCEYN1qE7ZrFYrNDqdrqFeplFpvEKsgt9lMdhGGf+DGQ6ZlQah+kVxE79GKZOom2VaRVPX5z8yEisbMgA6Optk60Bx8qDauXT9vrHOgBkOQeYMg30YCIIMBmY4BJkzmOEQZM5ghkOQOYMZDkHmDGY4BJmz/wfrgLZevYeThQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from utils.chunk_doc import get_retriever\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain import hub\n",
    "from typing import Annotated, Literal, Sequence, Any, Dict\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from model import llm_selected\n",
    "\n",
    "# Initialize the retriever\n",
    "print(\"Initializing retriever...\")\n",
    "retriever = get_retriever()\n",
    "\n",
    "# Create the retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_documents\",\n",
    "    \"\"\"Search and return relevant documents based on user's query.\"\"\"\n",
    ")\n",
    "\n",
    "# Add the retriever tool to the list of tools\n",
    "tools = [retriever_tool]\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    user_level: str\n",
    "\n",
    "def validate_dsa_question(state) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Determines whether the question is DSA-related, with improved handling of\n",
    "    context-dependent questions and greetings.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state containing messages and user_level\n",
    "        \n",
    "    Returns:\n",
    "        dict[str, Any]: Dictionary containing:\n",
    "            - messages: List of conversation messages\n",
    "            - user_level: User's current level\n",
    "            - next: Literal[\"proceed\", \"redirect\"] indicating next action\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[-1].content\n",
    "    user_level = state[\"user_level\"]\n",
    "    \n",
    "    class ValidationResult(BaseModel):\n",
    "        is_dsa: str = Field(description=\"Binary 'yes' or 'no' for DSA relevance\")\n",
    "        is_greeting: bool = Field(description=\"Whether the input is a greeting\")\n",
    "        redirect_message: str = Field(\n",
    "            description=\"Friendly respond that they cannot answer the question and redirect to DSA topics if needed\",\n",
    "            # default=\"I'd be happy to help you learn about data structures and algorithms. What DSA topic would you like to explore?\"\n",
    "        )\n",
    "        greeting_response: str = Field(\n",
    "            description=\"Friendly greeting response if input is a greeting\",\n",
    "            default=\"Hello! I'm your DSA learning assistant. I'd be happy to help you learn about data structures and algorithms. What topic would you like to explore?\"\n",
    "        )\n",
    "\n",
    "    # Get context from previous messages if they exist\n",
    "    context_messages = messages[-6:-1] if len(messages) > 6 else messages[:-1]\n",
    "    conversation_context = \"\\n\".join([\n",
    "        f\"{'User: ' if isinstance(m, HumanMessage) else 'Assistant: '}{m.content}\"\n",
    "        for m in context_messages\n",
    "    ])\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Analyze if this question is DSA-related or a greeting:\n",
    "\n",
    "Previous conversation:\n",
    "{context}\n",
    "\n",
    "Current input: {question}\n",
    "\n",
    "First check if the input is a greeting:\n",
    "- Common greetings: hi, hello, hey, good morning/afternoon/evening\n",
    "- Variations like \"hi there\", \"hello!\", etc.\n",
    "- If it's a greeting, set is_greeting=true and provide a friendly greeting_response\n",
    "\n",
    "If not a greeting, check if it's DSA-related:\n",
    "- Direct questions about DSA concepts\n",
    "- Follow-up questions about algorithms/data structures\n",
    "- Questions about implementation or complexity\n",
    "\n",
    "Return:\n",
    "1. is_dsa: \"yes\" for DSA questions, \"no\" otherwise\n",
    "2. is_greeting: true for greetings, false otherwise\n",
    "3. greeting_response: friendly greeting if applicable\n",
    "4. redirect_message: friendly suggestion for non-DSA questions\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\", \n",
    "            temperature=0, \n",
    "            streaming=True, \n",
    "            api_key=st.secrets[\"OpenAI_key\"]\n",
    "        )\n",
    "        chain = prompt | model.with_structured_output(ValidationResult)\n",
    "        result = chain.invoke({\n",
    "            \"context\": conversation_context,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # Handle greetings first\n",
    "        if result.is_greeting:\n",
    "            return {\n",
    "                \"messages\": [*messages, AIMessage(content=result.greeting_response)],\n",
    "                \"user_level\": user_level,\n",
    "                \"next\": \"redirect\"  # End after greeting response\n",
    "            }\n",
    "        \n",
    "        # Handle DSA vs non-DSA questions\n",
    "        return {\n",
    "            \"messages\": messages if result.is_dsa.lower() == \"yes\" else [*messages, AIMessage(content=result.redirect_message)],\n",
    "            \"user_level\": user_level,\n",
    "            \"next\": \"proceed\" if result.is_dsa.lower() == \"yes\" else \"redirect\"\n",
    "        }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Validation error: {str(e)}\")\n",
    "        # On error, proceed with original message\n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"user_level\": user_level,\n",
    "            \"next\": \"proceed\"\n",
    "        }\n",
    "\n",
    "def clarify_question(state):\n",
    "    \"\"\"\n",
    "    Clarifies ambiguous questions by maintaining conversation context,\n",
    "    with improved handling of pronoun references to DSA concepts.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== DEBUG: CLARIFY NODE ===\")\n",
    "    messages = state[\"messages\"]\n",
    "    current_question = messages[-1].content\n",
    "    print(f\"Original question: {current_question}\")\n",
    "    \n",
    "    # Get the last 3 exchanges (up to 6 messages) for relevant context\n",
    "    context_messages = messages[-6:-1] if len(messages) > 6 else messages[:-1]\n",
    "    conversation_context = \"\\n\".join([\n",
    "        f\"{'User: ' if isinstance(m, HumanMessage) else 'Assistant: '}{m.content}\"\n",
    "        for m in context_messages\n",
    "    ])\n",
    "    \n",
    "    class ClarificationResult(BaseModel):\n",
    "        needs_clarification: bool = Field(\n",
    "            description=\"Set to true if the question contains pronouns or implicit references to previously discussed DSA concepts\"\n",
    "        )\n",
    "        clarified_question: str = Field(\n",
    "            description=\"The clarified version of the question with pronouns replaced by their referents\",\n",
    "            default=\"\"  # Provide empty string as default\n",
    "        )\n",
    "        referenced_concept: str = Field(\n",
    "            description=\"The main DSA concept being referenced from previous context\",\n",
    "            default=\"\"  # Provide empty string as default\n",
    "        )\n",
    "    \n",
    "    model = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\", \n",
    "        temperature=0, \n",
    "        streaming=True, \n",
    "        api_key=st.secrets[\"OpenAI_key\"]\n",
    "    )\n",
    "    llm_with_clarification = model.with_structured_output(ClarificationResult)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "You are a DSA question processor. Transform user's prompt into clear, context-aware queries.\n",
    "\n",
    "OBJECTIVE: Rewrite user's prompt to include relevant context from chat history while maintaining original intent.\n",
    "\n",
    "Previous conversation:\n",
    "{context}\n",
    "\n",
    "Current question: {question}\n",
    "\n",
    "TRANSFORMATION RULES:\n",
    "1. Replace pronouns with specific references\n",
    "   Before: \"How do I implement it?\"\n",
    "   After: \"How do I implement a binary search tree?\"\n",
    "\n",
    "2. Include relevant context\n",
    "   Before: \"What about the time complexity?\"\n",
    "   After: \"What is the time complexity of quicksort's partitioning step?\"\n",
    "\n",
    "3. Maintain technical precision\n",
    "   Before: \"How does the fast one work?\"\n",
    "   After: \"How does the O(n log n) merge sort algorithm work?\"\n",
    "\n",
    "4. Keep original meaning\n",
    "   Do NOT add assumptions or change the question's scope\n",
    "\n",
    "Return ONLY the reformulated question without explanation.\n",
    "\n",
    "\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm_with_clarification\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke({\n",
    "            \"context\": conversation_context,\n",
    "            \"question\": current_question\n",
    "        })\n",
    "        \n",
    "        # For non-DSA questions or greetings, use original question\n",
    "        if not result.needs_clarification:\n",
    "            print(\"No clarification needed - using original question\")\n",
    "            return {\"messages\": messages, \"user_level\": state[\"user_level\"]}\n",
    "            \n",
    "        # For questions needing clarification, verify we have the clarified version\n",
    "        if result.needs_clarification and result.clarified_question:\n",
    "            print(f\"Referenced concept: {result.referenced_concept}\")\n",
    "            print(f\"Clarified to: {result.clarified_question}\")\n",
    "            return {\"messages\": [HumanMessage(content=result.clarified_question)], \"user_level\": state[\"user_level\"]}\n",
    "        else:\n",
    "            print(\"No clarification needed\")\n",
    "            return {\"messages\": messages, \"user_level\": state[\"user_level\"]}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in clarification: {str(e)}\")\n",
    "        # On error, proceed with original question\n",
    "        return {\"messages\": messages, \"user_level\": state[\"user_level\"]}\n",
    "\n",
    "\n",
    "# def agent(state):\n",
    "#     \"\"\"\n",
    "#     Enhanced agent function with correct retrieval method\n",
    "#     \"\"\"\n",
    "#     print(\"\\n=== DEBUG: AGENT NODE ===\")\n",
    "#     messages = state[\"messages\"]\n",
    "#     question = messages[-1].content\n",
    "#     print(f\"Question received by agent: {question}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Create system message to enforce proper tool usage and response format\n",
    "#         system_message = \"\"\"You are a DSA expert assistant. Follow these steps for EVERY question:\n",
    "#         1. ALWAYS use the retrieve_documents tool first to get information\n",
    "#         2. Wait for the tool's response\n",
    "#         3. Synthesize the retrieved information into a clear explanation\n",
    "#         4. If the retrieved information is insufficient, clearly state that and provide a general explanation\n",
    "        \n",
    "#         IMPORTANT: \n",
    "#         - Always complete the full retrieval and response process\n",
    "#         - Never return just a message about needing to retrieve information\n",
    "#         - Provide complete, informative responses\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Prepare messages with system instruction\n",
    "#         full_messages = [\n",
    "#             HumanMessage(content=system_message),\n",
    "#             *messages\n",
    "#         ]\n",
    "        \n",
    "#         # Initialize model with tools and strict temperature\n",
    "#         model = ChatOpenAI(\n",
    "#             model_name=\"gpt-4o-mini\", \n",
    "#             temperature=0, \n",
    "#             streaming=True, \n",
    "#             api_key=st.secrets[\"OpenAI_key\"]\n",
    "#         )\n",
    "#         model_with_tools = model.bind_tools(tools)\n",
    "        \n",
    "#         # Create a specific prompt to force tool usage\n",
    "#         tool_prompt = HumanMessage(content=f\"\"\"Please provide information about: {question}\n",
    "#         Remember to:\n",
    "#         1. Use the retrieve_documents tool first\n",
    "#         2. Process the retrieved information\n",
    "#         3. Provide a complete response\"\"\")\n",
    "        \n",
    "#         # Get response with explicit tool usage\n",
    "#         response = model_with_tools.invoke([*full_messages, tool_prompt])\n",
    "        \n",
    "#         # Validate response\n",
    "#         if not response.content.strip() or \"need to retrieve\" in response.content.lower():\n",
    "#             print(\"Warning: Invalid response detected, forcing retrieval\")\n",
    "#             # Force direct retrieval as fallback\n",
    "#             docs = retriever.invoke(question)\n",
    "#             if docs:\n",
    "#                 # Process the retrieved documents\n",
    "#                 context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "                \n",
    "#                 # Generate response with retrieved context\n",
    "#                 prompt = PromptTemplate(\n",
    "#                     template=\"\"\"Based on the following information, provide a complete explanation about {question}.\n",
    "                    \n",
    "#                     Retrieved information:\n",
    "#                     {context}\n",
    "                    \n",
    "#                     If the retrieved information is insufficient, provide a general explanation based on your knowledge\n",
    "#                     but clearly state that you're doing so.\n",
    "#                     \"\"\",\n",
    "#                     input_variables=[\"question\", \"context\"]\n",
    "#                 )\n",
    "                \n",
    "#                 chain = prompt | model | StrOutputParser()\n",
    "#                 response_content = chain.invoke({\n",
    "#                     \"question\": question,\n",
    "#                     \"context\": context\n",
    "#                 })\n",
    "#             else:\n",
    "#                 response_content = \"I apologize, but I couldn't find specific information about that. Let me provide a general explanation based on my knowledge.\"\n",
    "#         else:\n",
    "#             response_content = response.content\n",
    "            \n",
    "#         print(f\"Final response length: {len(response_content)}\")\n",
    "#         return {\"messages\": [AIMessage(content=response_content)], \"user_level\": state[\"user_level\"]}\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in agent: {str(e)}\")\n",
    "#         # Fallback to direct explanation\n",
    "#         try:\n",
    "#             print(\"Attempting fallback retrieval...\")\n",
    "#             docs = retriever.invoke(question)\n",
    "#             if docs:\n",
    "#                 context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "#                 return {\n",
    "#                     \"messages\": [AIMessage(content=f\"Here's what I found about {question}: {context}\")],\n",
    "#                     \"user_level\": state[\"user_level\"]\n",
    "#                 }\n",
    "#             else:\n",
    "#                 return {\n",
    "#                     \"messages\": [AIMessage(content=f\"I apologize, but I encountered an issue retrieving specific information about {question}. Please try rephrasing your question.\")],\n",
    "#                     \"user_level\": state[\"user_level\"]\n",
    "#                 }\n",
    "#         except Exception as e2:\n",
    "#             print(f\"Fallback retrieval failed: {str(e2)}\")\n",
    "#             return {\n",
    "#                 \"messages\": [AIMessage(content=\"I encountered an error processing your question. Please try asking in a different way.\")],\n",
    "#                 \"user_level\": state[\"user_level\"]\n",
    "#             }\n",
    "\n",
    "# def rewrite(state):\n",
    "#     \"\"\"Debug version of rewrite with validation\"\"\"\n",
    "#     print(\"\\n=== DEBUG: REWRITE NODE ===\")\n",
    "#     messages = state[\"messages\"]\n",
    "#     question = messages[-1].content\n",
    "#     print(f\"Rewriting question: {question}\")\n",
    "\n",
    "#     try:\n",
    "#         msg = [\n",
    "#             HumanMessage(\n",
    "#                 content=\"\"\"Improve this question to be more specific and searchable:\n",
    "#                 Question: {question}\n",
    "#                 Make it clearly about DSA concepts.\"\"\".format(question=question)\n",
    "#             )\n",
    "#         ]\n",
    "\n",
    "#         model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True, api_key=st.secrets[\"OpenAI_key\"])\n",
    "#         response = model.invoke(msg)\n",
    "#         print(f\"Rewritten as: {response.content}\")\n",
    "#         return {\"messages\": [AIMessage(content=response.content)], \"user_level\": state[\"user_level\"]}\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in rewrite: {str(e)}\")\n",
    "#         return {\"messages\": [HumanMessage(content=question)], \"user_level\": state[\"user_level\"]}\n",
    "\n",
    "# def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "#     \"\"\"Debug version of grading with content validation\"\"\"\n",
    "#     print(\"\\n=== DEBUG: GRADE DOCUMENTS ===\")\n",
    "    \n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "#     question = messages[0].content\n",
    "#     docs = last_message.content\n",
    "    \n",
    "#     print(f\"Grading question: {question}\")\n",
    "#     print(f\"Documents content length: {len(docs) if docs else 'None'}\")\n",
    "    \n",
    "#     # Basic content validation\n",
    "#     if not docs or len(docs.strip()) < 10:\n",
    "#         print(\"---NO VALID DOCS TO GRADE---\")\n",
    "#         return \"rewrite\"\n",
    "        \n",
    "#     try:\n",
    "#         class grade(BaseModel):\n",
    "#             binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "            \n",
    "#         model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True, api_key=st.secrets[\"OpenAI_key\"])\n",
    "#         llm_with_tool = model.with_structured_output(grade)\n",
    "        \n",
    "#         prompt = PromptTemplate(\n",
    "#             template=\"\"\"Grade if this content is relevant to the question.\n",
    "#             Question: {question}\n",
    "#             Content: {context}\n",
    "#             Give 'yes' if there's ANY relevant information about the topic.\"\"\",\n",
    "#             input_variables=[\"context\", \"question\"],\n",
    "#         )\n",
    "        \n",
    "#         chain = prompt | llm_with_tool\n",
    "#         result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "#         print(f\"Grade result: {result.binary_score}\")\n",
    "        \n",
    "#         return \"generate\" if result.binary_score.lower() == \"yes\" else \"rewrite\"\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in grading: {str(e)}\")\n",
    "#         return \"generate\" if docs else \"rewrite\"\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "    \n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"Debug version of generate with content validation\"\"\"\n",
    "    print(\"\\n=== DEBUG: GENERATE NODE ===\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[-1].content\n",
    "    user_level = state[\"user_level\"]\n",
    "    docs = messages[-1].content\n",
    "    \n",
    "    print(f\"Generate received question: {question}\")\n",
    "    print(f\"User level: {user_level}\")\n",
    "    print(f\"Docs length: {len(docs) if docs else 'None'}\")\n",
    "    \n",
    "    if not docs or len(docs.strip()) < 10:\n",
    "        print(\"---NO CONTENT TO GENERATE FROM---\")\n",
    "        return {\"messages\": [AIMessage(content=\"I apologize, but I couldn't find relevant information to answer your question. Please try rephrasing it.\")], \"user_level\": state[\"user_level\"]}\n",
    "        \n",
    "    try:\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True, api_key=st.secrets[\"OpenAI_key\"])\n",
    "        \n",
    "        # prompt = PromptTemplate(\n",
    "        #     template=\"\"\"Answer based on the following context. If context is insufficient, say so clearly.\n",
    "        #     Question: {question}\n",
    "        #     User Level: {user_level}\n",
    "        #     Context: {context}\n",
    "            \n",
    "        #     If you don't find specific information about the topic, you can provide a general explanation based on your knowledge, but clearly state that you're doing so.\"\"\",\n",
    "        #     input_variables=[\"context\", \"question\", \"user_level\"],\n",
    "        # )\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=['context', 'question','user_level'], \n",
    "            input_types={}, \n",
    "            partial_variables={}, \n",
    "            template=\"\"\"You are a DSA expert tutor. Adapt your teaching style to the user's level while maintaining technical accuracy.\n",
    "\n",
    "Current User Level: {user_level}\n",
    "\n",
    "• BEGINNER LEVEL APPROACH:\n",
    "  • Use simple analogies and metaphors\n",
    "    • Compare arrays to parking lots\n",
    "    • Explain stacks like plates in a cafeteria\n",
    "    • Describe trees as family trees\n",
    "  • Focus on basic understanding\n",
    "    • Avoid complexity discussions\n",
    "    • Use step-by-step explanations\n",
    "    • Provide visual examples when possible\n",
    "  • Keep language simple\n",
    "    • Minimize technical jargon\n",
    "    • Use everyday examples\n",
    "    • Explain concepts interactively\n",
    "\n",
    "• INTERMEDIATE LEVEL APPROACH:\n",
    "  • Technical content focus\n",
    "    • Include basic time/space complexity\n",
    "    • Show implementation details\n",
    "    • Provide code examples\n",
    "  • Teaching methods\n",
    "    • Compare different approaches\n",
    "    • Explain basic trade-offs\n",
    "    • Connect related concepts\n",
    "  • Code implementation\n",
    "    • Show practical examples\n",
    "    • Discuss common patterns\n",
    "    • Address basic optimizations\n",
    "\n",
    "• ADVANCED LEVEL APPROACH:\n",
    "  • Technical depth\n",
    "    • Deep optimization discussions\n",
    "    • Thorough edge case analysis\n",
    "    • Performance considerations\n",
    "  • System considerations\n",
    "    • Memory/cache implications\n",
    "    • Concurrency aspects\n",
    "    • Scalability factors\n",
    "  • Implementation focus\n",
    "    • Advanced optimization techniques\n",
    "    • System design impacts\n",
    "    • Complex trade-offs\n",
    "\n",
    "• UNIVERSAL RULES:\n",
    "  • Stay within DSA scope\n",
    "    • Redirect non-DSA questions politely\n",
    "    • Focus on one concept at a time\n",
    "    • Offer to explore related topics after\n",
    "  • Use context appropriately\n",
    "    • Start with provided context: \"{context}\"\n",
    "    • Clearly indicate when using general knowledge\n",
    "    • Stay within user's competency level\n",
    "  • Maintain level-appropriate depth\n",
    "    • Match technical depth to user level\n",
    "    • Scale example complexity appropriately\n",
    "    • Use suitable terminology\n",
    "\n",
    "Question: {question}\"\"\")\n",
    "        # prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        \n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        response = chain.invoke({\n",
    "            \"context\": docs,\n",
    "            \"question\": question,\n",
    "            \"user_level\": user_level\n",
    "        })\n",
    "        \n",
    "        print(f\"Generated response length: {len(response)}\")\n",
    "        return {\"messages\": [AIMessage(content=response)], \"user_level\": state[\"user_level\"]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate: {str(e)}\")\n",
    "        return {\"messages\": [AIMessage(content=\"I encountered an error generating a response. Please try asking your question again.\")], \"user_level\": state[\"user_level\"]}\n",
    "\n",
    "################################################################################################################\n",
    "# Graph setup\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes - remove the rewrite node\n",
    "workflow.add_node(\"clarify\", clarify_question)\n",
    "workflow.add_node(\"validate_topic\", validate_dsa_question)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"clarify\")\n",
    "workflow.add_edge(\"clarify\", \"validate_topic\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate_topic\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"proceed\": \"agent\",\n",
    "        \"redirect\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retrieve\",\n",
    "#     grade_documents,\n",
    "    \n",
    "#     {\n",
    "#         \"generate\": \"generate\",\n",
    "#         \"rewrite\": \"agent\"  # Changed from \"rewrite\" to \"agent\"\n",
    "#     }\n",
    "# )\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile graph\n",
    "from test_templates.memory import memory\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG: CLARIFY NODE ===\n",
      "Original question: what is Insertion Sort\n",
      "Referenced concept: Insertion Sort\n",
      "Clarified to: What is the Insertion Sort algorithm and how does it work?\n",
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\n",
      "=== DEBUG: GENERATE NODE ===\n",
      "Generate received question: As in selection sort, the items to the left of the current index are in sorted order dur\n",
      "ing the sort, but they are not in their final position, as they may have to be moved to\n",
      "make room for smaller items encountered later. The array is, however, fully sorted when\n",
      "the index reaches the right end.\n",
      "\n",
      "Unlike that of selection sort, the running time of insertion sort depends on the ini\n",
      "tial order of the items in the input. For example, if the array is large and its entries are\n",
      "already in order (or nearly in order), then insertion sort is much, much faster than if\n",
      "the entries are randomly ordered or in reverse order.\n",
      "\n",
      "\n",
      "**Proposition B.** Insertion sort uses �N [2]/4 compares and �N [2]/4 exchanges to sort\n",
      "a randomly ordered array of length N with distinct keys, on the average. The worst\n",
      "case is �N [2]/2 compares and �N [2]/2 exchanges and the best case is N � 1 compares\n",
      "and 0 exchanges.\n",
      "\n",
      "**Proof:** Just as for Proposition A, the number of compares and exchanges is easy to\n",
      "visualize in the N-by-N diagram that we use to illustrate the sort. We count entries\n",
      "below the diagonal—all of them, in the worst case, and none of them, in the best\n",
      "case. For randomly ordered arrays, we expect each item to go about halfway back,\n",
      "on the average, so we count one-half of the entries below the diagonal.\n",
      "\n",
      "The number of compares is the number of exchanges plus an additional term\n",
      "\n",
      "equal to N minus the number of times the item inserted is the smallest so far. In the\n",
      "worst case (array in reverse order), this term is negligible in relation to the total; in\n",
      "the best case (array in order) it is equal to N � 1.\n",
      "\n",
      "_CHAPTER 8. SORTING_ 67\n",
      "\n",
      "###### 8.4 Insertion Sort\n",
      "\n",
      "Insertion sort is a somewhat interesting algorithm with an expensive runtime of\n",
      "_O(n[2]). It can be best thought of as a sorting scheme similar to that of sorting_\n",
      "a hand of playing cards, i.e. you take one card and then look at the rest with\n",
      "the intent of building up an ordered set of cards in your hand.\n",
      "\n",
      "4 75 74\n",
      "\n",
      "4 75 74 2 54 4 75 74 2 54 4 75 74 2 54\n",
      "\n",
      "2 54\n",
      "\n",
      "4 74 75 2 54 2 4 74 75 54 2 4 54 74 75\n",
      "\n",
      "Figure 8.4: Insertion Sort Iterations\n",
      "\n",
      "1) algorithm Insertionsort(list)\n",
      "2) **Pre:** _list_ =\n",
      "_̸_ _∅_\n",
      "3) **Post: list has been sorted into values of ascending order**\n",
      "4) _unsorted_ 1\n",
      "_←_\n",
      "5) **while unsorted < list.Count**\n",
      "6) _hold_ _list[unsorted]_\n",
      "_←_\n",
      "7) _i_ _unsorted_ 1\n",
      "_←_ _−_\n",
      "8) **while i** 0 and hold < list[i]\n",
      "_≥_\n",
      "9) _list[i + 1]_ _list[i]_\n",
      "_←_\n",
      "10) _i_ _i_ 1\n",
      "_←_ _−_\n",
      "11) **end while**\n",
      "12) _list[i + 1]_ _hold_\n",
      "_←_\n",
      "13) _unsorted_ _unsorted + 1_\n",
      "_←_\n",
      "14) **end while**\n",
      "15) **return list**\n",
      "16) end Insertionsort\n",
      "\n",
      "|4|Col2|\n",
      "|---|---|\n",
      "|||\n",
      "\n",
      "|75|Col2|\n",
      "|---|---|\n",
      "|||\n",
      "\n",
      "|4|75|74|2|54|\n",
      "|---|---|---|---|---|\n",
      "\n",
      "|4|75|74|2|54|\n",
      "|---|---|---|---|---|\n",
      "\n",
      "|4|75|74|2|54|\n",
      "|---|---|---|---|---|\n",
      "||||||\n",
      "\n",
      "|2|4|54|74|75|\n",
      "|---|---|---|---|---|\n",
      "\n",
      "|4|74|75|2|54|\n",
      "|---|---|---|---|---|\n",
      "||||||\n",
      "\n",
      "|2|4|74|75|54|\n",
      "|---|---|---|---|---|\n",
      "||||||\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_CHAPTER 8. SORTING_ 68\n",
      "\n",
      "###### 8.5 Shell Sort\n",
      "\n",
      "Put simply shell sort can be thought of as a more efficient variation of insertion\n",
      "sort as described in 8.4, it achieves this mainly by comparing items of varying\n",
      "_§_\n",
      "distances apart resulting in a run time complexity of O(n log[2] _n)._\n",
      "\n",
      "Shell sort is fairly straight forward but may seem somewhat confusing at\n",
      "first as it differs from other sorting algorithms in the way it selects items to\n",
      "compare. Figure 8.5 shows shell sort being ran on an array of integers, the red\n",
      "coloured square is the current value we are holding.\n",
      "\n",
      "// This code is contributed by itsok.\n",
      "/* Function to sort an array using insertion sort*/\n",
      "function insertionSort(A, size)\n",
      "{\n",
      "   var i, key, j;\n",
      "   for (i = 1; i < size; i++)\n",
      "   {\n",
      "       key = A[i];\n",
      "       j = i-1;\n",
      "\n",
      "       /* Move elements of A[0..i-1], that are \n",
      "          greater than key, to one \n",
      "          position ahead of their current position.\n",
      "          This loop will run at most k times */\n",
      "       while (j >= 0 && A[j] > key)\n",
      "       {\n",
      "           A[j+1] = A[j];\n",
      "           j = j-1;\n",
      "       }\n",
      "       A[j+1] = key;\n",
      "   }\n",
      "}\n",
      "\n",
      "// This code is contributed by itsok.\n",
      "2 3 5 6 8 9 10 \n",
      "```\n",
      "\n",
      "\n",
      "#### Expected Approach – Using Heap – O(k + (n-k)*Log k) Time and O(k) Space\n",
      "\n",
      "\n",
      "Follow the below steps to solve the problem:\n",
      "\n",
      "\n",
      "\n",
      "- Create a Min Heap of size K+1 with the first K+1 elements. We are creating a heap of size K as the element can be at most K distance from its index in a sorted array.\n",
      "\n",
      "- One by one remove the min element from the heap, put it in the result array, and add a new element to the heap from the remaining elements.\n",
      "\n",
      "\n",
      "Below is the implementation of the above approach:\n",
      "\n",
      "\n",
      "```cpp\n",
      "#include <bits/stdc++.h>\n",
      "using namespace std;\n",
      "\n",
      "// Sorts a nearly sorted array where every element is at most\n",
      "// k positions away from its target position.\n",
      "void sortK(vector<int>& arr, int k)\n",
      "{\n",
      "    int n = arr.size();  \n",
      "\n",
      "    // Size of priority queue or min heap\n",
      "    int pqSz = (n == k) ? k : k + 1; \n",
      "    \n",
      "    // Min-heap to store the first k+1 elements\n",
      "    priority_queue<int, vector<int>, greater<int>> \n",
      "                    pq(arr.begin(), arr.begin() + pqSz);\n",
      "\n",
      "    int idx = 0;\n",
      "\n",
      "    // Process remaining elements\n",
      "    for (int i = k + 1; i < n; i++) {\n",
      "        arr[idx++] = pq.top();\n",
      "        pq.pop();\n",
      "        pq.push(arr[i]);\n",
      "    }\n",
      "\n",
      "    // Place remaining elements from the heap into \n",
      "    // the array\n",
      "    while (!pq.empty()) {\n",
      "        arr[idx++] = pq.top();\n",
      "        pq.pop();\n",
      "    }\n",
      "}\n",
      "\n",
      "**Bubble Sort** This is stable because no item is swapped past another unless they are\n",
      "in the wrong order. So items with identical keys will have their original\n",
      "order preserved.\n",
      "\n",
      "**Insertion Sort** This is stable because no item is swapped past another unless it has a\n",
      "smaller key. So items with identical keys will have their original order\n",
      "preserved.\n",
      "\n",
      "**Selection Sort** This is not stable, because there is nothing to stop an item being swapped\n",
      "past another item that has an identical key. For example, the array\n",
      "\n",
      "[21, 22, 13] would be sorted to [13, 22, 21] which has items 22 and 21 in the\n",
      "wrong order.\n",
      "\n",
      "The issue of sorting stability needs to be considered when developing more complex sorting\n",
      "algorithms. Often there are stable and non-stable versions of the algorithms, and one has to\n",
      "consider whether the extra cost of maintaining stability is worth the effort.\n",
      "\n",
      "##### 9.9 Treesort\n",
      "\n",
      "|1|4|3|2|\n",
      "|---|---|---|---|\n",
      "\n",
      "|1|3|4|2|\n",
      "|---|---|---|---|\n",
      "\n",
      "|1|2|3|4|\n",
      "|---|---|---|---|\n",
      "\n",
      "\n",
      "_n−1_\n",
      "�\n",
      "\n",
      "_i=1_\n",
      "\n",
      "\n",
      "_i_\n",
      "�\n",
      "\n",
      "1 =\n",
      "\n",
      "_j=1_\n",
      "\n",
      "\n",
      "_n−1_\n",
      "�\n",
      "\n",
      "_i_\n",
      "\n",
      "_i=1_\n",
      "\n",
      "\n",
      "= 1 + 2 + + (n 1)\n",
      "_· · ·_ _−_\n",
      "\n",
      "_n(n_ 1)\n",
      "_−_\n",
      "= ;\n",
      "\n",
      "2\n",
      "\n",
      "68\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "and in the average case it is half that, namely n(n 1)/4. Thus average and worst case\n",
      "_−_\n",
      "number of steps for of Insertion Sort are both proportional to n[2], and hence the average and\n",
      "worst case time complexities are both O(n[2]).\n",
      "\n",
      "##### 9.6 Selection Sort\n",
      "\n",
      "_Selection Sort is (not surprisingly) a form of selection sorting. It first finds the smallest item_\n",
      "and puts it into a[0] by exchanging it with whichever item is in that position already. Then\n",
      "it finds the second-smallest item and exchanges it with the item in a[1]. It continues this\n",
      "way until the whole array is sorted. More generally, at the ith stage, Selection Sort finds the\n",
      "_ith-smallest item and swaps it with the item in a[i-1]. Obviously there is no need to check_\n",
      "for the ith-smallest item in the first i 1 elements of the array.\n",
      "_−_\n",
      "For the example starting array 4 1 3 2, Selection Sort first finds the smallest item in\n",
      "the whole array, which is a[1]=1, and swaps this value with that in a[0] giving 1 4 3 2 .\n",
      "Then, for the second step, it finds the smallest item in the reduced array a[1],a[2],a[3],\n",
      "that is a[3]=2, and swaps that into a[1], giving 1 2 3 4 . Finally, it finds the smallest\n",
      "of the reduced array a[2],a[3], that is a[2]=3, and swaps that into a[2], or recognizes that\n",
      "a swap is not needed, giving 1 2 3 4 .\n",
      "The general algorithm for Selection Sort can be written:\n",
      "```\n",
      " for ( i = 0 ; i < n-1 ; i++ ) {\n",
      "   k = i\n",
      "   for ( j = i+1 ; j < n ; j++ )\n",
      "    if ( a[j] < a[k] )\n",
      "      k = j\n",
      "   swap a[i] and a[k]\n",
      " }\n",
      "\n",
      "[21, 22, 13] would be sorted to [13, 22, 21] which has items 22 and 21 in the\n",
      "wrong order.\n",
      "\n",
      "The issue of sorting stability needs to be considered when developing more complex sorting\n",
      "algorithms. Often there are stable and non-stable versions of the algorithms, and one has to\n",
      "consider whether the extra cost of maintaining stability is worth the effort.\n",
      "\n",
      "##### 9.9 Treesort\n",
      "\n",
      "Let us now consider a way of implementing an insertion sorting algorithm using a data\n",
      "structure better suited to the problem. The idea here, which we have already seen before,\n",
      "involves inserting the items to be sorted into an initially empty binary search tree. Then,\n",
      "when all items have been inserted, we know that we can traverse the binary search tree to\n",
      "visit all the items in the right order. This sorting algorithm is called Treesort, and for the\n",
      "basic version, we require that all the search keys be different.\n",
      "Obviously, the tree must be kept balanced in order to minimize the number of comparisons,\n",
      "since that depends on the height of the tree. For a balanced tree that is O(log2 n). If the tree\n",
      "is not kept balanced, it will be more than that, and potentially O(n).\n",
      "Treesort can be difficult to compare with other sorting algorithms, since it returns a tree,\n",
      "rather than an array, as the sorted data structure. It should be chosen if it is desirable\n",
      "to have the items stored in a binary search tree anyway. This is usually the case if items\n",
      "are frequently deleted or inserted, since a binary search tree allows these operations to be\n",
      "implemented efficiently, with time complexity O(log2 n) per item. Moreover, as we have seen\n",
      "before, searching for items is also efficient, again with time complexity O(log2 n).\n",
      "\n",
      "71\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "# Move elements of A[0..i-1], that are\n",
      "        # greater than key, to one position\n",
      "        # ahead of their current position.\n",
      "        # This loop will run at most k times\n",
      "        while j >= 0 and A[j] > key:\n",
      "            A[j + 1] = A[j]\n",
      "            j = j - 1\n",
      "        A[j + 1] = key\n",
      "# Function to sort an array using\n",
      "# insertion sort\n",
      "\n",
      "\n",
      "def insertionSort(A, size):\n",
      "    i, key, j = 0, 0, 0\n",
      "    for i in range(size):\n",
      "        key = A[i]\n",
      "        j = i-1\n",
      "\n",
      "        # Move elements of A[0..i-1], that are\n",
      "        # greater than key, to one position\n",
      "        # ahead of their current position.\n",
      "        # This loop will run at most k times\n",
      "        while j >= 0 and A[j] > key:\n",
      "            A[j + 1] = A[j]\n",
      "            j = j - 1\n",
      "        A[j + 1] = key\n",
      "/* C# Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that are greater than\n",
      "           key, to one position ahead of their current\n",
      "           position. This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* C# Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that are greater than\n",
      "           key, to one position ahead of their current\n",
      "           position. This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "function insertionSort(A, size)\n",
      "{\n",
      "   var i, key, j;\n",
      "   for (i = 1; i < size; i++)\n",
      "   {\n",
      "       key = A[i];\n",
      "       j = i-1;\n",
      "\n",
      "Insertion sort is an efficient method for such arrays; selection sort is not. Indeed, when\n",
      "the number of inversions is low, insertion sort is likely to be faster than any sorting\n",
      "method that we consider in this chapter.\n",
      "\n",
      "\n",
      "**Proposition C.** The number of exchanges used by insertion sort is equal to the\n",
      "number of inversions in the array, and the number of compares is at least equal to\n",
      "the number of inversions and at most equal to the number of inversions plus the\n",
      "array size minus 1.\n",
      "\n",
      "**Proof:** Every exchange involves two inverted adjacent entries and thus reduces the\n",
      "number of inversions by one, and the array is sorted when the number of inversions reaches zero. Every exchange corresponds to a compare, and an additional\n",
      "compare might happen for each value of i from 1 to N-1 (when a[i] does not\n",
      "reach the left end of the array).\n",
      "\n",
      "\n",
      "It is not difficult to speed up insertion sort substantially, by shortening its inner loop to\n",
      "move the larger entries to the right one position rather than doing full exchanges (thus\n",
      "cutting the number of array accesses in half). We leave this improvement for an exercise\n",
      "(see Exercise 2.1.25).\n",
      "\n",
      "In summary, insertion sort is an excellent method for partially sorted arrays and is also\n",
      "a fine method for tiny arrays. These facts are important not just because such arrays\n",
      "frequently arise in practice, but also because both types of arrays arise in intermediate\n",
      "stages of advanced sorting algorithms, so we will be considering insertion sort again in\n",
      "relation to such algorithms.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "###### Visualizing sorting algorithms Throughout this chapter, we will be using\n",
      "\n",
      "a simple visual representation to help describe the properties of sorting algorithms.\n",
      "Rather than tracing the progress of a sort with key values such as letters, numbers,\n",
      "or words, we use vertical bars, to be sorted by their\n",
      "heights. The advantage of such a representation is\n",
      "that it can give insights into the behavior of a sorting method.\n",
      "\n",
      "The number of compares is the number of exchanges plus an additional term\n",
      "\n",
      "equal to N minus the number of times the item inserted is the smallest so far. In the\n",
      "worst case (array in reverse order), this term is negligible in relation to the total; in\n",
      "the best case (array in order) it is equal to N � 1.\n",
      "\n",
      "\n",
      "Insertion sort works well for certain types of nonrandom arrays that often arise in\n",
      "practice, even if they are huge. For example, as just mentioned, consider what happens\n",
      "when you use insertion sort on an array that is already sorted. Each item is immediately\n",
      "determined to be in its proper place in the array, and the total running time is linear.\n",
      "(The running time of selection sort is quadratic for such an array.) The same is true\n",
      "for arrays whose keys are all equal (hence the condition in Proposition B that the keys\n",
      "must be distinct).\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "-----\n",
      "\n",
      "More generally, we consider the concept of a partially sorted array, as follows: An in\n",
      "_version is a pair of entries that are out of order in the array. For instance, E X A M P L E_\n",
      "has 11 inversions: E-A, X-A, X-M, X-P, X-L, X-E, M-L, M-E, P-L, P-E, and L-E. If the\n",
      "number of inversions in an array is less than a constant multiple of the array size, we\n",
      "say that the array is partially sorted. Typical examples of partially sorted arrays are the\n",
      "following:\n",
      "\n",
      "  - An array where each entry is not far from its final position\n",
      "\n",
      "  - A small array appended to a large sorted array\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "An array with only a few entries that are not in place\n",
      "\n",
      "\n",
      "Insertion sort is an efficient method for such arrays; selection sort is not. Indeed, when\n",
      "the number of inversions is low, insertion sort is likely to be faster than any sorting\n",
      "method that we consider in this chapter.\n",
      "\n",
      "\n",
      "**Proposition C.** The number of exchanges used by insertion sort is equal to the\n",
      "number of inversions in the array, and the number of compares is at least equal to\n",
      "the number of inversions and at most equal to the number of inversions plus the\n",
      "array size minus 1.\n",
      "\n",
      "/* Move elements of A[0..i-1], that\n",
      "            are greater than key, to one\n",
      "            position ahead of their current position.\n",
      "            This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that\n",
      "            are greater than key, to one\n",
      "            position ahead of their current position.\n",
      "            This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "# Function to sort an array using\n",
      "# insertion sort\n",
      "\n",
      "\n",
      "def insertionSort(A, size):\n",
      "    i, key, j = 0, 0, 0\n",
      "    for i in range(size):\n",
      "        key = A[i]\n",
      "        j = i-1\n",
      "\n",
      "        # Move elements of A[0..i-1], that are\n",
      "        # greater than key, to one position\n",
      "        # ahead of their current position.\n",
      "        # This loop will run at most k times\n",
      "        while j >= 0 and A[j] > key:\n",
      "            A[j + 1] = A[j]\n",
      "            j = j - 1\n",
      "        A[j + 1] = key\n",
      "# Function to sort an array using\n",
      "# insertion sort\n",
      "\n",
      "\n",
      "def insertionSort(A, size):\n",
      "    i, key, j = 0, 0, 0\n",
      "    for i in range(size):\n",
      "        key = A[i]\n",
      "        j = i-1\n",
      "\n",
      "        # Move elements of A[0..i-1], that are\n",
      "        # greater than key, to one position\n",
      "        # ahead of their current position.\n",
      "        # This loop will run at most k times\n",
      "        while j >= 0 and A[j] > key:\n",
      "            A[j + 1] = A[j]\n",
      "            j = j - 1\n",
      "        A[j + 1] = key\n",
      "/* C# Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "/* Move elements of A[0..i-1], that are\n",
      "           greater than key, to one\n",
      "           position ahead of their current position.\n",
      "           This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that\n",
      "            are greater than key, to one\n",
      "            position ahead of their current position.\n",
      "            This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that\n",
      "            are greater than key, to one\n",
      "            position ahead of their current position.\n",
      "            This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "# Function to sort an array using\n",
      "# insertion sort\n",
      "\n",
      "\n",
      "def insertionSort(A, size):\n",
      "    i, key, j = 0, 0, 0\n",
      "    for i in range(size):\n",
      "        key = A[i]\n",
      "        j = i-1\n",
      "\n",
      "        # Move elements of A[0..i-1], that are\n",
      "        # greater than key, to one position\n",
      "        # ahead of their current position.\n",
      "        # This loop will run at most k times\n",
      "        while j >= 0 and A[j] > key:\n",
      "            A[j + 1] = A[j]\n",
      "            j = j - 1\n",
      "        A[j + 1] = key\n",
      "# Function to sort an array using\n",
      "# insertion sort\n",
      "\n",
      "\n",
      "def insertionSort(A, size):\n",
      "    i, key, j = 0, 0, 0\n",
      "    for i in range(size):\n",
      "        key = A[i]\n",
      "        j = i-1\n",
      "\n",
      "# Move elements of A[0..i-1], that are\n",
      "        # greater than key, to one position\n",
      "        # ahead of their current position.\n",
      "        # This loop will run at most k times\n",
      "        while j >= 0 and A[j] > key:\n",
      "            A[j + 1] = A[j]\n",
      "            j = j - 1\n",
      "        A[j + 1] = key\n",
      "/* C# Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that are greater than\n",
      "           key, to one position ahead of their current\n",
      "           position. This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* C# Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that are greater than\n",
      "           key, to one position ahead of their current\n",
      "           position. This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "function insertionSort(A, size)\n",
      "{\n",
      "   var i, key, j;\n",
      "   for (i = 1; i < size; i++)\n",
      "   {\n",
      "       key = A[i];\n",
      "       j = i-1;\n",
      "\n",
      "       /* Move elements of A[0..i-1], that are \n",
      "          greater than key, to one \n",
      "          position ahead of their current position.\n",
      "          This loop will run at most k times */\n",
      "       while (j >= 0 && A[j] > key)\n",
      "       {\n",
      "           A[j+1] = A[j];\n",
      "           j = j-1;\n",
      "       }\n",
      "       A[j+1] = key;\n",
      "   }\n",
      "}\n",
      "\n",
      "// This code is contributed by itsok.\n",
      "/* Function to sort an array using insertion sort*/\n",
      "function insertionSort(A, size)\n",
      "{\n",
      "   var i, key, j;\n",
      "   for (i = 1; i < size; i++)\n",
      "   {\n",
      "       key = A[i];\n",
      "       j = i-1;\n",
      "\n",
      "void insertionSort(vector<int>& arr) {\n",
      "    \n",
      "    // Traverse through 1 to size of the array\n",
      "    for (int i = 1; i < arr.size(); i++) {\n",
      "        \n",
      "        // Move elements of arr[0..i-1], that are greater than key\n",
      "        // to one position ahead of their current position\n",
      "        int key = arr[i];\n",
      "        int j = i - 1;\n",
      "        while (j >= 0 && arr[j] > key) {\n",
      "            arr[j + 1] = arr[j];\n",
      "            j--;\n",
      "        }\n",
      "        arr[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    vector<int> arr = {6, 5, 3, 2, 8, 10, 9};\n",
      "    insertionSort(arr);    \n",
      "    for (int x : arr)\n",
      "        cout << x << \" \";\n",
      "    return 0;\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that are\n",
      "           greater than key, to one\n",
      "           position ahead of their current position.\n",
      "           This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "        /* Move elements of A[0..i-1], that are\n",
      "           greater than key, to one\n",
      "           position ahead of their current position.\n",
      "           This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "static void insertionSort(int A[], int size)\n",
      "{\n",
      "    int i, key, j;\n",
      "    for (i = 1; i < size; i++) {\n",
      "        key = A[i];\n",
      "        j = i - 1;\n",
      "\n",
      "Because we are using an array we need some way to calculate the index of a\n",
      "parent node, and the children of a node. The required expressions for this are\n",
      "defined as follows for a node at index:\n",
      "\n",
      "1. (index 1)/2 (parent index)\n",
      "_−_\n",
      "\n",
      "2. 2 _index + 1 (left child)_\n",
      "_∗_\n",
      "\n",
      "3. 2 _index + 2 (right child)_\n",
      "_∗_\n",
      "\n",
      "In Figure 4.4 a) represents the calculation of the right child of 12 (2 0 + 2);\n",
      "_∗_\n",
      "and b) calculates the index of the parent of 3 ((3 1)/2).\n",
      "_−_\n",
      "\n",
      "###### 4.1 Insertion\n",
      "\n",
      "Designing an algorithm for heap insertion is simple, but we must ensure that\n",
      "heap order is preserved after each insertion. Generally this is a post-insertion\n",
      "operation. Inserting a value into the next free slot in an array is simple: we just\n",
      "need to keep track of the next free index in the array as a counter, and increment\n",
      "it after each insertion. Inserting our value into the heap is the first part of the\n",
      "algorithm; the second is validating heap order. In the case of min-heap ordering\n",
      "this requires us to swap the values of a parent and its child if the value of the\n",
      "child is < the value of its parent. We must do this for each subtree containing\n",
      "the value we just inserted.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_CHAPTER 4. HEAP_ 34\n",
      "\n",
      "Figure 4.3: Converting a tree data structure to its array counterpart\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_CHAPTER 4. HEAP_ 35\n",
      "\n",
      "Figure 4.4: Calculating node properties\n",
      "\n",
      "The run time efficiency for heap insertion is O(log n). The run time is a\n",
      "by product of verifying heap order as the first part of the algorithm (the actual\n",
      "insertion into the array) is O(1).\n",
      "\n",
      "Figure 4.5 shows the steps of inserting the values 3, 9, 12, 7, and 1 into a\n",
      "min-heap.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_CHAPTER 4. HEAP_ 36\n",
      "\n",
      "Figure 4.5: Inserting values into a min-heap\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_CHAPTER 4. HEAP_ 37\n",
      "\n",
      "1) algorithm Add(value)\n",
      "2) **Pre: value is the value to add to the heap**\n",
      "3) Count is the number of items in the heap\n",
      "4) **Post: the value has been added to the heap**\n",
      "5) _heap[Count]_ _value_\n",
      "_←_\n",
      "6) Count Count +1\n",
      "_←_\n",
      "7) MinHeapify()\n",
      "8) end Add\n",
      "\n",
      "-----\n",
      "\n",
      "to assume that these costs are similar (though we will see a few significant exceptions).\n",
      "The following hypothesis follows directly:\n",
      "\n",
      "\n",
      "**Property D.** The running times of insertion sort and selection sort are quadratic\n",
      "and within a small constant factor of one another for randomly ordered arrays of\n",
      "distinct values.\n",
      "\n",
      "**Evidence:** This statement has been validated on many different computers over\n",
      "the past half-century. Insertion sort was about twice as fast as selection sort when\n",
      "the first edition of this book was written in 1980 and it still is today, even though it\n",
      "took several hours to sort 100,000 items with these algorithms then and just several\n",
      "seconds today. Is insertion sort a bit faster than selection sort on your computer?\n",
      "To find out, you can use the class SortCompare on the next page, which uses the\n",
      "sort() methods in the classes named as command-line arguments to perform the\n",
      "given number of experiments (sorting arrays of the given size) and prints the ratio\n",
      "of the observed running times of the algorithms.\n",
      "\n",
      "\n",
      "To validate this hypothesis, we use SortCompare (see page 256) to perform the experiments. As usual, we use Stopwatch to compute the running time. The implementation\n",
      "of time() shown here does the job for the basic sorts in this chapter. The “randomly ordered” input model is embedded in the timeRandomInput() method in SortCompare,\n",
      "which generates random Double values, sorts them, and returns the total measured\n",
      "time of the sort for the given\n",
      "number of trials. Using ran\n",
      "public static double time(String alg, Comparable[] a)\n",
      "\n",
      "dom Double values between {\n",
      "0.0 and 1.0 is much simpler Stopwatch timer = new Stopwatch();\n",
      "than the alternative of us- if (alg.equals(\"Insertion\")) Insertion.sort(a);\n",
      "\n",
      "if (alg.equals(\"Selection\")) Selection.sort(a);\n",
      "\n",
      "ing a library function such if (alg.equals(\"Shell\"))   Shell.sort(a);\n",
      "as StdRandom.shuffle() if (alg.equals(\"Merge\"))   Merge.sort(a);\n",
      "\n",
      "and is effective because equal if (alg.equals(\"Quick\"))   Quick.sort(a);\n",
      "\n",
      "/* Move elements of A[0..i-1], that are greater than\n",
      "           key, to one position ahead of their current\n",
      "           position. This loop will run at most k times */\n",
      "        while (j >= 0 && A[j] > key) {\n",
      "            A[j + 1] = A[j];\n",
      "            j = j - 1;\n",
      "        }\n",
      "        A[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "/* Function to sort an array using insertion sort*/\n",
      "function insertionSort(A, size)\n",
      "{\n",
      "   var i, key, j;\n",
      "   for (i = 1; i < size; i++)\n",
      "   {\n",
      "       key = A[i];\n",
      "       j = i-1;\n",
      "\n",
      "       /* Move elements of A[0..i-1], that are \n",
      "          greater than key, to one \n",
      "          position ahead of their current position.\n",
      "          This loop will run at most k times */\n",
      "       while (j >= 0 && A[j] > key)\n",
      "       {\n",
      "           A[j+1] = A[j];\n",
      "           j = j-1;\n",
      "       }\n",
      "       A[j+1] = key;\n",
      "   }\n",
      "}\n",
      "\n",
      "// This code is contributed by itsok.\n",
      "/* Function to sort an array using insertion sort*/\n",
      "function insertionSort(A, size)\n",
      "{\n",
      "   var i, key, j;\n",
      "   for (i = 1; i < size; i++)\n",
      "   {\n",
      "       key = A[i];\n",
      "       j = i-1;\n",
      "\n",
      "       /* Move elements of A[0..i-1], that are \n",
      "          greater than key, to one \n",
      "          position ahead of their current position.\n",
      "          This loop will run at most k times */\n",
      "       while (j >= 0 && A[j] > key)\n",
      "       {\n",
      "           A[j+1] = A[j];\n",
      "           j = j-1;\n",
      "       }\n",
      "       A[j+1] = key;\n",
      "   }\n",
      "}\n",
      "\n",
      "// This code is contributed by itsok.\n",
      "2 3 5 6 8 9 10 \n",
      "```\n",
      "\n",
      "\n",
      "#### Expected Approach – Using Heap – O(k + (n-k)*Log k) Time and O(k) Space\n",
      "\n",
      "\n",
      "Follow the below steps to solve the problem:\n",
      "\n",
      "\n",
      "\n",
      "- Create a Min Heap of size K+1 with the first K+1 elements. We are creating a heap of size K as the element can be at most K distance from its index in a sorted array.\n",
      "\n",
      "- One by one remove the min element from the heap, put it in the result array, and add a new element to the heap from the remaining elements.\n",
      "\n",
      "\n",
      "Below is the implementation of the above approach:\n",
      "\n",
      "\n",
      "```cpp\n",
      "#include <bits/stdc++.h>\n",
      "using namespace std;\n",
      "\n",
      "67\n",
      "\n",
      "|4|1|3|2|\n",
      "|---|---|---|---|\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "but that slot is holding a value already. So we first move a[0] ‘up’ one slot into a[1] (care\n",
      "being taken to remember a[1] first!), and then we can move the old a[1] to a[0], giving\n",
      "\n",
      "1 4 3 2 .\n",
      "At the next step, the algorithm treats a[0],a[1] as an already sorted array and tries to\n",
      "insert a[2]=3. This value obviously has to fit between a[0]=1 and a[1]=4. This is achieved\n",
      "by moving a[1] ‘up’ one slot to a[2] (the value of which we assume we have remembered),\n",
      "allowing us to move the current value into a[1], giving 1 3 4 2 .\n",
      "Finally, a[3]=2 has to be inserted into the sorted array a[0],...,a[2]. Since a[2]=4 is\n",
      "bigger than 2, it is moved ‘up’ one slot, and the same happens for a[1]=3. Comparison with\n",
      "`a[0]=1 shows that a[1] was the slot we were looking for, giving` 1 2 3 4 .\n",
      "The general algorithm for Insertion Sort can therefore be written:\n",
      "```\n",
      " for ( i = 1 ; i < n ; i++ ) {\n",
      "   for( j = i ; j > 0 ; j-- )\n",
      "    if ( a[j] < a[j-1] )\n",
      "      swap a[j] and a[j-1]\n",
      "    else break\n",
      " }\n",
      "\n",
      "```\n",
      "The outer loop goes over the n `1 items to be inserted, and the inner loop takes each next`\n",
      "_−_\n",
      "item and swaps it back through the currently sorted portion till it reaches its correct position.\n",
      "However, this typically involves swapping each next item many times to get it into its right\n",
      "position, so it is more efficient to store each next item in a temporary variable t and only\n",
      "insert it into its correct position when that has been found and its content moved:\n",
      "```\n",
      " for ( i = 1 ; i < n ; i++ ) {\n",
      "   j = i\n",
      "   t = a[j]\n",
      "   while ( j > 0 && t < a[j-1] ) {\n",
      "    a[j] = a[j-1]\n",
      "    j-   }\n",
      "   a[j] = t\n",
      " }\n",
      "\n",
      "_Sorting is the process of putting a collection of items in order. We shall formulate and discuss_\n",
      "many sorting algorithms later, but we are already able to present one of them.\n",
      "The node values stored in a binary search tree can be printed in ascending order by\n",
      "recursively printing each left sub-tree, root, and right sub-tree in the right order as follows:\n",
      "```\n",
      "  printInOrder(tree t) {\n",
      "   if ( not isEmpty(t) ) {\n",
      "     printInOrder(left(t))\n",
      "     print(root(t))\n",
      "     printInOrder(right(t))\n",
      "   }\n",
      "  }\n",
      "\n",
      "```\n",
      "Then, if the collection of items to be sorted is given as an array a of known size n, they can\n",
      "be printed in sorted order by the algorithm:\n",
      "```\n",
      "  sort(array a of size n) {\n",
      "   t = EmptyTree\n",
      "   for i = 0,1,...,n-1\n",
      "     t = insert(a[i],t)\n",
      "   printInOrder(t)\n",
      "  }\n",
      "\n",
      "```\n",
      "which starts with an empty tree, inserts all the items into it using insert(v, t) to give a\n",
      "binary search tree, and then prints them in order using printInOrder(t). Exercise: modify\n",
      "this algorithm so that instead of printing the sorted values, they are put back into the original\n",
      "array in ascending order.\n",
      "\n",
      "47\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "##### 7.10 Balancing binary search trees\n",
      "\n",
      "**insertion sorting** Take the items one at a time and insert them into an initially\n",
      "empty data structure such that the data structure continues to be\n",
      "sorted at each stage.\n",
      "\n",
      "**divide and conquer** Recursively split the problem into smaller sub-problems till you\n",
      "just have single items that are trivial to sort. Then put the sorted\n",
      "‘parts’ back together in a way that preserves the sorting.\n",
      "\n",
      "All these strategies are based on comparing items and then rearranging them accordingly.\n",
      "These are known as comparison-based sorting algorithms. We will later consider other noncomparison-based algorithms which are possible when we have specific prior knowledge about\n",
      "the items that can occur, or restrictions on the range of items that can occur.\n",
      "The ideas above are based on the assumption that all the items to be sorted will fit into\n",
      "the computer’s internal memory, which is why they are often referred to as being internal\n",
      "_sorting algorithms. If the whole set of items cannot be stored in the internal memory at one_\n",
      "time, different techniques have to be used. These days, given the growing power and memory\n",
      "of computers, external storage is becoming much less commonly needed when sorting, so we\n",
      "will not consider external sorting algorithms in detail. Suffice to say, they generally work by\n",
      "splitting the set of items into subsets containing as many items as can be handled at one time,\n",
      "sorting each subset in turn, and then carefully merging the results.\n",
      "\n",
      "##### 9.3 How many comparisons must it take?\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "Input:arr[] = {6, 5, 3, 2, 8, 10, 9}, K = 3Output:arr[] = {2, 3, 5, 6, 8, 9, 10}Input:arr[] = {10, 9, 8, 7, 4, 70, 60, 50}, k = 4Output:arr[] = {4, 7, 8, 9, 10, 50, 60, 70}\n",
      "\n",
      "\n",
      "#### Naive Approach – Insertion Sort – O(nk) Time and O(1) Space\n",
      "\n",
      "\n",
      "We simply useinsertion sortas it is to sort the array efficiently as index of every element can be changed by atmost K places, which will reduce the time complexity of this algorithm from O(n2) to O(nk) as the element would not be inserted more than k position away,\n",
      "\n",
      "\n",
      "```cpp\n",
      "#include <bits/stdc++.h>\n",
      "using namespace std;\n",
      "\n",
      "void insertionSort(vector<int>& arr) {\n",
      "    \n",
      "    // Traverse through 1 to size of the array\n",
      "    for (int i = 1; i < arr.size(); i++) {\n",
      "        \n",
      "        // Move elements of arr[0..i-1], that are greater than key\n",
      "        // to one position ahead of their current position\n",
      "        int key = arr[i];\n",
      "        int j = i - 1;\n",
      "        while (j >= 0 && arr[j] > key) {\n",
      "            arr[j + 1] = arr[j];\n",
      "            j--;\n",
      "        }\n",
      "        arr[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    vector<int> arr = {6, 5, 3, 2, 8, 10, 9};\n",
      "    insertionSort(arr);    \n",
      "    for (int x : arr)\n",
      "        cout << x << \" \";\n",
      "    return 0;\n",
      "}\n",
      "#include <bits/stdc++.h>\n",
      "using namespace std;\n",
      "\n",
      "void insertionSort(vector<int>& arr) {\n",
      "    \n",
      "    // Traverse through 1 to size of the array\n",
      "    for (int i = 1; i < arr.size(); i++) {\n",
      "        \n",
      "        // Move elements of arr[0..i-1], that are greater than key\n",
      "        // to one position ahead of their current position\n",
      "        int key = arr[i];\n",
      "        int j = i - 1;\n",
      "        while (j >= 0 && arr[j] > key) {\n",
      "            arr[j + 1] = arr[j];\n",
      "            j--;\n",
      "        }\n",
      "        arr[j + 1] = key;\n",
      "    }\n",
      "}\n",
      "User level: beginner\n",
      "Docs length: 34997\n",
      "Generated response length: 2797\n",
      "output: Let's break down the concept of **Insertion Sort** in a way that's easy to understand, using simple analogies and step-by-step explanations.\n",
      "\n",
      "### What is Insertion Sort?\n",
      "\n",
      "Think of **Insertion Sort** like sorting a hand of playing cards. When you sort cards, you take one card at a time and insert it into the correct position among the cards you have already sorted. \n",
      "\n",
      "### How Does It Work?\n",
      "\n",
      "1. **Start with the first card**: This card is already \"sorted\" because it's the only one you have.\n",
      "2. **Take the next card**: Look at this new card and compare it with the cards you have already sorted.\n",
      "3. **Find the right spot**: If the new card is smaller than the cards in your hand, you move those cards one position to the right to make space for the new card.\n",
      "4. **Insert the new card**: Place the new card in its correct position.\n",
      "5. **Repeat**: Keep taking the next card and inserting it into the sorted part of your hand until all cards are sorted.\n",
      "\n",
      "### Visual Example\n",
      "\n",
      "Let's say you have the following cards (numbers):\n",
      "\n",
      "```\n",
      "4, 3, 2, 1\n",
      "```\n",
      "\n",
      "- Start with `4`: \n",
      "  - Sorted: `4`\n",
      "- Next, take `3`: \n",
      "  - Compare with `4`, since `3` is smaller, move `4` to the right.\n",
      "  - Insert `3`: \n",
      "  - Sorted: `3, 4`\n",
      "- Next, take `2`: \n",
      "  - Compare with `4` (move `4`), then compare with `3` (move `3`).\n",
      "  - Insert `2`: \n",
      "  - Sorted: `2, 3, 4`\n",
      "- Finally, take `1`: \n",
      "  - Compare with `4`, `3`, and `2` (move all).\n",
      "  - Insert `1`: \n",
      "  - Sorted: `1, 2, 3, 4`\n",
      "\n",
      "### Key Points\n",
      "\n",
      "- **Best Case**: If the array is already sorted, Insertion Sort runs in linear time, O(N), because it only needs to make one comparison for each element.\n",
      "- **Worst Case**: If the array is in reverse order, it runs in quadratic time, O(N²), because each new element has to be compared with all the already sorted elements.\n",
      "- **Average Case**: For a randomly ordered array, it also runs in O(N²) time.\n",
      "\n",
      "### Code Example\n",
      "\n",
      "Here’s a simple code example in Python to illustrate how Insertion Sort works:\n",
      "\n",
      "```python\n",
      "def insertion_sort(arr):\n",
      "    for i in range(1, len(arr)):\n",
      "        key = arr[i]  # The current element to be inserted\n",
      "        j = i - 1\n",
      "        # Move elements of arr[0..i-1], that are greater than key,\n",
      "        # to one position ahead of their current position\n",
      "        while j >= 0 and arr[j] > key:\n",
      "            arr[j + 1] = arr[j]\n",
      "            j -= 1\n",
      "        arr[j + 1] = key  # Insert the key at the correct position\n",
      "\n",
      "# Example usage\n",
      "numbers = [4, 3, 2, 1]\n",
      "insertion_sort(numbers)\n",
      "print(numbers)  # Output: [1, 2, 3, 4]\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "Insertion Sort is a simple and intuitive sorting algorithm that works well for small or nearly sorted arrays. It builds the sorted array one element at a time, making it easy to understand and implement. If you have any questions or want to explore more about sorting algorithms, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"what is Insertion Sort\"),\n",
    "    ],\n",
    "    \"user_level\": \"beginner\"\n",
    "}\n",
    "\n",
    "output = graph.invoke(inputs)\n",
    "print('output:',output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat History using LangGraph\n",
    "[Langgraph][https://python.langchain.com/docs/how_to/message_history/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import streamlit as st\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    # Update message history with response:\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# for chunk in app.stream({\"messages\": input_messages}, config):\n",
    "#     print(chunk)\n",
    "    \n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].content\n",
    "# output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = app.get_state(config).values[\"messages\"]\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "updated_messages = [RemoveMessage(m.id) for m in messages]\n",
    "app.update_state(values = {\"messages\": updated_messages}, config = config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain with Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    response = runnable.invoke(state)\n",
    "    # Update message history with response:\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# input_dict = {\n",
    "#     \"messages\": [HumanMessage(\"Hi, I'm Bob.\")],\n",
    "#     \"language\": \"Spanish\",\n",
    "# }\n",
    "\n",
    "# output = app.invoke(input_dict, config)\n",
    "print(output)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "state = app.get_state(config).values\n",
    "\n",
    "# print(f'Language: {state[\"language\"]}')\n",
    "for message in state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append new messages manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "_ = app.update_state(config, {\"messages\": [HumanMessage(\"Test\")]})\n",
    "_ = app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.get_state(config).values\n",
    "\n",
    "print(f'Language: {state[\"language\"]}')\n",
    "for message in state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from utils.chunk_doc import get_retriever\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY']= st.secrets[\"New_Langsmith_key\"]\n",
    "os.environ['LANGCHAIN_PROJECT']=\"default\"\n",
    "\n",
    "retriever = get_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_documents\",\n",
    "    \"Search and return relevant documents based on user's query.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Prompt[rlm/rag-prompt]********************\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True,api_key=st.secrets[\"OpenAI_key\"])\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# inputs = {\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     \"messages\": [\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#         HumanMessage(\"What is quick sort\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     11\u001b[0m         HumanMessage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is obama\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     12\u001b[0m     ]\n\u001b[0;32m     13\u001b[0m }\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput:\u001b[39m\u001b[38;5;124m'\u001b[39m,output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# for output in graph.stream(inputs):\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     for key, value in output.items():\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#         pprint.pprint(f\"Output from node '{key}':\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         pprint.pprint(\"---\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#         pprint.pprint(value, indent=2, width=80, depth=None)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#     pprint.pprint(\"\\n---\\n\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1940\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1939\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1940\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1945\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1948\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1949\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1950\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1660\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1667\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\pregel\\runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:246\u001b[0m, in \u001b[0;36mToolNode.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    245\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:219\u001b[0m, in \u001b[0;36mToolNode._func\u001b[1;34m(self, input, config, store)\u001b[0m\n\u001b[0;32m    217\u001b[0m input_types \u001b[38;5;241m=\u001b[39m [input_type] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tool_calls)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m--> 219\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;241m*\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one, tool_calls, input_types, config_list)\n\u001b[0;32m    221\u001b[0m     ]\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# preserve existing behavior for non-command tool outputs for backwards compatibility\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(output, Command) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# TypedDict, pydantic, dataclass, etc. should all be able to load from dict\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\Elroy Lian\\.conda\\envs\\chatbot\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# inputs = {\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(\"What is quick sort\")\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"Who is obama\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "output = graph.invoke(inputs)\n",
    "print('output:',output[\"messages\"][-1].content)\n",
    "# for output in graph.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint.pprint(f\"Output from node '{key}':\")\n",
    "#         pprint.pprint(\"---\")\n",
    "#         pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "#     pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elroy\\AppData\\Local\\Temp\\ipykernel_30700\\1394110419.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import streamlit as st\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=st.secrets[\"OpenAI_key\"],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_templates.image_template import image_app\n",
    "def read_image_bytes(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        return image_file.read()\n",
    "img = read_image_bytes('test_image_1.jpeg')\n",
    "import base64\n",
    "\n",
    "image_data = base64.b64encode(img).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "# image_chain = get_image_chain()\n",
    "\n",
    "message_content = [{\"type\": \"text\", \"text\": \"Describe the image provided\"}]\n",
    "message_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
    "                })\n",
    "\n",
    "final_message = {\n",
    "    \"messages\":[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": message_content,\n",
    "    }],\n",
    "    \"user_level\": \"beginner\"}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "image_app.invoke(input = final_message,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing retrieval for: What is quicksort?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No index params provided. Could not determine relevance function. Use L2 distance as default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents found: 10\n",
      "\n",
      "--- Document 1 ---\n",
      "Source: data\\md\\algo1.md\n",
      "Content: ###### The basic algorithm  Quicksort is a divide-and-conquer method for sorting. It\n",
      "\n",
      "works by partitioning an array into two subarrays, then sorting the subarrays independently. Quicksort is compleme...\n",
      "\n",
      "--- Document 2 ---\n",
      "Source: data\\md\\algo1.md\n",
      "Content: -----\n",
      "\n",
      "###### Performance characteristics Quicksort has been subjected to very thorough\n",
      "\n",
      "mathematical analysis, so that we can make precise statements about its performance.\n",
      "The analysis has been vali...\n",
      "\n",
      "--- Document 3 ---\n",
      "Source: data\\md\\divide-and-conquer.md\n",
      "Content: - Slower than QuickSort in general. QuickSort is more cache friendly because it works in-place.\n",
      "\n",
      "\n",
      "Quick Links:\n",
      "\n",
      "\n",
      "\n",
      "- Merge Sort Based Coding Questions\n",
      "\n",
      "- Bottom up (or Iterative) Merge Sort\n",
      "\n",
      "- Recent A...\n",
      "\n",
      "--- Document 4 ---\n",
      "Source: data\\md\\randomized-algorithms.md\n",
      "Content: ### C#\n",
      "\n",
      "\n",
      "```\n",
      "// C# implementation of QuickSort// using Hoare's partition schemeusingSystem;publicclassGFG {// Driver CodepublicstaticvoidMain(){int[] arr = { 10, 7, 8, 9, 1, 5 };intn = arr.Length;quic...\n",
      "\n",
      "--- Document 5 ---\n",
      "Source: data\\md\\algo1.md\n",
      "Content: Thus, in most practical situations, quicksort is the method of choice. Still, given the\n",
      "broad reach of sorting and the broad variety of computers and systems, a flat statement\n",
      "like this is difficult t...\n",
      "\n",
      "--- Document 6 ---\n",
      "Source: data\\md\\randomized-algorithms.md\n",
      "Content: ### Javascript\n",
      "\n",
      "\n",
      "```\n",
      "// javascript implementation of QuickSort// using Hoare's partition scheme// This function takes last element as // pivot, places the pivot element at// its correct position in so...\n",
      "\n",
      "--- Document 7 ---\n",
      "Source: data\\md\\algo1.md\n",
      "Content: As with standard quicksort, the running time tends to the average as the array size\n",
      "\n",
      "grows, and large deviations from the average are extremely unlikely, so that you can\n",
      "depend on 3-way quicksort’s ru...\n",
      "\n",
      "--- Document 8 ---\n",
      "Source: data\\md\\dsa1.md\n",
      "Content: whole array to reflect the splitting. We say that we partition the array, and the Quicksort\n",
      "algorithm is then applied to the sub-arrays of this partitioned array.\n",
      "In order for the algorithm to be call...\n",
      "\n",
      "--- Document 9 ---\n",
      "Source: data\\md\\sorting-algorithms.md\n",
      "Content: def quicksort(arr, low, high):\n",
      "        if low < high:\n",
      "            pi = partition(arr, low, high)\n",
      "            quicksort(arr, low, pi - 1)\n",
      "            quicksort(arr, pi + 1, high)\n",
      "\n",
      "    quicksort(element...\n",
      "\n",
      "--- Document 10 ---\n",
      "Source: data\\md\\dsa1.md\n",
      "Content: ```\n",
      "77\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "```\n",
      "      a[acount++] = a[i]\n",
      "    else\n",
      "      b[bcount++] = a[i]\n",
      "   }\n",
      "   for ( i = 0 ; i < bcount ; i++ )\n",
      "    a[acount++] = b[i]\n",
      "   return right-bcount+1\n",
      " }\n",
      "\n",
      "```\n",
      "Like the first partition...\n"
     ]
    }
   ],
   "source": [
    "from utils.chunk_doc import get_retriever\n",
    "\n",
    "def test_quicksort_retrieval():\n",
    "    # Get the retriever\n",
    "    retriever = get_retriever()\n",
    "    \n",
    "    # Test quicksort query\n",
    "    query = \"What is quicksort?\"\n",
    "    print(f\"\\nTesting retrieval for: {query}\")\n",
    "    \n",
    "    try:\n",
    "        # Get documents\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        print(f\"\\nNumber of documents found: {len(docs)}\")\n",
    "        \n",
    "        if len(docs) > 0:\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                print(f\"\\n--- Document {i} ---\")\n",
    "                print(f\"Source: {doc.metadata.get('source', 'No source')}\")\n",
    "                print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        else:\n",
    "            print(\"No documents were retrieved!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {str(e)}\")\n",
    "\n",
    "# Run test\n",
    "if __name__ == \"__main__\":\n",
    "    test_quicksort_retrieval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
