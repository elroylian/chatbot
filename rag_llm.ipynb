{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store with Custom Embeddings Using Langchain and Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sentence_chunking import get_sentence_chunks\n",
    "\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter as Rec\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "# model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "        def __init__(self):\n",
    "            self.model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "    \n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return [self.model.encode(t).tolist() for t in texts]\n",
    "        \n",
    "        def embed_query(self, query: str) -> List[float]:\n",
    "            return self.model.encode(query).tolist()\n",
    "\n",
    "embedding_func = MyEmbeddings()\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "# client = chromadb.Client()\n",
    "client = chromadb.PersistentClient(path=\"db/pdfs\")\n",
    "\n",
    "\n",
    "# Create a collection in ChromaDB (will store embeddings)\n",
    "# collection = client.get_or_create_collection(name=\"markdown_chunks_collection\")\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# vector_store = Chroma(\n",
    "#     collection_name=\"markdown_chunks_collection\",\n",
    "#     embedding_function=embedding_func,\n",
    "#     # other params...\n",
    "# )\n",
    "\n",
    "vector_store = Chroma(\n",
    "    client = client,\n",
    "    collection_name=\"markdown_chunks_collection\",\n",
    "    embedding_function=embedding_func,\n",
    "    # persist_directory = \"db/pdfs\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "def split_chunks():\n",
    "    try:\n",
    "        # Path to markdown directory\n",
    "        md_dir = Path(\"data/md/\")\n",
    "        chunk_id_counter = 1  # Initialize a counter for unique chunk IDs\n",
    "        ids = []\n",
    "        documents = []\n",
    "\n",
    "        # Loop through all markdown files in the md directory\n",
    "        for md_file in md_dir.glob(\"*.md\"):\n",
    "            with open(md_file, \"r\") as f:\n",
    "                md_content = f.read()\n",
    "\n",
    "            # Chunk the markdown content\n",
    "            ## Chunk Method 1: Sentence Chunking\n",
    "            # chunks = get_sentence_chunks(md_content, tokenizer)\n",
    "            \n",
    "            ## Chunk Method 2: CST Token Chunking\n",
    "            # chunks = get_cst_token_chunks(md_content, tokenizer)\n",
    "            \n",
    "            ## Chunk Method 3: Recursive Character Chunking\n",
    "            text_splitter = Rec(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=500,\n",
    "                length_function=len,\n",
    "                add_start_index=True\n",
    "            )\n",
    "            chunks = text_splitter.split_text(md_content)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                # Create a Document object for the chunk\n",
    "                document_to_add = Document(\n",
    "                    page_content = chunk,\n",
    "                    metadata = {\"source\": str(md_file)}\n",
    "                )\n",
    "                \n",
    "                documents.append(document_to_add)\n",
    "                \n",
    "                ids.append(str(chunk_id_counter)) # Add document ID to the list\n",
    "\n",
    "                chunk_id_counter += 1  # Increment the ID counter\n",
    "        \n",
    "        vector_store.add_documents(documents = documents, ids = ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_chunks()\n",
    "    \n",
    "    # results = vector_store.similarity_search(query=\"insertion sort\",k=1)\n",
    "    # for doc in results:\n",
    "    #     print(f\"* {doc.page_content} [{doc.metadata}]\")\n",
    "    \n",
    "    retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 10, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering Using Langchain and Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If the answer is not in the retrieved context,\"\n",
    "    \"say that you don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "response = rag_chain.invoke({\"input\": \"What is Elon Musk's Son name?\"})\n",
    "response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search relevant documents based on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* _CHAPTER 8. SORTING_ 67\n",
      "\n",
      "###### 8.4 Insertion Sort\n",
      "\n",
      "Insertion sort is a somewhat interesting algorithm with an expensive runtime of\n",
      "_O(n[2]). It can be best thought of as a sorting scheme similar to that of sorting_\n",
      "a hand of playing cards, i.e. you take one card and then look at the rest with\n",
      "the intent of building up an ordered set of cards in your hand.\n",
      "\n",
      "4 75 74\n",
      "\n",
      "4 75 74 2 54 4 75 74 2 54 4 75 74 2 54\n",
      "\n",
      "2 54\n",
      "\n",
      "4 74 75 2 54 2 4 74 75 54 2 4 54 74 75\n",
      "\n",
      "Figure 8.4: Insertion Sort Iterations\n",
      "\n",
      "1) algorithm Insertionsort(list)\n",
      "2) **Pre:** _list_ =\n",
      "_̸_ _∅_\n",
      "3) **Post: list has been sorted into values of ascending order**\n",
      "4) _unsorted_ 1\n",
      "_←_\n",
      "5) **while unsorted < list.Count**\n",
      "6) _hold_ _list[unsorted]_\n",
      "_←_\n",
      "7) _i_ _unsorted_ 1\n",
      "_←_ _−_\n",
      "8) **while i** 0 and hold < list[i]\n",
      "_≥_\n",
      "9) _list[i + 1]_ _list[i]_\n",
      "_←_\n",
      "10) _i_ _i_ 1\n",
      "_←_ _−_\n",
      "11) **end while**\n",
      "12) _list[i + 1]_ _hold_\n",
      "_←_\n",
      "13) _unsorted_ _unsorted + 1_\n",
      "_←_\n",
      "14) **end while**\n",
      "15) **return list**\n",
      "16) end Insertionsort\n",
      "\n",
      "|4|Col2|\n",
      "|---|---|\n",
      "||| [{'source': 'data\\\\md\\\\dsa.md'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query=\"insertion sort\",k=1)\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
