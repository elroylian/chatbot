[
    {
        "question": "Describe how you would detect a cycle in a linked list. What are the different approaches, and how do their time and space complexities compare?",
        "answer": "Approach 1: Floyd\u2019s Cycle-Finding Algorithm (Tortoise and Hare): Two pointers, one moving slowly (one step at a time) and the other quickly (two steps at a time). If there\u2019s a cycle, they will eventually meet. Time complexity: O(n), Space complexity: O(1). Approach 2: Hash Set Method: Traverse the linked list and store each node in a hash set. If you encounter a node already in the set, there\u2019s a cycle. Time complexity: O(n), Space complexity: O(n)."
    },
    {
        "question": "Compare depth-first search (DFS) and breadth-first search (BFS). In what scenarios would you use one over the other, and how does their memory usage differ?",
        "answer": "DFS: Explores as far as possible along a branch before backtracking. Used when the solution is deep in the tree, e.g., topological sorting. Memory usage: O(h), where h is the depth of the tree (or height of the graph). BFS: Explores all neighbors at the present depth before moving to nodes at the next depth level. Used for finding the shortest path in unweighted graphs. Memory usage: O(w), where w is the width of the graph or number of nodes at a level."
    },
    {
        "question": "Explain the concept of amortized time complexity using the example of a dynamic array resizing operation. How does it differ from average-case time complexity?",
        "answer": "Amortized complexity accounts for the overall cost spread across multiple operations. Example: in dynamic array resizing, resizing happens only after several insertions, so while resizing takes O(n) time, each insertion is amortized to O(1). Average-case complexity refers to the expected time per operation assuming average inputs, but it doesn\u2019t account for the overall performance across multiple operations like amortization does."
    },
    {
        "question": "Design an LRU (Least Recently Used) cache using appropriate data structures. What are the time complexities of the operations (get, put) in your implementation?",
        "answer": "Data structures: Use a combination of a doubly linked list and a hash map. Hash map for O(1) access to cache entries. Doubly linked list for O(1) insertion and deletion of the least/most recently used elements. Operations: Get: O(1), Put: O(1)."
    },
    {
        "question": "How would you implement a priority queue using a binary heap? Explain the time complexities for insertion, deletion, and finding the minimum element.",
        "answer": "A binary heap (min-heap for priority queue) can be implemented as an array. Insert: O(log n) (heapify up), Delete minimum (or maximum): O(log n) (heapify down), Find minimum: O(1) (root of the heap)."
    },
    {
        "question": "What are the differences between an AVL tree and a Red-Black tree? Explain the trade-offs between these two types of self-balancing binary search trees.",
        "answer": "AVL tree: More strictly balanced, with a height difference of at most 1 for any node. Pro: Faster lookups due to stricter balancing. Con: More frequent rotations, making insertions/deletions slower. Red-Black tree: Less strictly balanced, allowing a height difference of 2. Pro: Faster insertions and deletions due to fewer rotations. Con: Slower lookups compared to AVL trees."
    },
    {
        "question": "Given a recursive solution to a problem, explain how you would optimize it using dynamic programming. Illustrate with an example such as the longest common subsequence (LCS) problem.",
        "answer": "Recursive solution involves overlapping subproblems, so you can store solutions to subproblems in a table to avoid redundant calculations. Example (LCS): Recursive solution has O(2^n) time complexity. Dynamic programming (memoization or tabulation) reduces it to O(m \u00d7 n), where m and n are the lengths of the input strings."
    },
    {
        "question": "Quicksort has a worst-case time complexity of O(n^2). What strategies can be used to improve its performance, and under what conditions does quicksort perform well?",
        "answer": "Worst-case occurs when the pivot is consistently the smallest or largest element. To mitigate: Use randomized pivot selection. Use median-of-three (choose the median of the first, middle, and last elements as the pivot). Quicksort performs well on average with O(n log n) time when the data is randomly ordered or when using randomized/median-based pivoting."
    },
    {
        "question": "Explain the difference between a trie (prefix tree) and a binary search tree (BST). When would using a trie be more efficient than a BST?",
        "answer": "Trie: A tree structure used for storing strings, where each node represents a character. Efficient for: Prefix searches, autocomplete, and dictionary word lookups. BST: A binary tree used to store keys that can be compared (e.g., numbers, strings). Trie is more efficient when searching for prefixes or storing large datasets with common prefixes, as lookup time is proportional to the length of the key, not the number of keys."
    },
    {
        "question": "Describe an efficient algorithm to find the lowest common ancestor (LCA) in a binary search tree. How would you modify this algorithm to work on a general binary tree, and what would the time complexity be?",
        "answer": "In a binary search tree (BST): Start at the root and traverse down the tree. If both nodes are less than the current node, move to the left child; if both are greater, move to the right child. The first node where one key is smaller and the other is larger (or equal) is the LCA. Time complexity: O(h), where h is the height of the tree. In a general binary tree: Use a recursive approach: For each node, recursively check if the left and right subtrees contain one of the target nodes. The first node where both subtrees return true is the LCA. Time complexity: O(n), where n is the number of nodes."
    }
]